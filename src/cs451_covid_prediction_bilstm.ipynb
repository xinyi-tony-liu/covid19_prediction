{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "cs451-covid-prediction-bilstm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueWyH6hDAtim"
      },
      "source": [
        "# CS451 Covid Prediction\n",
        "\n",
        "This notebook contains the group project solution for the cs451 final project, which aims to predict the spread of COVID-19 in Canada through usage of large quantities of Twitter data and other sources. The collection and sentiment analysis of Twitter data is performed separately, and is then used as an extra input alongside other covid-related data with the hopes of improving the model accuracy. We have chosen to implement the techniques outlined in a paper which boasts improvements on state-of-the-art techniques, including ARIMA, Simple Moving Average witha 6-day window, and Double Exponential Moving Average. Their technique uses a bidirectional LSTM and clusters countries by demographic, socioeconomic and health sector indicators to train the model on a richer dataset. We will include the Twitter sentiment analysis as an extra feature and see if this can improve the results even further."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4_hT5At5nsr"
      },
      "source": [
        "# Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6E4CHlqKnIoj"
      },
      "source": [
        "In order to expand our dataset for better accuracy, we performed K-Means clustering on countries using several demographic, socioeconomic and health sector indicators to find countries similar to Canada. We then collected data pertaining to the degree of lockdown measures for each country as supplemental features to our model. The data consists of:\n",
        "\n",
        "*   School closing:\n",
        "   * 0: No measures.\n",
        "   * 1: Safety precautions are required.\n",
        "   * 2: Recommended closing.\n",
        "   * 3: Require closing (only some levels/categories).\n",
        "   * 4: Require closing at all levels.\n",
        "*   Workplace closing:\n",
        "   * 0: No measures.\n",
        "   * 1: Safety precautions are required.\n",
        "   * 2: Recommended closing or working from home.\n",
        "   * 3: Require closing or working from home for some sectors or categories of workers.\n",
        "   * 4: Require closing for all sectors except for essential workplaces.\n",
        "*   Restrictions on gatherings:\n",
        "   * 0: No restrictions.\n",
        "   * 1: Restrictions on very large gatherings (limit > 1000 people).\n",
        "   * 2: Restrictions on gatherings between 101-1000 people.\n",
        "   * 3: Restrictions on gatherings between 11-100 people.\n",
        "   * 4: Restrictions on gatherings of 10 people or less.\n",
        "*   Public transport shutdown:\n",
        "   * 0: No restrictions.\n",
        "   * 1: Recommended closing or significantly reduce volume or routes or means of transportation available.\n",
        "   * 2: Require closing.\n",
        "*   International travel controls:\n",
        "   * 0: No restrictions.\n",
        "   * 1: Screening arrivals.\n",
        "   * 2: Quarantine arrivals from some or all regions.\n",
        "   * 3: Ban arrivals from some regions.\n",
        "   * 4: Ban on all regions or total border closure.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q34cXyZdfdZo"
      },
      "source": [
        "from datetime import date\n",
        "\n",
        "# Data for Canada was collected from https://www.cihi.ca/en/covid-19-intervention-timeline-in-canada\n",
        "# In case of discrepancy for certain measures between provinces, the most common date\n",
        "# of the measure implementation between the most populous provinces was selected.\n",
        "canada_lockdown_measures = {\n",
        "    'school-closing': [\n",
        "        (date(2020, 1, 23), 0),\n",
        "        (date(2020, 3, 17), 4),\n",
        "        (date(2020, 9, 8), 1),\n",
        "    ],\n",
        "    'workplace-closing': [\n",
        "        (date(2020, 1, 23), 0),\n",
        "        (date(2020, 3, 17), 3),\n",
        "        (date(2020, 3, 25), 4),\n",
        "        (date(2020, 5, 5), 3),\n",
        "    ],\n",
        "    'gatherings': [\n",
        "        (date(2020, 1, 23), 0),\n",
        "        (date(2020, 3, 16), 2),\n",
        "        (date(2020, 3, 27), 4),\n",
        "    ],\n",
        "    'public-transport': [\n",
        "        (date(2020, 1, 23), 0),\n",
        "    ],\n",
        "    'international-travel': [\n",
        "        (date(2020, 1, 23), 0),\n",
        "        (date(2020, 3, 16), 2),\n",
        "    ],\n",
        "}\n",
        "\n",
        "new_zealand_lockdown_measures = {\n",
        "    'school-closing': [\n",
        "        (date(2020, 1, 23), 0),\n",
        "        (date(2020, 3, 21), 2),\n",
        "        (date(2020, 3, 23), 4),\n",
        "        (date(2020, 5, 13), 2),\n",
        "        (date(2020, 6, 8), 1),\n",
        "        (date(2020, 8, 11), 3),\n",
        "        (date(2020, 8, 30), 2),\n",
        "        (date(2020, 9, 21), 1),\n",
        "    ],\n",
        "    'workplace-closing': [\n",
        "        (date(2020, 1, 23), 0),\n",
        "        (date(2020, 3, 21), 1),\n",
        "        (date(2020, 3, 25), 4),\n",
        "        (date(2020, 4, 27), 1),\n",
        "        (date(2020, 6, 8), 0),\n",
        "        (date(2020, 8, 11), 1),\n",
        "        (date(2020, 9, 21), 0),\n",
        "    ],\n",
        "    'gatherings': [\n",
        "        (date(2020, 1, 23), 0),\n",
        "        (date(2020, 3, 21), 2),\n",
        "        (date(2020, 3, 23), 4),\n",
        "        (date(2020, 5, 13), 2),\n",
        "        (date(2020, 6, 8), 0),\n",
        "        (date(2020, 8, 11), 3),\n",
        "        (date(2020, 8, 30), 2),\n",
        "        (date(2020, 9, 21), 0),\n",
        "    ],\n",
        "    'public-transport': [\n",
        "        (date(2020, 1, 23), 0),\n",
        "        (date(2020, 3, 23), 1),\n",
        "        (date(2020, 3, 25), 2),\n",
        "        (date(2020, 4, 27), 1),\n",
        "        (date(2020, 5, 13), 0),\n",
        "        (date(2020, 8, 11), 1),\n",
        "        (date(2020, 8, 30), 0),\n",
        "    ],\n",
        "    'international-travel': [\n",
        "        (date(2020, 1, 23), 0),\n",
        "        (date(2020, 3, 21), 2),\n",
        "        (date(2020, 3, 23), 3),\n",
        "        (date(2020, 3, 25), 4),\n",
        "        (date(2020, 4, 27), 3),\n",
        "        (date(2020, 5, 13), 2),\n",
        "        (date(2020, 6, 8), 1),\n",
        "        (date(2020, 8, 11), 3),\n",
        "        (date(2020, 8, 30), 2),\n",
        "        (date(2020, 9, 21), 1),\n",
        "    ],\n",
        "}\n",
        "\n",
        "# data collected from\n",
        "# https://en.wikipedia.org/wiki/COVID-19_pandemic_in_Australia\n",
        "australia_lockdown_measures = {\n",
        "    'school-closing': [\n",
        "        (date(2020, 1, 23), 0),\n",
        "        (date(2020, 3, 21), 4),\n",
        "        (date(2020, 5, 15), 2),\n",
        "        (date(2020, 6, 20), 3),\n",
        "        (date(2020, 7, 7), 4),\n",
        "        (date(2020, 10, 27), 1),\n",
        "    ],\n",
        "    'workplace-closing': [\n",
        "        (date(2020, 1, 23), 0),\n",
        "        (date(2020, 3, 21), 4),\n",
        "        (date(2020, 5, 15), 2),\n",
        "        (date(2020, 6, 20), 3),\n",
        "        (date(2020, 7, 7), 4),\n",
        "        (date(2020, 10, 27), 1),\n",
        "    ],\n",
        "    'gatherings': [\n",
        "        (date(2020, 1, 23), 0),\n",
        "        (date(2020, 3, 21), 3),\n",
        "        (date(2020, 5, 15), 2),\n",
        "        (date(2020, 6, 20), 3),\n",
        "        (date(2020, 10, 27), 1),\n",
        "    ],\n",
        "    'public-transport': [\n",
        "        (date(2020, 1, 23), 0),\n",
        "    ],\n",
        "    'international-travel': [\n",
        "        (date(2020, 1, 23), 0),\n",
        "        (date(2020, 2, 27), 2),\n",
        "        (date(2020, 3, 21), 4),\n",
        "        (date(2020, 5, 15), 2),\n",
        "        (date(2020, 7, 7), 4),\n",
        "        (date(2020, 10, 27), 1),\n",
        "    ],\n",
        "}\n",
        "\n",
        "# data collected from \n",
        "# https://en.wikipedia.org/wiki/COVID-19_pandemic_in_Iceland\n",
        "iceland_lockdown_measures = {\n",
        "    'school-closing': [\n",
        "        (date(2020, 1, 23), 0),\n",
        "        (date(2020, 3, 13), 4),\n",
        "        (date(2020, 4, 21), 1),\n",
        "        (date(2020, 10, 5), 4),\n",
        "        (date(2020, 11, 15), 1),\n",
        "    ],\n",
        "    'workplace-closing': [\n",
        "        (date(2020, 1, 23), 0),\n",
        "        (date(2020, 3, 13), 1),\n",
        "        (date(2020, 3, 24), 4),\n",
        "        (date(2020, 4, 21), 1),\n",
        "        (date(2020, 10, 5), 4),\n",
        "        (date(2020, 11, 15), 1),\n",
        "    ],\n",
        "    'gatherings': [\n",
        "        (date(2020, 1, 23), 0),\n",
        "        (date(2020, 3, 13), 2),\n",
        "        (date(2020, 3, 21), 3),\n",
        "        (date(2020, 4, 21), 1),\n",
        "        (date(2020, 10, 5), 3),\n",
        "        (date(2020, 11, 15), 1),\n",
        "    ],\n",
        "    'public-transport': [\n",
        "        (date(2020, 1, 23), 0),\n",
        "    ],\n",
        "    'international-travel': [\n",
        "        (date(2020, 1, 23), 0),\n",
        "        (date(2020, 3, 2), 2),\n",
        "        (date(2020, 3, 18), 4),\n",
        "        (date(2020, 4, 21), 2),\n",
        "    ],\n",
        "}\n",
        "\n",
        "# data collected from \n",
        "# https://en.wikipedia.org/wiki/COVID-19_pandemic_in_Cyprus\n",
        "cyprus_lockdown_measures = {\n",
        "    'school-closing': [\n",
        "        (date(2020, 1, 23), 0),\n",
        "        (date(2020, 3, 13), 4),\n",
        "        (date(2020, 5, 4), 1),\n",
        "        (date(2020, 11, 30), 4),\n",
        "        (date(2020, 12, 4), 1),\n",
        "    ],\n",
        "    'workplace-closing': [\n",
        "        (date(2020, 1, 23), 0),\n",
        "        (date(2020, 3, 13), 4),\n",
        "        (date(2020, 5, 21), 1),\n",
        "        (date(2020, 11, 30), 4),\n",
        "        (date(2020, 12, 4), 1),\n",
        "    ],\n",
        "    'gatherings': [\n",
        "        (date(2020, 1, 23), 0),\n",
        "        (date(2020, 3, 13), 3),\n",
        "        (date(2020, 5, 21), 2),\n",
        "        (date(2020, 11, 30), 3),\n",
        "        (date(2020, 12, 4), 0),\n",
        "    ],\n",
        "    'public-transport': [\n",
        "        (date(2020, 1, 23), 0),\n",
        "    ],\n",
        "    'international-travel': [\n",
        "        (date(2020, 1, 23), 0),\n",
        "        (date(2020, 3, 13), 4),\n",
        "        (date(2020, 5, 21), 2),\n",
        "        (date(2020, 11, 30), 4),\n",
        "        (date(2020, 12, 4), 2),\n",
        "    ],\n",
        "}\n",
        "\n",
        "# data collected from \n",
        "# https://en.wikipedia.org/wiki/COVID-19_pandemic_in_the_United_States\n",
        "# https://en.wikipedia.org/wiki/COVID-19_pandemic_in_New_York_(state)\n",
        "# https://www.nbcnews.com/health/health-news/here-are-stay-home-orders-across-country-n1168736\n",
        "us_lockdown_measures = {\n",
        "    'school-closing': [\n",
        "        (date(2020, 1, 23), 0),\n",
        "        (date(2020, 4, 10), 4),\n",
        "        (date(2020, 9, 4), 3),\n",
        "    ],\n",
        "    'workplace-closing': [\n",
        "        (date(2020, 1, 23), 0),\n",
        "        (date(2020, 3, 20), 4),\n",
        "        (date(2020, 6, 8), 2),\n",
        "        (date(2020, 6, 22), 0),\n",
        "    ],\n",
        "    'gatherings': [\n",
        "        (date(2020, 1, 23), 0),\n",
        "        (date(2020, 3, 20), 4),\n",
        "        (date(2020, 6, 22), 2),\n",
        "        (date(2020, 7, 10), 1),\n",
        "    ],\n",
        "    'public-transport': [\n",
        "        (date(2020, 1, 23), 0),\n",
        "        (date(2020, 3, 20), 2),\n",
        "    ],\n",
        "    'international-travel': [\n",
        "        (date(2020, 1, 23), 0),\n",
        "        (date(2020, 3, 19), 2),\n",
        "    ],\n",
        "}"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKykINhQybf6"
      },
      "source": [
        "from datetime import timedelta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def daterange(start_date, end_date):\n",
        "    for n in range(int((end_date - start_date).days)):\n",
        "        yield start_date + timedelta(n)\n",
        "\n",
        "measures = ['school-closing', 'workplace-closing', 'gatherings', 'public-transport', 'international-travel']\n",
        "\n",
        "def generate_time_series(lockdown_measures, start_date=date(2020, 1, 23), end_date=date(2020, 12, 10)):\n",
        "    data = {\n",
        "        'school-closing': [],\n",
        "        'workplace-closing': [],\n",
        "        'gatherings': [],\n",
        "        'public-transport': [],\n",
        "        'international-travel': [],\n",
        "    }\n",
        "    data['date'] = list(daterange(start_date, end_date))\n",
        "    for measure in measures:\n",
        "        curr_date = start_date\n",
        "        curr_val = 0\n",
        "        for d, val in lockdown_measures[measure]:\n",
        "            data[measure] += [curr_val for _ in daterange(curr_date, d)]\n",
        "            curr_date = d\n",
        "            curr_val = val\n",
        "        if curr_date < end_date:\n",
        "            data[measure] += [curr_val for _ in daterange(curr_date, end_date)]\n",
        "    return pd.DataFrame.from_dict(data)\n",
        "\n",
        "canada_df = generate_time_series(canada_lockdown_measures)\n",
        "new_zealand_df = generate_time_series(new_zealand_lockdown_measures)\n",
        "australia_df = generate_time_series(australia_lockdown_measures)\n",
        "iceland_df = generate_time_series(iceland_lockdown_measures)\n",
        "cyprus_df = generate_time_series(cyprus_lockdown_measures)\n",
        "us_df = generate_time_series(us_lockdown_measures)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vK_uiyNs58VC"
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def get_twitter_sentiment(path='./twitter-sentiment.csv'):\n",
        "    df = pd.read_csv(path)\n",
        "    data = {}\n",
        "    for _, row in df.iterrows():\n",
        "        cc = row['countrycode']\n",
        "        start_date = datetime.strptime(str(row['startdate']), '%Y%m%d').date()\n",
        "        end_date = datetime.strptime(str(row['enddate']), '%Y%m%d').date() + timedelta(1)\n",
        "        avg_p = row['avg_polarity']\n",
        "        avg_s = row['avg_subjectivity']\n",
        "        pc = row['positive_count']\n",
        "        nc = row['negative_count']\n",
        "        new_row = [(d, avg_p, avg_s, pc, nc) for d in daterange(start_date, end_date)]\n",
        "        if cc not in data:\n",
        "            data[cc] = {\n",
        "                'date': [],\n",
        "                'avg_polarity': [],\n",
        "                'avg_subjectivity': [],\n",
        "                'positive_count': [],\n",
        "                'negative_count': [],\n",
        "            }\n",
        "        for x in new_row:\n",
        "            data[cc]['date'].append(x[0])\n",
        "            data[cc]['avg_polarity'].append(x[1])\n",
        "            data[cc]['avg_subjectivity'].append(x[2])\n",
        "            data[cc]['positive_count'].append(x[3])\n",
        "            data[cc]['negative_count'].append(x[4])\n",
        "    for k in data.keys():\n",
        "        data[k] = pd.DataFrame.from_dict(data[k])\n",
        "    return data\n",
        "\n",
        "twitter_sentiment = get_twitter_sentiment()\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uBGI6HJoyH5"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Set seed to ensure our results are reproducible\n",
        "np.random.seed(42)\n",
        "\n",
        "# Merge total cases dataframe with lockdown measure dataframe\n",
        "def merge_dfs(lockdown_measures_df, twitter_sentiment, total_cases_worldwide_df, location):\n",
        "    df = total_cases_worldwide_df[total_cases_worldwide_df['location'] == location].dropna()\n",
        "    df['date'] = [datetime.strptime(x, '%Y-%m-%d').date() for x in df['date']]\n",
        "    merged_df = df.merge(lockdown_measures_df, how='left', on='date').dropna()\n",
        "    merged_df = twitter_sentiment.merge(merged_df, how='left', on='date').dropna()\n",
        "    merged_df = merged_df.drop(columns=['location', 'date'])\n",
        "    merged_vals = np.array(merged_df[[\n",
        "        'school-closing',\n",
        "        'workplace-closing',\n",
        "        'gatherings',\n",
        "        'public-transport',\n",
        "        'international-travel',\n",
        "        'avg_polarity',\n",
        "        'avg_subjectivity',\n",
        "        'total_cases',\n",
        "    ]].values)\n",
        "    return merged_vals\n",
        "\n",
        "total_cases_worldwide_df = pd.read_csv('world-covid-data.csv', usecols=['location', 'total_cases', 'date'])\n",
        "\n",
        "canada_data = merge_dfs(canada_df, twitter_sentiment['CA'], total_cases_worldwide_df, 'Canada')\n",
        "new_zealand_data = merge_dfs(canada_df, twitter_sentiment['NZ'], total_cases_worldwide_df, 'New Zealand')\n",
        "australia_data = merge_dfs(australia_df, twitter_sentiment['AU'], total_cases_worldwide_df, 'Australia')\n",
        "iceland_data = merge_dfs(iceland_df, twitter_sentiment['IS'], total_cases_worldwide_df, 'Iceland')\n",
        "cyprus_data = merge_dfs(cyprus_df, twitter_sentiment['CY'], total_cases_worldwide_df, 'Cyprus')\n",
        "us_data = merge_dfs(us_df, twitter_sentiment['US'], total_cases_worldwide_df, 'United States')\n",
        "\n",
        "data = np.concatenate((canada_data, new_zealand_data, australia_data, iceland_data, cyprus_data, us_data), axis=0)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N34_e4Jxt5Tm"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from itertools import islice\n",
        "\n",
        "# Since LSTMs are sensitive to the scale of the input data,\n",
        "# we normalize the inputs:\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "len_to_split = [len(canada_data), len(new_zealand_data), len(australia_data), len(iceland_data), len(cyprus_data), len(us_data)]\n",
        "inputt = iter(scaled_data)# iter(scaled_data)\n",
        "split_data = [list(islice(inputt, elem)) for elem in len_to_split]\n",
        "canada_data = split_data[0]\n",
        "new_zealand_data = split_data[1]\n",
        "australia_data = split_data[2]\n",
        "iceland_data = split_data[3]\n",
        "cyprus_data = split_data[4]\n",
        "us_data = split_data[5]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pltUooYxgLmP"
      },
      "source": [
        "Our input data will be a sequence of values, and our output will be the single value for the cumulative COVID-19 cases. Given input data $\\{x_1, x_2, ..., x_{n-1}\\}$ and output data $\\{y_1, y_2, ..., y_n\\}$, if we would like to predict a value $y_i$, we will use a sequence $x_{i-k-1},...,x_{i-1}$ to make the prediction. For this purpose, we have defined the `create_dataset` method to reshape the data in this format, where `look_back` corresponds to $k$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zt8ofNwru4zH"
      },
      "source": [
        "def create_dataset(data, look_back=1, test_size=.25, val_size=.2):\n",
        "    data = np.expand_dims(data, axis=1)\n",
        "    data_x, data_y = [], []\n",
        "    for i in range(len(data) - look_back-1):\n",
        "        a = data[i:(i+look_back), 0]\n",
        "        data_x.append(a)\n",
        "        data_y.append(data[i + 1:i + look_back + 1, 0, -1])\n",
        "    \n",
        "\n",
        "    data_x = np.array(data_x)\n",
        "    data_y = np.array(data_y)\n",
        "    test_len = int(len(data) * test_size)\n",
        "    val_len = int((len(data) - test_len) * val_size)\n",
        "    train_len = len(data) - test_len - val_len\n",
        "    train_x = np.array(data_x[0:train_len,:])\n",
        "    val_x = np.array(data_x[train_len:train_len+val_len,:])\n",
        "    test_x = np.array(data_x[train_len+val_len:,:])\n",
        "    train_y = np.array(data_y[0:train_len])\n",
        "    val_y = np.array(data_y[train_len:train_len+val_len])\n",
        "    test_y = np.array(data_y[train_len+val_len:])\n",
        "    return train_x, val_x, test_x, train_y, val_y, test_y\n",
        "\n",
        "look_back=7\n",
        "ca_train_x, ca_val_x, ca_test_x, ca_train_y, ca_val_y, ca_test_y = create_dataset(canada_data, look_back)\n",
        "nz_train_x, nz_val_x, nz_test_x, nz_train_y, nz_val_y, nz_test_y = create_dataset(new_zealand_data, look_back)\n",
        "au_train_x, au_val_x, au_test_x, au_train_y, au_val_y, au_test_y = create_dataset(australia_data, look_back)\n",
        "il_train_x, il_val_x, il_test_x, il_train_y, il_val_y, il_test_y = create_dataset(iceland_data, look_back)\n",
        "cy_train_x, cy_val_x, cy_test_x, cy_train_y, cy_val_y, cy_test_y = create_dataset(cyprus_data, look_back)\n",
        "us_train_x, us_val_x, us_test_x, us_train_y, us_val_y, us_test_y = create_dataset(us_data, look_back)\n",
        "\n",
        "\n",
        "train_x = np.concatenate((\n",
        "    ca_train_x,\n",
        "    nz_train_x,\n",
        "    au_train_x,\n",
        "    il_train_x,\n",
        "    cy_train_x,\n",
        "    us_train_x,\n",
        "    ),\n",
        "    axis=0\n",
        ")\n",
        "test_x = np.concatenate((\n",
        "    ca_test_x,\n",
        "    nz_test_x,\n",
        "    au_test_x,\n",
        "    il_test_x,\n",
        "    cy_test_x,\n",
        "    us_test_x,\n",
        "    ),\n",
        "    axis=0\n",
        ")\n",
        "val_x = np.concatenate((\n",
        "    ca_val_x,\n",
        "    nz_val_x,\n",
        "    au_val_x,\n",
        "    il_val_x,\n",
        "    cy_val_x,\n",
        "    us_val_x,\n",
        "    ),\n",
        "    axis=0\n",
        ")\n",
        "train_y = np.concatenate((\n",
        "    ca_train_y,\n",
        "    nz_train_y,\n",
        "    au_train_y,\n",
        "    il_train_y,\n",
        "    cy_train_y,\n",
        "    us_train_y,\n",
        "    ),\n",
        "    axis=0\n",
        ")\n",
        "test_y = np.concatenate((\n",
        "    ca_test_y,\n",
        "    nz_test_y,\n",
        "    au_test_y,\n",
        "    il_test_y,\n",
        "    cy_test_y,\n",
        "    us_test_y,\n",
        "    ),\n",
        "    axis=0\n",
        ")\n",
        "val_y = np.concatenate((\n",
        "    ca_val_y,\n",
        "    nz_val_y,\n",
        "    au_val_y,\n",
        "    il_val_y,\n",
        "    cy_val_y,\n",
        "    us_val_y,\n",
        "    ),\n",
        "    axis=0\n",
        ")\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SC_PP-J0OaNj"
      },
      "source": [
        "Next we define a dataset class inheriting from `torch.utils.data.Dataset`, which will make loading data for training a cleaner and easier process. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxu31zlLOulZ"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CovidDataset(Dataset):\n",
        "    def __init__(self, data, targets):\n",
        "        self.data = data\n",
        "        self.targets = targets\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return torch.FloatTensor(self.data[idx]).to(device), \\\n",
        "            torch.FloatTensor([self.targets[idx]]).to(device)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igaP4WVnHw5z"
      },
      "source": [
        "batch_size=32\n",
        "train_loader = DataLoader(CovidDataset(train_x, train_y), shuffle=False, batch_size=batch_size)\n",
        "test_loader = DataLoader(CovidDataset(test_x, test_y), shuffle=False, batch_size=batch_size)\n",
        "val_loader = DataLoader(CovidDataset(val_x, val_y), shuffle=False, batch_size=batch_size)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u60g3bILi39a"
      },
      "source": [
        "# Model\n",
        "Now that our data is formatted we will create the model and perform the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJk2FszQAmhc"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCzPs0hUCFcP"
      },
      "source": [
        "import copy\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from math import inf\n",
        "\n",
        "# Define the model\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, seq_len=1, batch_size=1, device=None, verbose=False):\n",
        "        super(BiLSTM, self).__init__()\n",
        "\n",
        "        self.is_training = False\n",
        "\n",
        "        # Number of features\n",
        "        self.input_size = input_size\n",
        "\n",
        "        # Number of features in hidden state\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Number of stacked recurrent layers\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Size of each input sequence \n",
        "        self.seq_len = seq_len\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.device = device\n",
        "        self.verbose = verbose\n",
        "        \n",
        "        # We use a bidirectional LSTM to continuously update older predictions\n",
        "        # based on newer data, which will hopefully improve accuracy by using\n",
        "        # greater context.\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size,\n",
        "            hidden_size,\n",
        "            num_layers,\n",
        "            bidirectional=True,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # The input to the linear layer will be the output of the LSTM.\n",
        "        # Since we are using a bidirectional LSTM, we will have both the outputs\n",
        "        # of the forward-LSTM and the backward-LSTM concatenated, which we will then\n",
        "        # use to create our prediction. This is why the input size is twice the\n",
        "        # size of the hidden output dimension. Our output size is 1 since we are\n",
        "        # performing regression, and need a single value.\n",
        "        self.fc = nn.Linear(hidden_size * 2, 1)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def init_hidden(self):\n",
        "        return (torch.zeros(self.num_layers * 2, self.batch_size, self.hidden_size).to(self.device),\n",
        "            torch.zeros(self.num_layers * 2, self.batch_size, self.hidden_size).to(self.device))\n",
        "    \n",
        "    def gaussian(self, ins, stddev=0.05):\n",
        "        if self.is_training:\n",
        "            return ins + Variable(torch.randn(ins.size()).cuda() * stddev)\n",
        "        return ins\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x.view(-1, self.seq_len, self.input_size))#, self.hidden)\n",
        "\n",
        "        #self.hidden = (hn.detach(), cn.detach())\n",
        "\n",
        "        #out = self.gaussian(out)\n",
        "        #out = self.relu(out)\n",
        "        out = self.fc(out.view(-1, self.seq_len, self.hidden_size * 2))\n",
        "        return out\n",
        "    \n",
        "    def train(\n",
        "        self,\n",
        "        opt,\n",
        "        criterion,\n",
        "        train_loader,\n",
        "        val_loader=None,\n",
        "        epochs=50,\n",
        "        ):\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        smallest_loss = inf\n",
        "        best_model = None\n",
        "        for epoch in range(epochs):\n",
        "            self.hidden = self.init_hidden()\n",
        "            train_loss = 0\n",
        "            train_steps = 0\n",
        "            self.is_training = True\n",
        "            for data, target in train_loader:\n",
        "                opt.zero_grad()\n",
        "        \n",
        "                # Forward\n",
        "                prediction = self.forward(data)\n",
        "                if self.hidden[0].shape[0] != self.batch_size:\n",
        "                    self.hidden = self.init_hidden()\n",
        "        \n",
        "                # Calculate loss\n",
        "                loss = criterion(prediction.squeeze(), target.squeeze())\n",
        "        \n",
        "                # Backpropagate\n",
        "                loss.backward()\n",
        "        \n",
        "                # Perform Adam step\n",
        "                opt.step()\n",
        "        \n",
        "                train_loss += loss.item()\n",
        "                train_steps += 1\n",
        "        \n",
        "            train_losses += [train_loss / train_steps]\n",
        "        \n",
        "\n",
        "            if val_loader is not None:\n",
        "                self.hidden = self.init_hidden()\n",
        "                val_loss = 0\n",
        "                val_steps = 0\n",
        "                self.is_training = False\n",
        "                with torch.no_grad():\n",
        "                    for data, target in val_loader:\n",
        "            \n",
        "                        # Calculate prediction\n",
        "                        prediction = self.forward(data)\n",
        "                        if self.hidden[0].shape[0] != self.batch_size:\n",
        "                            self.hidden = self.init_hidden()\n",
        "                        \n",
        "                        # Calculate loss\n",
        "                        loss = criterion(prediction.squeeze(), target.squeeze())\n",
        "            \n",
        "                        val_loss += loss.item()\n",
        "                        val_steps += 1\n",
        "                \n",
        "                val_losses += [val_loss / val_steps]\n",
        "                if val_losses[-1] < smallest_loss:\n",
        "                    smallest_loss = val_losses[-1]\n",
        "                    best_model = copy.deepcopy(self)\n",
        "            \n",
        "            if self.verbose:\n",
        "                if val_loader is not None:\n",
        "                    print('[INFO] epoch: {}, train loss: {:.10f}, val loss: {:.10f}'.format(epoch, train_losses[-1], val_losses[-1]))\n",
        "                else:\n",
        "                    print('[INFO] epoch: {}, train loss: {:.5f}'.format(epoch, train_losses[-1]))\n",
        "        print('[INFO] Finished training! Smallest loss: {:.10f}'.format(smallest_loss))\n",
        "        return train_losses, val_losses, best_model\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXM2OC7qi9pE"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "epochs = 1000\n",
        "batch_size = 1\n",
        "input_size = test_x.shape[2]\n",
        "num_layers = 2\n",
        "hidden_size = 256\n",
        "\n",
        "#model = BiLSTM(input_size, hidden_size, num_layers, look_back, batch_size)\n",
        "#model.to(device)\n",
        "\n",
        "#criterion = nn.MSELoss()\n",
        "#optimizer = optim.Adam(model.parameters(), lr=1e-4)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ek3GIhx83u-j"
      },
      "source": [
        "import os.path\n",
        "from ray import tune\n",
        "\n",
        "epochs = 1000\n",
        "\n",
        "def train_model(config, checkpoint_dir=None, train_loader=None, val_loader=None):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = BiLSTM(input_size, config['hidden_size'], config['num_layers'], look_back, batch_size, device)\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
        "\n",
        "    if checkpoint_dir:\n",
        "        model_state, optimizer_state = torch.load(\n",
        "            os.path.join(checkpoint_dir, 'checkpoint')\n",
        "        )\n",
        "        model.load_state_dict(model_state)\n",
        "        optimizer.load_state_dict(optimizer_state)\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        model.hidden = model.init_hidden()\n",
        "        running_loss = 0.\n",
        "        epoch_steps = 0\n",
        "        for i, (data, target) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "    \n",
        "            # Forward\n",
        "            prediction = model(data)\n",
        "    \n",
        "            # Calculate loss\n",
        "            loss = criterion(prediction.squeeze(), target.squeeze())\n",
        "    \n",
        "            # Backpropagate\n",
        "            loss.backward()\n",
        "    \n",
        "            # Perform Adam step\n",
        "            optimizer.step()\n",
        "    \n",
        "            running_loss += loss.item()\n",
        "            epoch_steps += 1\n",
        "    \n",
        "        model.hidden = model.init_hidden()\n",
        "        val_loss = 0.\n",
        "        val_steps = 0\n",
        "        with torch.no_grad():\n",
        "            for i, (data, target) in enumerate(val_loader):\n",
        "                # Calculate prediction\n",
        "                prediction = model(data)\n",
        "                \n",
        "                # Calculate loss\n",
        "                loss = criterion(prediction.squeeze(), target.squeeze())\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                val_steps += 1\n",
        "        \n",
        "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
        "            path = os.path.join(checkpoint_dir, 'checkpoint')\n",
        "            torch.save((model.state_dict(), optimizer.state_dict()), path)\n",
        "        \n",
        "        tune.report(loss=(val_loss / val_steps))\n",
        "        print('[INFO] epoch: {}, train loss: {:.5f}, val loss: {:.5f}'.format(epoch, running_loss / epoch_steps, val_loss / val_steps))\n",
        "    print('[INFO] Finished training')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNz1OzTf7Puk"
      },
      "source": [
        "def test_loss(model):\n",
        "    loss = 0.\n",
        "    steps = 0\n",
        "    criterion = nn.MSELoss()\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            prediction = model(data)\n",
        "            loss += criterion(prediction.squeeze(), target.squeeze()).item()\n",
        "            steps += 1\n",
        "    return loss / steps\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3b5Ga-p9Ut7"
      },
      "source": [
        "from ray.tune import CLIReporter\n",
        "\n",
        "config = {\n",
        "    'hidden_size': tune.grid_search([32, 64, 128, 256]),\n",
        "    'num_layers': tune.grid_search([1, 2, 3, 4]),\n",
        "    'lr': tune.grid_search([1e-3,1e-4,1e-5]),\n",
        "}\n",
        "\n",
        "reporter = CLIReporter(metric_columns=['loss', 'training_iteration'])\n",
        "\n",
        "#result = tune.run(\n",
        "#    tune.with_parameters(train_model, train_loader=train_loader, val_loader=val_loader),\n",
        "#    config=config,\n",
        "#    progress_reporter=reporter,\n",
        "#    resources_per_trial={'cpu': 2, 'gpu': 1},\n",
        "#)\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4W65Z3YkI1YQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "aa4525b1-9c13-4995-ff2e-72dc61612490"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "best_num_layers = 2\n",
        "best_lr = 1e-4\n",
        "best_hidden_size = 128\n",
        "\n",
        "best_model = BiLSTM(\n",
        "    input_size,\n",
        "    best_hidden_size,\n",
        "    best_num_layers,\n",
        "    look_back,\n",
        "    device=device,\n",
        "    verbose=True,\n",
        "    batch_size=batch_size\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "opt = optim.Adam(best_model.parameters(), lr=best_lr)\n",
        "\n",
        "train_losses, val_losses, best_trained_model = best_model.train(opt, criterion, train_loader, test_loader, epochs=4000)\n",
        "\n",
        "plt.plot(train_losses, label='train')\n",
        "plt.plot(val_losses, label='val')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "test_l = test_loss(best_trained_model)\n",
        "print(\"Best trial test set loss: {}\".format(test_l))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] epoch: 0, train loss: 0.0542679038, val loss: 0.2616716186\n",
            "[INFO] epoch: 1, train loss: 0.0458077028, val loss: 0.2351563655\n",
            "[INFO] epoch: 2, train loss: 0.0400058262, val loss: 0.2132934019\n",
            "[INFO] epoch: 3, train loss: 0.0358322106, val loss: 0.1951789756\n",
            "[INFO] epoch: 4, train loss: 0.0320855716, val loss: 0.1790489839\n",
            "[INFO] epoch: 5, train loss: 0.0282226312, val loss: 0.1633790888\n",
            "[INFO] epoch: 6, train loss: 0.0240691439, val loss: 0.1470255234\n",
            "[INFO] epoch: 7, train loss: 0.0196275865, val loss: 0.1292509348\n",
            "[INFO] epoch: 8, train loss: 0.0150437540, val loss: 0.1098304811\n",
            "[INFO] epoch: 9, train loss: 0.0106538440, val loss: 0.0893020338\n",
            "[INFO] epoch: 10, train loss: 0.0070024946, val loss: 0.0692796835\n",
            "[INFO] epoch: 11, train loss: 0.0046553083, val loss: 0.0523837288\n",
            "[INFO] epoch: 12, train loss: 0.0037008070, val loss: 0.0410614326\n",
            "[INFO] epoch: 13, train loss: 0.0035039832, val loss: 0.0357103556\n",
            "[INFO] epoch: 14, train loss: 0.0034101852, val loss: 0.0343107876\n",
            "[INFO] epoch: 15, train loss: 0.0032525285, val loss: 0.0342585417\n",
            "[INFO] epoch: 16, train loss: 0.0030657194, val loss: 0.0341197479\n",
            "[INFO] epoch: 17, train loss: 0.0028854067, val loss: 0.0336159476\n",
            "[INFO] epoch: 18, train loss: 0.0027241533, val loss: 0.0329020738\n",
            "[INFO] epoch: 19, train loss: 0.0025809765, val loss: 0.0321627227\n",
            "[INFO] epoch: 20, train loss: 0.0024500963, val loss: 0.0314994181\n",
            "[INFO] epoch: 21, train loss: 0.0023273125, val loss: 0.0309247103\n",
            "[INFO] epoch: 22, train loss: 0.0022112444, val loss: 0.0304076473\n",
            "[INFO] epoch: 23, train loss: 0.0021016258, val loss: 0.0299222837\n",
            "[INFO] epoch: 24, train loss: 0.0019982669, val loss: 0.0294612550\n",
            "[INFO] epoch: 25, train loss: 0.0019008710, val loss: 0.0290256066\n",
            "[INFO] epoch: 26, train loss: 0.0018090465, val loss: 0.0286162759\n",
            "[INFO] epoch: 27, train loss: 0.0017223785, val loss: 0.0282325047\n",
            "[INFO] epoch: 28, train loss: 0.0016405151, val loss: 0.0278725745\n",
            "[INFO] epoch: 29, train loss: 0.0015631764, val loss: 0.0275345483\n",
            "[INFO] epoch: 30, train loss: 0.0014901207, val loss: 0.0272168467\n",
            "[INFO] epoch: 31, train loss: 0.0014211238, val loss: 0.0269183136\n",
            "[INFO] epoch: 32, train loss: 0.0013559764, val loss: 0.0266379154\n",
            "[INFO] epoch: 33, train loss: 0.0012944819, val loss: 0.0263745799\n",
            "[INFO] epoch: 34, train loss: 0.0012364575, val loss: 0.0261272947\n",
            "[INFO] epoch: 35, train loss: 0.0011817304, val loss: 0.0258950478\n",
            "[INFO] epoch: 36, train loss: 0.0011301360, val loss: 0.0256768608\n",
            "[INFO] epoch: 37, train loss: 0.0010815175, val loss: 0.0254718043\n",
            "[INFO] epoch: 38, train loss: 0.0010357220, val loss: 0.0252789522\n",
            "[INFO] epoch: 39, train loss: 0.0009926025, val loss: 0.0250974007\n",
            "[INFO] epoch: 40, train loss: 0.0009520169, val loss: 0.0249262537\n",
            "[INFO] epoch: 41, train loss: 0.0009138272, val loss: 0.0247646142\n",
            "[INFO] epoch: 42, train loss: 0.0008778991, val loss: 0.0246116603\n",
            "[INFO] epoch: 43, train loss: 0.0008441041, val loss: 0.0244664778\n",
            "[INFO] epoch: 44, train loss: 0.0008123164, val loss: 0.0243282958\n",
            "[INFO] epoch: 45, train loss: 0.0007824165, val loss: 0.0241962744\n",
            "[INFO] epoch: 46, train loss: 0.0007542882, val loss: 0.0240696558\n",
            "[INFO] epoch: 47, train loss: 0.0007278217, val loss: 0.0239476770\n",
            "[INFO] epoch: 48, train loss: 0.0007029118, val loss: 0.0238296199\n",
            "[INFO] epoch: 49, train loss: 0.0006794575, val loss: 0.0237147901\n",
            "[INFO] epoch: 50, train loss: 0.0006573654, val loss: 0.0236025773\n",
            "[INFO] epoch: 51, train loss: 0.0006365450, val loss: 0.0234923329\n",
            "[INFO] epoch: 52, train loss: 0.0006169110, val loss: 0.0233835104\n",
            "[INFO] epoch: 53, train loss: 0.0005983843, val loss: 0.0232755932\n",
            "[INFO] epoch: 54, train loss: 0.0005808900, val loss: 0.0231680805\n",
            "[INFO] epoch: 55, train loss: 0.0005643573, val loss: 0.0230605693\n",
            "[INFO] epoch: 56, train loss: 0.0005487203, val loss: 0.0229526275\n",
            "[INFO] epoch: 57, train loss: 0.0005339168, val loss: 0.0228439231\n",
            "[INFO] epoch: 58, train loss: 0.0005198892, val loss: 0.0227341375\n",
            "[INFO] epoch: 59, train loss: 0.0005065827, val loss: 0.0226230006\n",
            "[INFO] epoch: 60, train loss: 0.0004939466, val loss: 0.0225102936\n",
            "[INFO] epoch: 61, train loss: 0.0004819335, val loss: 0.0223957710\n",
            "[INFO] epoch: 62, train loss: 0.0004704989, val loss: 0.0222793094\n",
            "[INFO] epoch: 63, train loss: 0.0004596015, val loss: 0.0221607415\n",
            "[INFO] epoch: 64, train loss: 0.0004492018, val loss: 0.0220399836\n",
            "[INFO] epoch: 65, train loss: 0.0004392651, val loss: 0.0219169378\n",
            "[INFO] epoch: 66, train loss: 0.0004297571, val loss: 0.0217915347\n",
            "[INFO] epoch: 67, train loss: 0.0004206471, val loss: 0.0216637669\n",
            "[INFO] epoch: 68, train loss: 0.0004119060, val loss: 0.0215335948\n",
            "[INFO] epoch: 69, train loss: 0.0004035078, val loss: 0.0214010195\n",
            "[INFO] epoch: 70, train loss: 0.0003954272, val loss: 0.0212660692\n",
            "[INFO] epoch: 71, train loss: 0.0003876422, val loss: 0.0211287651\n",
            "[INFO] epoch: 72, train loss: 0.0003801317, val loss: 0.0209891373\n",
            "[INFO] epoch: 73, train loss: 0.0003728767, val loss: 0.0208472335\n",
            "[INFO] epoch: 74, train loss: 0.0003658591, val loss: 0.0207031375\n",
            "[INFO] epoch: 75, train loss: 0.0003590635, val loss: 0.0205568970\n",
            "[INFO] epoch: 76, train loss: 0.0003524749, val loss: 0.0204085772\n",
            "[INFO] epoch: 77, train loss: 0.0003460795, val loss: 0.0202582517\n",
            "[INFO] epoch: 78, train loss: 0.0003398660, val loss: 0.0201060050\n",
            "[INFO] epoch: 79, train loss: 0.0003338222, val loss: 0.0199519044\n",
            "[INFO] epoch: 80, train loss: 0.0003279387, val loss: 0.0197960683\n",
            "[INFO] epoch: 81, train loss: 0.0003222059, val loss: 0.0196385132\n",
            "[INFO] epoch: 82, train loss: 0.0003166153, val loss: 0.0194794028\n",
            "[INFO] epoch: 83, train loss: 0.0003111594, val loss: 0.0193187436\n",
            "[INFO] epoch: 84, train loss: 0.0003058308, val loss: 0.0191566787\n",
            "[INFO] epoch: 85, train loss: 0.0003006238, val loss: 0.0189932376\n",
            "[INFO] epoch: 86, train loss: 0.0002955320, val loss: 0.0188285347\n",
            "[INFO] epoch: 87, train loss: 0.0002905505, val loss: 0.0186626338\n",
            "[INFO] epoch: 88, train loss: 0.0002856743, val loss: 0.0184956220\n",
            "[INFO] epoch: 89, train loss: 0.0002808992, val loss: 0.0183275444\n",
            "[INFO] epoch: 90, train loss: 0.0002762208, val loss: 0.0181585232\n",
            "[INFO] epoch: 91, train loss: 0.0002716359, val loss: 0.0179885581\n",
            "[INFO] epoch: 92, train loss: 0.0002671407, val loss: 0.0178177853\n",
            "[INFO] epoch: 93, train loss: 0.0002627320, val loss: 0.0176462168\n",
            "[INFO] epoch: 94, train loss: 0.0002584072, val loss: 0.0174739711\n",
            "[INFO] epoch: 95, train loss: 0.0002541636, val loss: 0.0173010413\n",
            "[INFO] epoch: 96, train loss: 0.0002499984, val loss: 0.0171275578\n",
            "[INFO] epoch: 97, train loss: 0.0002459101, val loss: 0.0169534926\n",
            "[INFO] epoch: 98, train loss: 0.0002418953, val loss: 0.0167790152\n",
            "[INFO] epoch: 99, train loss: 0.0002379535, val loss: 0.0166040464\n",
            "[INFO] epoch: 100, train loss: 0.0002340811, val loss: 0.0164287766\n",
            "[INFO] epoch: 101, train loss: 0.0002302783, val loss: 0.0162530904\n",
            "[INFO] epoch: 102, train loss: 0.0002265413, val loss: 0.0160772378\n",
            "[INFO] epoch: 103, train loss: 0.0002228714, val loss: 0.0159010209\n",
            "[INFO] epoch: 104, train loss: 0.0002192630, val loss: 0.0157247669\n",
            "[INFO] epoch: 105, train loss: 0.0002157203, val loss: 0.0155481875\n",
            "[INFO] epoch: 106, train loss: 0.0002122346, val loss: 0.0153717379\n",
            "[INFO] epoch: 107, train loss: 0.0002088143, val loss: 0.0151949312\n",
            "[INFO] epoch: 108, train loss: 0.0002054457, val loss: 0.0150184746\n",
            "[INFO] epoch: 109, train loss: 0.0002021438, val loss: 0.0148415572\n",
            "[INFO] epoch: 110, train loss: 0.0001988862, val loss: 0.0146652963\n",
            "[INFO] epoch: 111, train loss: 0.0001956996, val loss: 0.0144883766\n",
            "[INFO] epoch: 112, train loss: 0.0001925474, val loss: 0.0143125118\n",
            "[INFO] epoch: 113, train loss: 0.0001894747, val loss: 0.0141356057\n",
            "[INFO] epoch: 114, train loss: 0.0001864193, val loss: 0.0139604556\n",
            "[INFO] epoch: 115, train loss: 0.0001834626, val loss: 0.0137834691\n",
            "[INFO] epoch: 116, train loss: 0.0001804924, val loss: 0.0136094518\n",
            "[INFO] epoch: 117, train loss: 0.0001776592, val loss: 0.0134321240\n",
            "[INFO] epoch: 118, train loss: 0.0001747548, val loss: 0.0132599123\n",
            "[INFO] epoch: 119, train loss: 0.0001720644, val loss: 0.0130815970\n",
            "[INFO] epoch: 120, train loss: 0.0001691905, val loss: 0.0129123823\n",
            "[INFO] epoch: 121, train loss: 0.0001666869, val loss: 0.0127316829\n",
            "[INFO] epoch: 122, train loss: 0.0001637727, val loss: 0.0125677294\n",
            "[INFO] epoch: 123, train loss: 0.0001615545, val loss: 0.0123817224\n",
            "[INFO] epoch: 124, train loss: 0.0001584538, val loss: 0.0122274842\n",
            "[INFO] epoch: 125, train loss: 0.0001567436, val loss: 0.0120300822\n",
            "[INFO] epoch: 126, train loss: 0.0001531494, val loss: 0.0118946699\n",
            "[INFO] epoch: 127, train loss: 0.0001524659, val loss: 0.0116730614\n",
            "[INFO] epoch: 128, train loss: 0.0001477396, val loss: 0.0115757561\n",
            "[INFO] epoch: 129, train loss: 0.0001493746, val loss: 0.0113027267\n",
            "[INFO] epoch: 130, train loss: 0.0001423072, val loss: 0.0112855576\n",
            "[INFO] epoch: 131, train loss: 0.0001498366, val loss: 0.0109029844\n",
            "[INFO] epoch: 132, train loss: 0.0001390269, val loss: 0.0110618627\n",
            "[INFO] epoch: 133, train loss: 0.0001639847, val loss: 0.0104468476\n",
            "[INFO] epoch: 134, train loss: 0.0001546871, val loss: 0.0110200170\n",
            "[INFO] epoch: 135, train loss: 0.0002406299, val loss: 0.0099182106\n",
            "[INFO] epoch: 136, train loss: 0.0003003008, val loss: 0.0116226731\n",
            "[INFO] epoch: 137, train loss: 0.0006120353, val loss: 0.0094213287\n",
            "[INFO] epoch: 138, train loss: 0.0012154625, val loss: 0.0151958253\n",
            "[INFO] epoch: 139, train loss: 0.0019447523, val loss: 0.0088068349\n",
            "[INFO] epoch: 140, train loss: 0.0043045276, val loss: 0.0284105762\n",
            "[INFO] epoch: 141, train loss: 0.0028317986, val loss: 0.0070864465\n",
            "[INFO] epoch: 142, train loss: 0.0035810722, val loss: 0.0320938868\n",
            "[INFO] epoch: 143, train loss: 0.0014150745, val loss: 0.0070460207\n",
            "[INFO] epoch: 144, train loss: 0.0011051082, val loss: 0.0225180579\n",
            "[INFO] epoch: 145, train loss: 0.0005811264, val loss: 0.0086106021\n",
            "[INFO] epoch: 146, train loss: 0.0003406300, val loss: 0.0161474831\n",
            "[INFO] epoch: 147, train loss: 0.0002822486, val loss: 0.0102566313\n",
            "[INFO] epoch: 148, train loss: 0.0002013991, val loss: 0.0135423186\n",
            "[INFO] epoch: 149, train loss: 0.0002014201, val loss: 0.0107391229\n",
            "[INFO] epoch: 150, train loss: 0.0001671044, val loss: 0.0124350914\n",
            "[INFO] epoch: 151, train loss: 0.0001700551, val loss: 0.0107304900\n",
            "[INFO] epoch: 152, train loss: 0.0001503835, val loss: 0.0118015333\n",
            "[INFO] epoch: 153, train loss: 0.0001537775, val loss: 0.0105469844\n",
            "[INFO] epoch: 154, train loss: 0.0001398577, val loss: 0.0113815496\n",
            "[INFO] epoch: 155, train loss: 0.0001437838, val loss: 0.0103035276\n",
            "[INFO] epoch: 156, train loss: 0.0001324868, val loss: 0.0110671731\n",
            "[INFO] epoch: 157, train loss: 0.0001375490, val loss: 0.0100309759\n",
            "[INFO] epoch: 158, train loss: 0.0001274085, val loss: 0.0108261481\n",
            "[INFO] epoch: 159, train loss: 0.0001344378, val loss: 0.0097311473\n",
            "[INFO] epoch: 160, train loss: 0.0001246991, val loss: 0.0106510880\n",
            "[INFO] epoch: 161, train loss: 0.0001351230, val loss: 0.0093951438\n",
            "[INFO] epoch: 162, train loss: 0.0001257057, val loss: 0.0105557224\n",
            "[INFO] epoch: 163, train loss: 0.0001422907, val loss: 0.0090054377\n",
            "[INFO] epoch: 164, train loss: 0.0001347038, val loss: 0.0105808570\n",
            "[INFO] epoch: 165, train loss: 0.0001631282, val loss: 0.0085369274\n",
            "[INFO] epoch: 166, train loss: 0.0001638439, val loss: 0.0108164704\n",
            "[INFO] epoch: 167, train loss: 0.0002156505, val loss: 0.0079614852\n",
            "[INFO] epoch: 168, train loss: 0.0002465218, val loss: 0.0114538955\n",
            "[INFO] epoch: 169, train loss: 0.0003403887, val loss: 0.0072682104\n",
            "[INFO] epoch: 170, train loss: 0.0004628326, val loss: 0.0128734481\n",
            "[INFO] epoch: 171, train loss: 0.0005957120, val loss: 0.0065086907\n",
            "[INFO] epoch: 172, train loss: 0.0009131445, val loss: 0.0155267621\n",
            "[INFO] epoch: 173, train loss: 0.0009278917, val loss: 0.0058468363\n",
            "[INFO] epoch: 174, train loss: 0.0013723155, val loss: 0.0184901185\n",
            "[INFO] epoch: 175, train loss: 0.0010092348, val loss: 0.0055823424\n",
            "[INFO] epoch: 176, train loss: 0.0012135136, val loss: 0.0186172842\n",
            "[INFO] epoch: 177, train loss: 0.0007672334, val loss: 0.0058674252\n",
            "[INFO] epoch: 178, train loss: 0.0007342887, val loss: 0.0163386658\n",
            "[INFO] epoch: 179, train loss: 0.0004968167, val loss: 0.0064850227\n",
            "[INFO] epoch: 180, train loss: 0.0004123728, val loss: 0.0140026588\n",
            "[INFO] epoch: 181, train loss: 0.0003257093, val loss: 0.0070768174\n",
            "[INFO] epoch: 182, train loss: 0.0002611619, val loss: 0.0123734414\n",
            "[INFO] epoch: 183, train loss: 0.0002378900, val loss: 0.0074275499\n",
            "[INFO] epoch: 184, train loss: 0.0001960231, val loss: 0.0113576590\n",
            "[INFO] epoch: 185, train loss: 0.0001951043, val loss: 0.0075497435\n",
            "[INFO] epoch: 186, train loss: 0.0001669969, val loss: 0.0107182261\n",
            "[INFO] epoch: 187, train loss: 0.0001748231, val loss: 0.0075165613\n",
            "[INFO] epoch: 188, train loss: 0.0001547291, val loss: 0.0103111081\n",
            "[INFO] epoch: 189, train loss: 0.0001673406, val loss: 0.0073813926\n",
            "[INFO] epoch: 190, train loss: 0.0001526280, val loss: 0.0100641841\n",
            "[INFO] epoch: 191, train loss: 0.0001694834, val loss: 0.0071732002\n",
            "[INFO] epoch: 192, train loss: 0.0001597485, val loss: 0.0099513756\n",
            "[INFO] epoch: 193, train loss: 0.0001817929, val loss: 0.0069050187\n",
            "[INFO] epoch: 194, train loss: 0.0001788337, val loss: 0.0099775155\n",
            "[INFO] epoch: 195, train loss: 0.0002077607, val loss: 0.0065830389\n",
            "[INFO] epoch: 196, train loss: 0.0002164717, val loss: 0.0101723995\n",
            "[INFO] epoch: 197, train loss: 0.0002536083, val loss: 0.0062146213\n",
            "[INFO] epoch: 198, train loss: 0.0002832991, val loss: 0.0105848711\n",
            "[INFO] epoch: 199, train loss: 0.0003261703, val loss: 0.0058178090\n",
            "[INFO] epoch: 200, train loss: 0.0003897390, val loss: 0.0112563978\n",
            "[INFO] epoch: 201, train loss: 0.0004243651, val loss: 0.0054311283\n",
            "[INFO] epoch: 202, train loss: 0.0005285117, val loss: 0.0121376664\n",
            "[INFO] epoch: 203, train loss: 0.0005234509, val loss: 0.0051164314\n",
            "[INFO] epoch: 204, train loss: 0.0006480401, val loss: 0.0129592070\n",
            "[INFO] epoch: 205, train loss: 0.0005747919, val loss: 0.0049420645\n",
            "[INFO] epoch: 206, train loss: 0.0006723809, val loss: 0.0132897063\n",
            "[INFO] epoch: 207, train loss: 0.0005492806, val loss: 0.0049415348\n",
            "[INFO] epoch: 208, train loss: 0.0005903203, val loss: 0.0129491168\n",
            "[INFO] epoch: 209, train loss: 0.0004709161, val loss: 0.0050812589\n",
            "[INFO] epoch: 210, train loss: 0.0004672763, val loss: 0.0121835670\n",
            "[INFO] epoch: 211, train loss: 0.0003837222, val loss: 0.0052790866\n",
            "[INFO] epoch: 212, train loss: 0.0003609538, val loss: 0.0113362769\n",
            "[INFO] epoch: 213, train loss: 0.0003135749, val loss: 0.0054532430\n",
            "[INFO] epoch: 214, train loss: 0.0002883181, val loss: 0.0106039507\n",
            "[INFO] epoch: 215, train loss: 0.0002659043, val loss: 0.0055574977\n",
            "[INFO] epoch: 216, train loss: 0.0002445585, val loss: 0.0100413418\n",
            "[INFO] epoch: 217, train loss: 0.0002372467, val loss: 0.0055817966\n",
            "[INFO] epoch: 218, train loss: 0.0002212783, val loss: 0.0096389162\n",
            "[INFO] epoch: 219, train loss: 0.0002230733, val loss: 0.0055347273\n",
            "[INFO] epoch: 220, train loss: 0.0002126542, val loss: 0.0093737700\n",
            "[INFO] epoch: 221, train loss: 0.0002203701, val loss: 0.0054295832\n",
            "[INFO] epoch: 222, train loss: 0.0002158903, val loss: 0.0092283059\n",
            "[INFO] epoch: 223, train loss: 0.0002278063, val loss: 0.0052789949\n",
            "[INFO] epoch: 224, train loss: 0.0002303885, val loss: 0.0091935594\n",
            "[INFO] epoch: 225, train loss: 0.0002451721, val loss: 0.0050947255\n",
            "[INFO] epoch: 226, train loss: 0.0002567424, val loss: 0.0092660086\n",
            "[INFO] epoch: 227, train loss: 0.0002724499, val loss: 0.0048894022\n",
            "[INFO] epoch: 228, train loss: 0.0002952977, val loss: 0.0094401465\n",
            "[INFO] epoch: 229, train loss: 0.0003083341, val loss: 0.0046786837\n",
            "[INFO] epoch: 230, train loss: 0.0003437980, val loss: 0.0096960612\n",
            "[INFO] epoch: 231, train loss: 0.0003483432, val loss: 0.0044819793\n",
            "[INFO] epoch: 232, train loss: 0.0003945741, val loss: 0.0099848479\n",
            "[INFO] epoch: 233, train loss: 0.0003838577, val loss: 0.0043205431\n",
            "[INFO] epoch: 234, train loss: 0.0004341566, val loss: 0.0102248537\n",
            "[INFO] epoch: 235, train loss: 0.0004044945, val loss: 0.0042119750\n",
            "[INFO] epoch: 236, train loss: 0.0004490330, val loss: 0.0103272838\n",
            "[INFO] epoch: 237, train loss: 0.0004039286, val loss: 0.0041630441\n",
            "[INFO] epoch: 238, train loss: 0.0004350070, val loss: 0.0102448431\n",
            "[INFO] epoch: 239, train loss: 0.0003841328, val loss: 0.0041655714\n",
            "[INFO] epoch: 240, train loss: 0.0004001362, val loss: 0.0099989185\n",
            "[INFO] epoch: 241, train loss: 0.0003534222, val loss: 0.0041991571\n",
            "[INFO] epoch: 242, train loss: 0.0003578642, val loss: 0.0096576507\n",
            "[INFO] epoch: 243, train loss: 0.0003209090, val loss: 0.0042393934\n",
            "[INFO] epoch: 244, train loss: 0.0003189173, val loss: 0.0092936736\n",
            "[INFO] epoch: 245, train loss: 0.0002927676, val loss: 0.0042659581\n",
            "[INFO] epoch: 246, train loss: 0.0002886043, val loss: 0.0089584070\n",
            "[INFO] epoch: 247, train loss: 0.0002717930, val loss: 0.0042666979\n",
            "[INFO] epoch: 248, train loss: 0.0002682488, val loss: 0.0086789325\n",
            "[INFO] epoch: 249, train loss: 0.0002585916, val loss: 0.0042373290\n",
            "[INFO] epoch: 250, train loss: 0.0002573566, val loss: 0.0084654628\n",
            "[INFO] epoch: 251, train loss: 0.0002527945, val loss: 0.0041790045\n",
            "[INFO] epoch: 252, train loss: 0.0002549751, val loss: 0.0083191960\n",
            "[INFO] epoch: 253, train loss: 0.0002537399, val loss: 0.0040960629\n",
            "[INFO] epoch: 254, train loss: 0.0002601941, val loss: 0.0082370717\n",
            "[INFO] epoch: 255, train loss: 0.0002606606, val loss: 0.0039946123\n",
            "[INFO] epoch: 256, train loss: 0.0002720858, val loss: 0.0082131682\n",
            "[INFO] epoch: 257, train loss: 0.0002725468, val loss: 0.0038819260\n",
            "[INFO] epoch: 258, train loss: 0.0002893406, val loss: 0.0082379067\n",
            "[INFO] epoch: 259, train loss: 0.0002878594, val loss: 0.0037661035\n",
            "[INFO] epoch: 260, train loss: 0.0003098122, val loss: 0.0082962097\n",
            "[INFO] epoch: 261, train loss: 0.0003043087, val loss: 0.0036556332\n",
            "[INFO] epoch: 262, train loss: 0.0003302692, val loss: 0.0083664539\n",
            "[INFO] epoch: 263, train loss: 0.0003189472, val loss: 0.0035584030\n",
            "[INFO] epoch: 264, train loss: 0.0003467401, val loss: 0.0084221195\n",
            "[INFO] epoch: 265, train loss: 0.0003287903, val loss: 0.0034803500\n",
            "[INFO] epoch: 266, train loss: 0.0003556473, val loss: 0.0084375326\n",
            "[INFO] epoch: 267, train loss: 0.0003318083, val loss: 0.0034240498\n",
            "[INFO] epoch: 268, train loss: 0.0003552364, val loss: 0.0083958992\n",
            "[INFO] epoch: 269, train loss: 0.0003277180, val loss: 0.0033879691\n",
            "[INFO] epoch: 270, train loss: 0.0003463183, val loss: 0.0082949895\n",
            "[INFO] epoch: 271, train loss: 0.0003179991, val loss: 0.0033669099\n",
            "[INFO] epoch: 272, train loss: 0.0003317293, val loss: 0.0081465149\n",
            "[INFO] epoch: 273, train loss: 0.0003051607, val loss: 0.0033535564\n",
            "[INFO] epoch: 274, train loss: 0.0003150068, val loss: 0.0079702972\n",
            "[INFO] epoch: 275, train loss: 0.0002917922, val loss: 0.0033404047\n",
            "[INFO] epoch: 276, train loss: 0.0002991981, val loss: 0.0077871400\n",
            "[INFO] epoch: 277, train loss: 0.0002799235, val loss: 0.0033213583\n",
            "[INFO] epoch: 278, train loss: 0.0002863350, val loss: 0.0076141181\n",
            "[INFO] epoch: 279, train loss: 0.0002708242, val loss: 0.0032925611\n",
            "[INFO] epoch: 280, train loss: 0.0002774598, val loss: 0.0074627655\n",
            "[INFO] epoch: 281, train loss: 0.0002650902, val loss: 0.0032524664\n",
            "[INFO] epoch: 282, train loss: 0.0002728913, val loss: 0.0073393710\n",
            "[INFO] epoch: 283, train loss: 0.0002628230, val loss: 0.0032014694\n",
            "[INFO] epoch: 284, train loss: 0.0002724711, val loss: 0.0072459992\n",
            "[INFO] epoch: 285, train loss: 0.0002637689, val loss: 0.0031414295\n",
            "[INFO] epoch: 286, train loss: 0.0002757039, val loss: 0.0071813529\n",
            "[INFO] epoch: 287, train loss: 0.0002673927, val loss: 0.0030751895\n",
            "[INFO] epoch: 288, train loss: 0.0002817977, val loss: 0.0071413196\n",
            "[INFO] epoch: 289, train loss: 0.0002729042, val loss: 0.0030061376\n",
            "[INFO] epoch: 290, train loss: 0.0002896661, val loss: 0.0071192848\n",
            "[INFO] epoch: 291, train loss: 0.0002792901, val loss: 0.0029378086\n",
            "[INFO] epoch: 292, train loss: 0.0002979674, val loss: 0.0071065977\n",
            "[INFO] epoch: 293, train loss: 0.0002854010, val loss: 0.0028734807\n",
            "[INFO] epoch: 294, train loss: 0.0003052489, val loss: 0.0070934413\n",
            "[INFO] epoch: 295, train loss: 0.0002901232, val loss: 0.0028156827\n",
            "[INFO] epoch: 296, train loss: 0.0003102093, val loss: 0.0070704084\n",
            "[INFO] epoch: 297, train loss: 0.0002926015, val loss: 0.0027658682\n",
            "[INFO] epoch: 298, train loss: 0.0003119965, val loss: 0.0070302744\n",
            "[INFO] epoch: 299, train loss: 0.0002924371, val loss: 0.0027241791\n",
            "[INFO] epoch: 300, train loss: 0.0003104252, val loss: 0.0069695347\n",
            "[INFO] epoch: 301, train loss: 0.0002897640, val loss: 0.0026894920\n",
            "[INFO] epoch: 302, train loss: 0.0003059822, val loss: 0.0068889212\n",
            "[INFO] epoch: 303, train loss: 0.0002851700, val loss: 0.0026597525\n",
            "[INFO] epoch: 304, train loss: 0.0002996342, val loss: 0.0067927655\n",
            "[INFO] epoch: 305, train loss: 0.0002795028, val loss: 0.0026324241\n",
            "[INFO] epoch: 306, train loss: 0.0002925297, val loss: 0.0066875369\n",
            "[INFO] epoch: 307, train loss: 0.0002736518, val loss: 0.0026050377\n",
            "[INFO] epoch: 308, train loss: 0.0002857353, val loss: 0.0065802186\n",
            "[INFO] epoch: 309, train loss: 0.0002683771, val loss: 0.0025755764\n",
            "[INFO] epoch: 310, train loss: 0.0002800661, val loss: 0.0064769851\n",
            "[INFO] epoch: 311, train loss: 0.0002642209, val loss: 0.0025427391\n",
            "[INFO] epoch: 312, train loss: 0.0002760324, val loss: 0.0063824656\n",
            "[INFO] epoch: 313, train loss: 0.0002614839, val loss: 0.0025059948\n",
            "[INFO] epoch: 314, train loss: 0.0002738464, val loss: 0.0062994487\n",
            "[INFO] epoch: 315, train loss: 0.0002602385, val loss: 0.0024655369\n",
            "[INFO] epoch: 316, train loss: 0.0002734627, val loss: 0.0062289030\n",
            "[INFO] epoch: 317, train loss: 0.0002603635, val loss: 0.0024221204\n",
            "[INFO] epoch: 318, train loss: 0.0002746257, val loss: 0.0061701831\n",
            "[INFO] epoch: 319, train loss: 0.0002615788, val loss: 0.0023768955\n",
            "[INFO] epoch: 320, train loss: 0.0002769116, val loss: 0.0061212668\n",
            "[INFO] epoch: 321, train loss: 0.0002634875, val loss: 0.0023311917\n",
            "[INFO] epoch: 322, train loss: 0.0002797798, val loss: 0.0060790731\n",
            "[INFO] epoch: 323, train loss: 0.0002656250, val loss: 0.0022863466\n",
            "[INFO] epoch: 324, train loss: 0.0002826406, val loss: 0.0060399501\n",
            "[INFO] epoch: 325, train loss: 0.0002675238, val loss: 0.0022434960\n",
            "[INFO] epoch: 326, train loss: 0.0002849368, val loss: 0.0060001483\n",
            "[INFO] epoch: 327, train loss: 0.0002687819, val loss: 0.0022034625\n",
            "[INFO] epoch: 328, train loss: 0.0002862341, val loss: 0.0059564050\n",
            "[INFO] epoch: 329, train loss: 0.0002691299, val loss: 0.0021666155\n",
            "[INFO] epoch: 330, train loss: 0.0002862939, val loss: 0.0059064680\n",
            "[INFO] epoch: 331, train loss: 0.0002684688, val loss: 0.0021328853\n",
            "[INFO] epoch: 332, train loss: 0.0002851018, val loss: 0.0058493824\n",
            "[INFO] epoch: 333, train loss: 0.0002668749, val loss: 0.0021018155\n",
            "[INFO] epoch: 334, train loss: 0.0002828549, val loss: 0.0057855529\n",
            "[INFO] epoch: 335, train loss: 0.0002645680, val loss: 0.0020726614\n",
            "[INFO] epoch: 336, train loss: 0.0002798985, val loss: 0.0057164956\n",
            "[INFO] epoch: 337, train loss: 0.0002618553, val loss: 0.0020445492\n",
            "[INFO] epoch: 338, train loss: 0.0002766444, val loss: 0.0056444357\n",
            "[INFO] epoch: 339, train loss: 0.0002590643, val loss: 0.0020166406\n",
            "[INFO] epoch: 340, train loss: 0.0002734920, val loss: 0.0055718331\n",
            "[INFO] epoch: 341, train loss: 0.0002564903, val loss: 0.0019882437\n",
            "[INFO] epoch: 342, train loss: 0.0002707681, val loss: 0.0055009636\n",
            "[INFO] epoch: 343, train loss: 0.0002543552, val loss: 0.0019589058\n",
            "[INFO] epoch: 344, train loss: 0.0002686915, val loss: 0.0054336052\n",
            "[INFO] epoch: 345, train loss: 0.0002527905, val loss: 0.0019284429\n",
            "[INFO] epoch: 346, train loss: 0.0002673655, val loss: 0.0053709010\n",
            "[INFO] epoch: 347, train loss: 0.0002518343, val loss: 0.0018969184\n",
            "[INFO] epoch: 348, train loss: 0.0002667776, val loss: 0.0053132602\n",
            "[INFO] epoch: 349, train loss: 0.0002514361, val loss: 0.0018646041\n",
            "[INFO] epoch: 350, train loss: 0.0002668175, val loss: 0.0052604377\n",
            "[INFO] epoch: 351, train loss: 0.0002514755, val loss: 0.0018319170\n",
            "[INFO] epoch: 352, train loss: 0.0002673019, val loss: 0.0052116104\n",
            "[INFO] epoch: 353, train loss: 0.0002517853, val loss: 0.0017993395\n",
            "[INFO] epoch: 354, train loss: 0.0002680054, val loss: 0.0051655926\n",
            "[INFO] epoch: 355, train loss: 0.0002521763, val loss: 0.0017673332\n",
            "[INFO] epoch: 356, train loss: 0.0002686895, val loss: 0.0051209679\n",
            "[INFO] epoch: 357, train loss: 0.0002524649, val loss: 0.0017362965\n",
            "[INFO] epoch: 358, train loss: 0.0002691428, val loss: 0.0050763597\n",
            "[INFO] epoch: 359, train loss: 0.0002525017, val loss: 0.0017064989\n",
            "[INFO] epoch: 360, train loss: 0.0002692081, val loss: 0.0050306052\n",
            "[INFO] epoch: 361, train loss: 0.0002521891, val loss: 0.0016780493\n",
            "[INFO] epoch: 362, train loss: 0.0002688006, val loss: 0.0049829085\n",
            "[INFO] epoch: 363, train loss: 0.0002514932, val loss: 0.0016509229\n",
            "[INFO] epoch: 364, train loss: 0.0002679203, val loss: 0.0049329441\n",
            "[INFO] epoch: 365, train loss: 0.0002504434, val loss: 0.0016249464\n",
            "[INFO] epoch: 366, train loss: 0.0002666380, val loss: 0.0048808346\n",
            "[INFO] epoch: 367, train loss: 0.0002491188, val loss: 0.0015998711\n",
            "[INFO] epoch: 368, train loss: 0.0002650772, val loss: 0.0048270846\n",
            "[INFO] epoch: 369, train loss: 0.0002476318, val loss: 0.0015754005\n",
            "[INFO] epoch: 370, train loss: 0.0002633906, val loss: 0.0047724636\n",
            "[INFO] epoch: 371, train loss: 0.0002461055, val loss: 0.0015512480\n",
            "[INFO] epoch: 372, train loss: 0.0002617288, val loss: 0.0047178357\n",
            "[INFO] epoch: 373, train loss: 0.0002446541, val loss: 0.0015271738\n",
            "[INFO] epoch: 374, train loss: 0.0002602227, val loss: 0.0046640405\n",
            "[INFO] epoch: 375, train loss: 0.0002433680, val loss: 0.0015030150\n",
            "[INFO] epoch: 376, train loss: 0.0002589628, val loss: 0.0046117356\n",
            "[INFO] epoch: 377, train loss: 0.0002423029, val loss: 0.0014786994\n",
            "[INFO] epoch: 378, train loss: 0.0002579965, val loss: 0.0045613897\n",
            "[INFO] epoch: 379, train loss: 0.0002414779, val loss: 0.0014542393\n",
            "[INFO] epoch: 380, train loss: 0.0002573224, val loss: 0.0045131663\n",
            "[INFO] epoch: 381, train loss: 0.0002408756, val loss: 0.0014297234\n",
            "[INFO] epoch: 382, train loss: 0.0002569004, val loss: 0.0044670085\n",
            "[INFO] epoch: 383, train loss: 0.0002404509, val loss: 0.0014052912\n",
            "[INFO] epoch: 384, train loss: 0.0002566608, val loss: 0.0044226246\n",
            "[INFO] epoch: 385, train loss: 0.0002401402, val loss: 0.0013810971\n",
            "[INFO] epoch: 386, train loss: 0.0002565154, val loss: 0.0043795836\n",
            "[INFO] epoch: 387, train loss: 0.0002398689, val loss: 0.0013573004\n",
            "[INFO] epoch: 388, train loss: 0.0002563713, val loss: 0.0043373746\n",
            "[INFO] epoch: 389, train loss: 0.0002395658, val loss: 0.0013340341\n",
            "[INFO] epoch: 390, train loss: 0.0002561462, val loss: 0.0042954871\n",
            "[INFO] epoch: 391, train loss: 0.0002391730, val loss: 0.0013113910\n",
            "[INFO] epoch: 392, train loss: 0.0002557795, val loss: 0.0042534938\n",
            "[INFO] epoch: 393, train loss: 0.0002386525, val loss: 0.0012894116\n",
            "[INFO] epoch: 394, train loss: 0.0002552386, val loss: 0.0042111092\n",
            "[INFO] epoch: 395, train loss: 0.0002379903, val loss: 0.0012680811\n",
            "[INFO] epoch: 396, train loss: 0.0002545203, val loss: 0.0041681847\n",
            "[INFO] epoch: 397, train loss: 0.0002371946, val loss: 0.0012473503\n",
            "[INFO] epoch: 398, train loss: 0.0002536477, val loss: 0.0041247554\n",
            "[INFO] epoch: 399, train loss: 0.0002362922, val loss: 0.0012271337\n",
            "[INFO] epoch: 400, train loss: 0.0002526646, val loss: 0.0040809678\n",
            "[INFO] epoch: 401, train loss: 0.0002353233, val loss: 0.0012073344\n",
            "[INFO] epoch: 402, train loss: 0.0002516254, val loss: 0.0040370784\n",
            "[INFO] epoch: 403, train loss: 0.0002343341, val loss: 0.0011878546\n",
            "[INFO] epoch: 404, train loss: 0.0002505881, val loss: 0.0039933870\n",
            "[INFO] epoch: 405, train loss: 0.0002333687, val loss: 0.0011686117\n",
            "[INFO] epoch: 406, train loss: 0.0002496038, val loss: 0.0039501942\n",
            "[INFO] epoch: 407, train loss: 0.0002324623, val loss: 0.0011495476\n",
            "[INFO] epoch: 408, train loss: 0.0002487091, val loss: 0.0039077377\n",
            "[INFO] epoch: 409, train loss: 0.0002316396, val loss: 0.0011306254\n",
            "[INFO] epoch: 410, train loss: 0.0002479267, val loss: 0.0038662109\n",
            "[INFO] epoch: 411, train loss: 0.0002309096, val loss: 0.0011118497\n",
            "[INFO] epoch: 412, train loss: 0.0002472581, val loss: 0.0038256839\n",
            "[INFO] epoch: 413, train loss: 0.0002302692, val loss: 0.0010932357\n",
            "[INFO] epoch: 414, train loss: 0.0002466913, val loss: 0.0037861487\n",
            "[INFO] epoch: 415, train loss: 0.0002297019, val loss: 0.0010748312\n",
            "[INFO] epoch: 416, train loss: 0.0002462000, val loss: 0.0037475062\n",
            "[INFO] epoch: 417, train loss: 0.0002291833, val loss: 0.0010566817\n",
            "[INFO] epoch: 418, train loss: 0.0002457518, val loss: 0.0037096132\n",
            "[INFO] epoch: 419, train loss: 0.0002286875, val loss: 0.0010388360\n",
            "[INFO] epoch: 420, train loss: 0.0002453128, val loss: 0.0036722959\n",
            "[INFO] epoch: 421, train loss: 0.0002281867, val loss: 0.0010213390\n",
            "[INFO] epoch: 422, train loss: 0.0002448511, val loss: 0.0036353641\n",
            "[INFO] epoch: 423, train loss: 0.0002276593, val loss: 0.0010042152\n",
            "[INFO] epoch: 424, train loss: 0.0002443428, val loss: 0.0035986730\n",
            "[INFO] epoch: 425, train loss: 0.0002270878, val loss: 0.0009874841\n",
            "[INFO] epoch: 426, train loss: 0.0002437714, val loss: 0.0035620838\n",
            "[INFO] epoch: 427, train loss: 0.0002264651, val loss: 0.0009711446\n",
            "[INFO] epoch: 428, train loss: 0.0002431350, val loss: 0.0035255509\n",
            "[INFO] epoch: 429, train loss: 0.0002257935, val loss: 0.0009551758\n",
            "[INFO] epoch: 430, train loss: 0.0002424399, val loss: 0.0034890624\n",
            "[INFO] epoch: 431, train loss: 0.0002250811, val loss: 0.0009395508\n",
            "[INFO] epoch: 432, train loss: 0.0002417013, val loss: 0.0034526623\n",
            "[INFO] epoch: 433, train loss: 0.0002243428, val loss: 0.0009242381\n",
            "[INFO] epoch: 434, train loss: 0.0002409396, val loss: 0.0034164372\n",
            "[INFO] epoch: 435, train loss: 0.0002235955, val loss: 0.0009092060\n",
            "[INFO] epoch: 436, train loss: 0.0002401759, val loss: 0.0033804842\n",
            "[INFO] epoch: 437, train loss: 0.0002228556, val loss: 0.0008944217\n",
            "[INFO] epoch: 438, train loss: 0.0002394291, val loss: 0.0033449041\n",
            "[INFO] epoch: 439, train loss: 0.0002221367, val loss: 0.0008798634\n",
            "[INFO] epoch: 440, train loss: 0.0002387146, val loss: 0.0033097815\n",
            "[INFO] epoch: 441, train loss: 0.0002214486, val loss: 0.0008655192\n",
            "[INFO] epoch: 442, train loss: 0.0002380413, val loss: 0.0032751869\n",
            "[INFO] epoch: 443, train loss: 0.0002207972, val loss: 0.0008513803\n",
            "[INFO] epoch: 444, train loss: 0.0002374137, val loss: 0.0032411669\n",
            "[INFO] epoch: 445, train loss: 0.0002201814, val loss: 0.0008374523\n",
            "[INFO] epoch: 446, train loss: 0.0002368257, val loss: 0.0032076946\n",
            "[INFO] epoch: 447, train loss: 0.0002195963, val loss: 0.0008237474\n",
            "[INFO] epoch: 448, train loss: 0.0002362710, val loss: 0.0031747738\n",
            "[INFO] epoch: 449, train loss: 0.0002190346, val loss: 0.0008102710\n",
            "[INFO] epoch: 450, train loss: 0.0002357379, val loss: 0.0031423365\n",
            "[INFO] epoch: 451, train loss: 0.0002184859, val loss: 0.0007970415\n",
            "[INFO] epoch: 452, train loss: 0.0002352130, val loss: 0.0031103299\n",
            "[INFO] epoch: 453, train loss: 0.0002179398, val loss: 0.0007840698\n",
            "[INFO] epoch: 454, train loss: 0.0002346832, val loss: 0.0030786833\n",
            "[INFO] epoch: 455, train loss: 0.0002173868, val loss: 0.0007713662\n",
            "[INFO] epoch: 456, train loss: 0.0002341404, val loss: 0.0030473397\n",
            "[INFO] epoch: 457, train loss: 0.0002168211, val loss: 0.0007589304\n",
            "[INFO] epoch: 458, train loss: 0.0002335761, val loss: 0.0030162474\n",
            "[INFO] epoch: 459, train loss: 0.0002162384, val loss: 0.0007467661\n",
            "[INFO] epoch: 460, train loss: 0.0002329896, val loss: 0.0029853781\n",
            "[INFO] epoch: 461, train loss: 0.0002156399, val loss: 0.0007348655\n",
            "[INFO] epoch: 462, train loss: 0.0002323839, val loss: 0.0029547375\n",
            "[INFO] epoch: 463, train loss: 0.0002150279, val loss: 0.0007232135\n",
            "[INFO] epoch: 464, train loss: 0.0002317613, val loss: 0.0029243229\n",
            "[INFO] epoch: 465, train loss: 0.0002144056, val loss: 0.0007118075\n",
            "[INFO] epoch: 466, train loss: 0.0002311288, val loss: 0.0028941523\n",
            "[INFO] epoch: 467, train loss: 0.0002137802, val loss: 0.0007006304\n",
            "[INFO] epoch: 468, train loss: 0.0002304960, val loss: 0.0028642708\n",
            "[INFO] epoch: 469, train loss: 0.0002131584, val loss: 0.0006896670\n",
            "[INFO] epoch: 470, train loss: 0.0002298697, val loss: 0.0028347047\n",
            "[INFO] epoch: 471, train loss: 0.0002125452, val loss: 0.0006789155\n",
            "[INFO] epoch: 472, train loss: 0.0002292555, val loss: 0.0028054824\n",
            "[INFO] epoch: 473, train loss: 0.0002119443, val loss: 0.0006683650\n",
            "[INFO] epoch: 474, train loss: 0.0002286574, val loss: 0.0027766309\n",
            "[INFO] epoch: 475, train loss: 0.0002113581, val loss: 0.0006580125\n",
            "[INFO] epoch: 476, train loss: 0.0002280769, val loss: 0.0027481639\n",
            "[INFO] epoch: 477, train loss: 0.0002107872, val loss: 0.0006478543\n",
            "[INFO] epoch: 478, train loss: 0.0002275140, val loss: 0.0027200792\n",
            "[INFO] epoch: 479, train loss: 0.0002102299, val loss: 0.0006378929\n",
            "[INFO] epoch: 480, train loss: 0.0002269647, val loss: 0.0026923659\n",
            "[INFO] epoch: 481, train loss: 0.0002096835, val loss: 0.0006281303\n",
            "[INFO] epoch: 482, train loss: 0.0002264263, val loss: 0.0026650151\n",
            "[INFO] epoch: 483, train loss: 0.0002091444, val loss: 0.0006185660\n",
            "[INFO] epoch: 484, train loss: 0.0002258934, val loss: 0.0026380006\n",
            "[INFO] epoch: 485, train loss: 0.0002086096, val loss: 0.0006092042\n",
            "[INFO] epoch: 486, train loss: 0.0002253623, val loss: 0.0026113011\n",
            "[INFO] epoch: 487, train loss: 0.0002080752, val loss: 0.0006000451\n",
            "[INFO] epoch: 488, train loss: 0.0002248287, val loss: 0.0025848929\n",
            "[INFO] epoch: 489, train loss: 0.0002075381, val loss: 0.0005910897\n",
            "[INFO] epoch: 490, train loss: 0.0002242896, val loss: 0.0025587516\n",
            "[INFO] epoch: 491, train loss: 0.0002069979, val loss: 0.0005823323\n",
            "[INFO] epoch: 492, train loss: 0.0002237454, val loss: 0.0025328754\n",
            "[INFO] epoch: 493, train loss: 0.0002064537, val loss: 0.0005737722\n",
            "[INFO] epoch: 494, train loss: 0.0002231946, val loss: 0.0025072500\n",
            "[INFO] epoch: 495, train loss: 0.0002059061, val loss: 0.0005654040\n",
            "[INFO] epoch: 496, train loss: 0.0002226398, val loss: 0.0024818828\n",
            "[INFO] epoch: 497, train loss: 0.0002053564, val loss: 0.0005572262\n",
            "[INFO] epoch: 498, train loss: 0.0002220821, val loss: 0.0024567688\n",
            "[INFO] epoch: 499, train loss: 0.0002048068, val loss: 0.0005492308\n",
            "[INFO] epoch: 500, train loss: 0.0002215252, val loss: 0.0024319215\n",
            "[INFO] epoch: 501, train loss: 0.0002042594, val loss: 0.0005414125\n",
            "[INFO] epoch: 502, train loss: 0.0002209706, val loss: 0.0024073511\n",
            "[INFO] epoch: 503, train loss: 0.0002037156, val loss: 0.0005337670\n",
            "[INFO] epoch: 504, train loss: 0.0002204212, val loss: 0.0023830632\n",
            "[INFO] epoch: 505, train loss: 0.0002031775, val loss: 0.0005262910\n",
            "[INFO] epoch: 506, train loss: 0.0002198779, val loss: 0.0023590565\n",
            "[INFO] epoch: 507, train loss: 0.0002026457, val loss: 0.0005189820\n",
            "[INFO] epoch: 508, train loss: 0.0002193424, val loss: 0.0023353454\n",
            "[INFO] epoch: 509, train loss: 0.0002021214, val loss: 0.0005118356\n",
            "[INFO] epoch: 510, train loss: 0.0002188149, val loss: 0.0023119308\n",
            "[INFO] epoch: 511, train loss: 0.0002016037, val loss: 0.0005048507\n",
            "[INFO] epoch: 512, train loss: 0.0002182946, val loss: 0.0022888030\n",
            "[INFO] epoch: 513, train loss: 0.0002010929, val loss: 0.0004980248\n",
            "[INFO] epoch: 514, train loss: 0.0002177806, val loss: 0.0022659667\n",
            "[INFO] epoch: 515, train loss: 0.0002005868, val loss: 0.0004913574\n",
            "[INFO] epoch: 516, train loss: 0.0002172711, val loss: 0.0022434049\n",
            "[INFO] epoch: 517, train loss: 0.0002000844, val loss: 0.0004848473\n",
            "[INFO] epoch: 518, train loss: 0.0002167643, val loss: 0.0022211098\n",
            "[INFO] epoch: 519, train loss: 0.0001995850, val loss: 0.0004784941\n",
            "[INFO] epoch: 520, train loss: 0.0002162600, val loss: 0.0021990826\n",
            "[INFO] epoch: 521, train loss: 0.0001990871, val loss: 0.0004722944\n",
            "[INFO] epoch: 522, train loss: 0.0002157546, val loss: 0.0021772974\n",
            "[INFO] epoch: 523, train loss: 0.0001985888, val loss: 0.0004662477\n",
            "[INFO] epoch: 524, train loss: 0.0002152476, val loss: 0.0021557511\n",
            "[INFO] epoch: 525, train loss: 0.0001980897, val loss: 0.0004603533\n",
            "[INFO] epoch: 526, train loss: 0.0002147391, val loss: 0.0021344396\n",
            "[INFO] epoch: 527, train loss: 0.0001975911, val loss: 0.0004546058\n",
            "[INFO] epoch: 528, train loss: 0.0002142310, val loss: 0.0021133686\n",
            "[INFO] epoch: 529, train loss: 0.0001970934, val loss: 0.0004490030\n",
            "[INFO] epoch: 530, train loss: 0.0002137233, val loss: 0.0020925296\n",
            "[INFO] epoch: 531, train loss: 0.0001965978, val loss: 0.0004435412\n",
            "[INFO] epoch: 532, train loss: 0.0002132175, val loss: 0.0020719308\n",
            "[INFO] epoch: 533, train loss: 0.0001961041, val loss: 0.0004382179\n",
            "[INFO] epoch: 534, train loss: 0.0002127137, val loss: 0.0020515669\n",
            "[INFO] epoch: 535, train loss: 0.0001956132, val loss: 0.0004330302\n",
            "[INFO] epoch: 536, train loss: 0.0002122121, val loss: 0.0020314421\n",
            "[INFO] epoch: 537, train loss: 0.0001951252, val loss: 0.0004279763\n",
            "[INFO] epoch: 538, train loss: 0.0002117145, val loss: 0.0020115555\n",
            "[INFO] epoch: 539, train loss: 0.0001946411, val loss: 0.0004230526\n",
            "[INFO] epoch: 540, train loss: 0.0002112205, val loss: 0.0019919065\n",
            "[INFO] epoch: 541, train loss: 0.0001941609, val loss: 0.0004182587\n",
            "[INFO] epoch: 542, train loss: 0.0002107308, val loss: 0.0019724923\n",
            "[INFO] epoch: 543, train loss: 0.0001936851, val loss: 0.0004135891\n",
            "[INFO] epoch: 544, train loss: 0.0002102457, val loss: 0.0019533210\n",
            "[INFO] epoch: 545, train loss: 0.0001932134, val loss: 0.0004090426\n",
            "[INFO] epoch: 546, train loss: 0.0002097643, val loss: 0.0019343804\n",
            "[INFO] epoch: 547, train loss: 0.0001927453, val loss: 0.0004046200\n",
            "[INFO] epoch: 548, train loss: 0.0002092862, val loss: 0.0019156675\n",
            "[INFO] epoch: 549, train loss: 0.0001922801, val loss: 0.0004003162\n",
            "[INFO] epoch: 550, train loss: 0.0002088106, val loss: 0.0018971836\n",
            "[INFO] epoch: 551, train loss: 0.0001918171, val loss: 0.0003961320\n",
            "[INFO] epoch: 552, train loss: 0.0002083366, val loss: 0.0018789141\n",
            "[INFO] epoch: 553, train loss: 0.0001913567, val loss: 0.0003920636\n",
            "[INFO] epoch: 554, train loss: 0.0002078651, val loss: 0.0018608682\n",
            "[INFO] epoch: 555, train loss: 0.0001908980, val loss: 0.0003881109\n",
            "[INFO] epoch: 556, train loss: 0.0002073944, val loss: 0.0018430318\n",
            "[INFO] epoch: 557, train loss: 0.0001904407, val loss: 0.0003842703\n",
            "[INFO] epoch: 558, train loss: 0.0002069240, val loss: 0.0018253994\n",
            "[INFO] epoch: 559, train loss: 0.0001899841, val loss: 0.0003805410\n",
            "[INFO] epoch: 560, train loss: 0.0002064541, val loss: 0.0018079673\n",
            "[INFO] epoch: 561, train loss: 0.0001895293, val loss: 0.0003769205\n",
            "[INFO] epoch: 562, train loss: 0.0002059858, val loss: 0.0017907457\n",
            "[INFO] epoch: 563, train loss: 0.0001890763, val loss: 0.0003734062\n",
            "[INFO] epoch: 564, train loss: 0.0002055191, val loss: 0.0017737215\n",
            "[INFO] epoch: 565, train loss: 0.0001886257, val loss: 0.0003699966\n",
            "[INFO] epoch: 566, train loss: 0.0002050556, val loss: 0.0017569039\n",
            "[INFO] epoch: 567, train loss: 0.0001881777, val loss: 0.0003666882\n",
            "[INFO] epoch: 568, train loss: 0.0002045933, val loss: 0.0017402839\n",
            "[INFO] epoch: 569, train loss: 0.0001877317, val loss: 0.0003634801\n",
            "[INFO] epoch: 570, train loss: 0.0002041336, val loss: 0.0017238613\n",
            "[INFO] epoch: 571, train loss: 0.0001872889, val loss: 0.0003603703\n",
            "[INFO] epoch: 572, train loss: 0.0002036765, val loss: 0.0017076385\n",
            "[INFO] epoch: 573, train loss: 0.0001868484, val loss: 0.0003573566\n",
            "[INFO] epoch: 574, train loss: 0.0002032226, val loss: 0.0016916112\n",
            "[INFO] epoch: 575, train loss: 0.0001864115, val loss: 0.0003544371\n",
            "[INFO] epoch: 576, train loss: 0.0002027717, val loss: 0.0016757842\n",
            "[INFO] epoch: 577, train loss: 0.0001859767, val loss: 0.0003516094\n",
            "[INFO] epoch: 578, train loss: 0.0002023220, val loss: 0.0016601391\n",
            "[INFO] epoch: 579, train loss: 0.0001855441, val loss: 0.0003488734\n",
            "[INFO] epoch: 580, train loss: 0.0002018756, val loss: 0.0016446886\n",
            "[INFO] epoch: 581, train loss: 0.0001851150, val loss: 0.0003462254\n",
            "[INFO] epoch: 582, train loss: 0.0002014316, val loss: 0.0016294245\n",
            "[INFO] epoch: 583, train loss: 0.0001846876, val loss: 0.0003436646\n",
            "[INFO] epoch: 584, train loss: 0.0002009889, val loss: 0.0016143420\n",
            "[INFO] epoch: 585, train loss: 0.0001842620, val loss: 0.0003411896\n",
            "[INFO] epoch: 586, train loss: 0.0002005479, val loss: 0.0015994422\n",
            "[INFO] epoch: 587, train loss: 0.0001838383, val loss: 0.0003387988\n",
            "[INFO] epoch: 588, train loss: 0.0002001089, val loss: 0.0015847186\n",
            "[INFO] epoch: 589, train loss: 0.0001834175, val loss: 0.0003364893\n",
            "[INFO] epoch: 590, train loss: 0.0001996726, val loss: 0.0015701768\n",
            "[INFO] epoch: 591, train loss: 0.0001829982, val loss: 0.0003342606\n",
            "[INFO] epoch: 592, train loss: 0.0001992368, val loss: 0.0015558019\n",
            "[INFO] epoch: 593, train loss: 0.0001825805, val loss: 0.0003321111\n",
            "[INFO] epoch: 594, train loss: 0.0001988028, val loss: 0.0015416041\n",
            "[INFO] epoch: 595, train loss: 0.0001821649, val loss: 0.0003300381\n",
            "[INFO] epoch: 596, train loss: 0.0001983711, val loss: 0.0015275729\n",
            "[INFO] epoch: 597, train loss: 0.0001817515, val loss: 0.0003280400\n",
            "[INFO] epoch: 598, train loss: 0.0001979405, val loss: 0.0015137124\n",
            "[INFO] epoch: 599, train loss: 0.0001813395, val loss: 0.0003261170\n",
            "[INFO] epoch: 600, train loss: 0.0001975113, val loss: 0.0015000103\n",
            "[INFO] epoch: 601, train loss: 0.0001809294, val loss: 0.0003242659\n",
            "[INFO] epoch: 602, train loss: 0.0001970844, val loss: 0.0014864775\n",
            "[INFO] epoch: 603, train loss: 0.0001805216, val loss: 0.0003224847\n",
            "[INFO] epoch: 604, train loss: 0.0001966596, val loss: 0.0014731088\n",
            "[INFO] epoch: 605, train loss: 0.0001801160, val loss: 0.0003207727\n",
            "[INFO] epoch: 606, train loss: 0.0001962362, val loss: 0.0014598987\n",
            "[INFO] epoch: 607, train loss: 0.0001797121, val loss: 0.0003191290\n",
            "[INFO] epoch: 608, train loss: 0.0001958151, val loss: 0.0014468472\n",
            "[INFO] epoch: 609, train loss: 0.0001793108, val loss: 0.0003175511\n",
            "[INFO] epoch: 610, train loss: 0.0001953961, val loss: 0.0014339558\n",
            "[INFO] epoch: 611, train loss: 0.0001789115, val loss: 0.0003160368\n",
            "[INFO] epoch: 612, train loss: 0.0001949793, val loss: 0.0014212164\n",
            "[INFO] epoch: 613, train loss: 0.0001785144, val loss: 0.0003145870\n",
            "[INFO] epoch: 614, train loss: 0.0001945640, val loss: 0.0014086301\n",
            "[INFO] epoch: 615, train loss: 0.0001781193, val loss: 0.0003131981\n",
            "[INFO] epoch: 616, train loss: 0.0001941517, val loss: 0.0013961999\n",
            "[INFO] epoch: 617, train loss: 0.0001777272, val loss: 0.0003118691\n",
            "[INFO] epoch: 618, train loss: 0.0001937415, val loss: 0.0013839217\n",
            "[INFO] epoch: 619, train loss: 0.0001773369, val loss: 0.0003105992\n",
            "[INFO] epoch: 620, train loss: 0.0001933329, val loss: 0.0013717902\n",
            "[INFO] epoch: 621, train loss: 0.0001769484, val loss: 0.0003093871\n",
            "[INFO] epoch: 622, train loss: 0.0001929256, val loss: 0.0013598005\n",
            "[INFO] epoch: 623, train loss: 0.0001765609, val loss: 0.0003082307\n",
            "[INFO] epoch: 624, train loss: 0.0001925194, val loss: 0.0013479565\n",
            "[INFO] epoch: 625, train loss: 0.0001761755, val loss: 0.0003071294\n",
            "[INFO] epoch: 626, train loss: 0.0001921157, val loss: 0.0013362548\n",
            "[INFO] epoch: 627, train loss: 0.0001757923, val loss: 0.0003060822\n",
            "[INFO] epoch: 628, train loss: 0.0001917133, val loss: 0.0013246913\n",
            "[INFO] epoch: 629, train loss: 0.0001754109, val loss: 0.0003050869\n",
            "[INFO] epoch: 630, train loss: 0.0001913134, val loss: 0.0013132690\n",
            "[INFO] epoch: 631, train loss: 0.0001750314, val loss: 0.0003041428\n",
            "[INFO] epoch: 632, train loss: 0.0001909139, val loss: 0.0013019825\n",
            "[INFO] epoch: 633, train loss: 0.0001746529, val loss: 0.0003032483\n",
            "[INFO] epoch: 634, train loss: 0.0001905160, val loss: 0.0012908262\n",
            "[INFO] epoch: 635, train loss: 0.0001742763, val loss: 0.0003024025\n",
            "[INFO] epoch: 636, train loss: 0.0001901201, val loss: 0.0012798046\n",
            "[INFO] epoch: 637, train loss: 0.0001739021, val loss: 0.0003016043\n",
            "[INFO] epoch: 638, train loss: 0.0001897268, val loss: 0.0012689209\n",
            "[INFO] epoch: 639, train loss: 0.0001735302, val loss: 0.0003008523\n",
            "[INFO] epoch: 640, train loss: 0.0001893352, val loss: 0.0012581640\n",
            "[INFO] epoch: 641, train loss: 0.0001731596, val loss: 0.0003001451\n",
            "[INFO] epoch: 642, train loss: 0.0001889444, val loss: 0.0012475337\n",
            "[INFO] epoch: 643, train loss: 0.0001727905, val loss: 0.0002994823\n",
            "[INFO] epoch: 644, train loss: 0.0001885559, val loss: 0.0012370310\n",
            "[INFO] epoch: 645, train loss: 0.0001724240, val loss: 0.0002988616\n",
            "[INFO] epoch: 646, train loss: 0.0001881693, val loss: 0.0012266558\n",
            "[INFO] epoch: 647, train loss: 0.0001720588, val loss: 0.0002982829\n",
            "[INFO] epoch: 648, train loss: 0.0001877841, val loss: 0.0012164002\n",
            "[INFO] epoch: 649, train loss: 0.0001716960, val loss: 0.0002977449\n",
            "[INFO] epoch: 650, train loss: 0.0001874020, val loss: 0.0012062753\n",
            "[INFO] epoch: 651, train loss: 0.0001713355, val loss: 0.0002972458\n",
            "[INFO] epoch: 652, train loss: 0.0001870209, val loss: 0.0011962659\n",
            "[INFO] epoch: 653, train loss: 0.0001709760, val loss: 0.0002967856\n",
            "[INFO] epoch: 654, train loss: 0.0001866412, val loss: 0.0011863762\n",
            "[INFO] epoch: 655, train loss: 0.0001706185, val loss: 0.0002963627\n",
            "[INFO] epoch: 656, train loss: 0.0001862638, val loss: 0.0011766067\n",
            "[INFO] epoch: 657, train loss: 0.0001702632, val loss: 0.0002959769\n",
            "[INFO] epoch: 658, train loss: 0.0001858881, val loss: 0.0011669500\n",
            "[INFO] epoch: 659, train loss: 0.0001699093, val loss: 0.0002956264\n",
            "[INFO] epoch: 660, train loss: 0.0001855134, val loss: 0.0011574111\n",
            "[INFO] epoch: 661, train loss: 0.0001695562, val loss: 0.0002953101\n",
            "[INFO] epoch: 662, train loss: 0.0001851395, val loss: 0.0011479800\n",
            "[INFO] epoch: 663, train loss: 0.0001692050, val loss: 0.0002950272\n",
            "[INFO] epoch: 664, train loss: 0.0001847674, val loss: 0.0011386603\n",
            "[INFO] epoch: 665, train loss: 0.0001688550, val loss: 0.0002947773\n",
            "[INFO] epoch: 666, train loss: 0.0001843959, val loss: 0.0011294485\n",
            "[INFO] epoch: 667, train loss: 0.0001685060, val loss: 0.0002945588\n",
            "[INFO] epoch: 668, train loss: 0.0001840257, val loss: 0.0011203417\n",
            "[INFO] epoch: 669, train loss: 0.0001681591, val loss: 0.0002943719\n",
            "[INFO] epoch: 670, train loss: 0.0001836580, val loss: 0.0011113481\n",
            "[INFO] epoch: 671, train loss: 0.0001678139, val loss: 0.0002942146\n",
            "[INFO] epoch: 672, train loss: 0.0001832914, val loss: 0.0011024560\n",
            "[INFO] epoch: 673, train loss: 0.0001674700, val loss: 0.0002940869\n",
            "[INFO] epoch: 674, train loss: 0.0001829265, val loss: 0.0010936661\n",
            "[INFO] epoch: 675, train loss: 0.0001671288, val loss: 0.0002939870\n",
            "[INFO] epoch: 676, train loss: 0.0001825650, val loss: 0.0010849889\n",
            "[INFO] epoch: 677, train loss: 0.0001667900, val loss: 0.0002939148\n",
            "[INFO] epoch: 678, train loss: 0.0001822049, val loss: 0.0010764108\n",
            "[INFO] epoch: 679, train loss: 0.0001664521, val loss: 0.0002938693\n",
            "[INFO] epoch: 680, train loss: 0.0001818453, val loss: 0.0010679299\n",
            "[INFO] epoch: 681, train loss: 0.0001661157, val loss: 0.0002938495\n",
            "[INFO] epoch: 682, train loss: 0.0001814880, val loss: 0.0010595528\n",
            "[INFO] epoch: 683, train loss: 0.0001657813, val loss: 0.0002938546\n",
            "[INFO] epoch: 684, train loss: 0.0001811322, val loss: 0.0010512719\n",
            "[INFO] epoch: 685, train loss: 0.0001654486, val loss: 0.0002938840\n",
            "[INFO] epoch: 686, train loss: 0.0001807787, val loss: 0.0010430885\n",
            "[INFO] epoch: 687, train loss: 0.0001651177, val loss: 0.0002939374\n",
            "[INFO] epoch: 688, train loss: 0.0001804261, val loss: 0.0010350001\n",
            "[INFO] epoch: 689, train loss: 0.0001647875, val loss: 0.0002940126\n",
            "[INFO] epoch: 690, train loss: 0.0001800739, val loss: 0.0010270025\n",
            "[INFO] epoch: 691, train loss: 0.0001644583, val loss: 0.0002941104\n",
            "[INFO] epoch: 692, train loss: 0.0001797228, val loss: 0.0010190982\n",
            "[INFO] epoch: 693, train loss: 0.0001641300, val loss: 0.0002942291\n",
            "[INFO] epoch: 694, train loss: 0.0001793723, val loss: 0.0010112797\n",
            "[INFO] epoch: 695, train loss: 0.0001638032, val loss: 0.0002943690\n",
            "[INFO] epoch: 696, train loss: 0.0001790236, val loss: 0.0010035524\n",
            "[INFO] epoch: 697, train loss: 0.0001634783, val loss: 0.0002945292\n",
            "[INFO] epoch: 698, train loss: 0.0001786776, val loss: 0.0009959185\n",
            "[INFO] epoch: 699, train loss: 0.0001631553, val loss: 0.0002947091\n",
            "[INFO] epoch: 700, train loss: 0.0001783322, val loss: 0.0009883725\n",
            "[INFO] epoch: 701, train loss: 0.0001628327, val loss: 0.0002949071\n",
            "[INFO] epoch: 702, train loss: 0.0001779873, val loss: 0.0009809050\n",
            "[INFO] epoch: 703, train loss: 0.0001625119, val loss: 0.0002951234\n",
            "[INFO] epoch: 704, train loss: 0.0001776448, val loss: 0.0009735279\n",
            "[INFO] epoch: 705, train loss: 0.0001621927, val loss: 0.0002953569\n",
            "[INFO] epoch: 706, train loss: 0.0001773035, val loss: 0.0009662368\n",
            "[INFO] epoch: 707, train loss: 0.0001618751, val loss: 0.0002956077\n",
            "[INFO] epoch: 708, train loss: 0.0001769643, val loss: 0.0009590286\n",
            "[INFO] epoch: 709, train loss: 0.0001615590, val loss: 0.0002958749\n",
            "[INFO] epoch: 710, train loss: 0.0001766259, val loss: 0.0009518990\n",
            "[INFO] epoch: 711, train loss: 0.0001612439, val loss: 0.0002961581\n",
            "[INFO] epoch: 712, train loss: 0.0001762887, val loss: 0.0009448537\n",
            "[INFO] epoch: 713, train loss: 0.0001609303, val loss: 0.0002964553\n",
            "[INFO] epoch: 714, train loss: 0.0001759528, val loss: 0.0009378864\n",
            "[INFO] epoch: 715, train loss: 0.0001606180, val loss: 0.0002967681\n",
            "[INFO] epoch: 716, train loss: 0.0001756183, val loss: 0.0009309982\n",
            "[INFO] epoch: 717, train loss: 0.0001603068, val loss: 0.0002970945\n",
            "[INFO] epoch: 718, train loss: 0.0001752844, val loss: 0.0009241871\n",
            "[INFO] epoch: 719, train loss: 0.0001599963, val loss: 0.0002974344\n",
            "[INFO] epoch: 720, train loss: 0.0001749515, val loss: 0.0009174513\n",
            "[INFO] epoch: 721, train loss: 0.0001596872, val loss: 0.0002977874\n",
            "[INFO] epoch: 722, train loss: 0.0001746202, val loss: 0.0009107919\n",
            "[INFO] epoch: 723, train loss: 0.0001593793, val loss: 0.0002981532\n",
            "[INFO] epoch: 724, train loss: 0.0001742891, val loss: 0.0009042042\n",
            "[INFO] epoch: 725, train loss: 0.0001590718, val loss: 0.0002985305\n",
            "[INFO] epoch: 726, train loss: 0.0001739589, val loss: 0.0008976885\n",
            "[INFO] epoch: 727, train loss: 0.0001587656, val loss: 0.0002989193\n",
            "[INFO] epoch: 728, train loss: 0.0001736294, val loss: 0.0008912445\n",
            "[INFO] epoch: 729, train loss: 0.0001584600, val loss: 0.0002993190\n",
            "[INFO] epoch: 730, train loss: 0.0001733010, val loss: 0.0008848689\n",
            "[INFO] epoch: 731, train loss: 0.0001581562, val loss: 0.0002997301\n",
            "[INFO] epoch: 732, train loss: 0.0001729749, val loss: 0.0008785698\n",
            "[INFO] epoch: 733, train loss: 0.0001578540, val loss: 0.0003001512\n",
            "[INFO] epoch: 734, train loss: 0.0001726500, val loss: 0.0008723391\n",
            "[INFO] epoch: 735, train loss: 0.0001575531, val loss: 0.0003005825\n",
            "[INFO] epoch: 736, train loss: 0.0001723267, val loss: 0.0008661795\n",
            "[INFO] epoch: 737, train loss: 0.0001572535, val loss: 0.0003010228\n",
            "[INFO] epoch: 738, train loss: 0.0001720040, val loss: 0.0008600857\n",
            "[INFO] epoch: 739, train loss: 0.0001569547, val loss: 0.0003014719\n",
            "[INFO] epoch: 740, train loss: 0.0001716823, val loss: 0.0008540575\n",
            "[INFO] epoch: 741, train loss: 0.0001566567, val loss: 0.0003019285\n",
            "[INFO] epoch: 742, train loss: 0.0001713609, val loss: 0.0008480959\n",
            "[INFO] epoch: 743, train loss: 0.0001563594, val loss: 0.0003023938\n",
            "[INFO] epoch: 744, train loss: 0.0001710400, val loss: 0.0008421962\n",
            "[INFO] epoch: 745, train loss: 0.0001560628, val loss: 0.0003028662\n",
            "[INFO] epoch: 746, train loss: 0.0001707204, val loss: 0.0008363623\n",
            "[INFO] epoch: 747, train loss: 0.0001557673, val loss: 0.0003033459\n",
            "[INFO] epoch: 748, train loss: 0.0001704020, val loss: 0.0008305909\n",
            "[INFO] epoch: 749, train loss: 0.0001554733, val loss: 0.0003038334\n",
            "[INFO] epoch: 750, train loss: 0.0001700844, val loss: 0.0008248813\n",
            "[INFO] epoch: 751, train loss: 0.0001551800, val loss: 0.0003043264\n",
            "[INFO] epoch: 752, train loss: 0.0001697678, val loss: 0.0008192338\n",
            "[INFO] epoch: 753, train loss: 0.0001548879, val loss: 0.0003048258\n",
            "[INFO] epoch: 754, train loss: 0.0001694529, val loss: 0.0008136477\n",
            "[INFO] epoch: 755, train loss: 0.0001545974, val loss: 0.0003053316\n",
            "[INFO] epoch: 756, train loss: 0.0001691392, val loss: 0.0008081233\n",
            "[INFO] epoch: 757, train loss: 0.0001543078, val loss: 0.0003058423\n",
            "[INFO] epoch: 758, train loss: 0.0001688261, val loss: 0.0008026576\n",
            "[INFO] epoch: 759, train loss: 0.0001540187, val loss: 0.0003063579\n",
            "[INFO] epoch: 760, train loss: 0.0001685132, val loss: 0.0007972487\n",
            "[INFO] epoch: 761, train loss: 0.0001537301, val loss: 0.0003068779\n",
            "[INFO] epoch: 762, train loss: 0.0001682008, val loss: 0.0007918963\n",
            "[INFO] epoch: 763, train loss: 0.0001534420, val loss: 0.0003074015\n",
            "[INFO] epoch: 764, train loss: 0.0001678891, val loss: 0.0007865999\n",
            "[INFO] epoch: 765, train loss: 0.0001531551, val loss: 0.0003079302\n",
            "[INFO] epoch: 766, train loss: 0.0001675786, val loss: 0.0007813597\n",
            "[INFO] epoch: 767, train loss: 0.0001528689, val loss: 0.0003084632\n",
            "[INFO] epoch: 768, train loss: 0.0001672679, val loss: 0.0007761728\n",
            "[INFO] epoch: 769, train loss: 0.0001525828, val loss: 0.0003089981\n",
            "[INFO] epoch: 770, train loss: 0.0001669578, val loss: 0.0007710385\n",
            "[INFO] epoch: 771, train loss: 0.0001522979, val loss: 0.0003095370\n",
            "[INFO] epoch: 772, train loss: 0.0001666494, val loss: 0.0007659606\n",
            "[INFO] epoch: 773, train loss: 0.0001520142, val loss: 0.0003100791\n",
            "[INFO] epoch: 774, train loss: 0.0001663416, val loss: 0.0007609350\n",
            "[INFO] epoch: 775, train loss: 0.0001517311, val loss: 0.0003106238\n",
            "[INFO] epoch: 776, train loss: 0.0001660342, val loss: 0.0007559590\n",
            "[INFO] epoch: 777, train loss: 0.0001514493, val loss: 0.0003111709\n",
            "[INFO] epoch: 778, train loss: 0.0001657290, val loss: 0.0007510400\n",
            "[INFO] epoch: 779, train loss: 0.0001511690, val loss: 0.0003117206\n",
            "[INFO] epoch: 780, train loss: 0.0001654248, val loss: 0.0007461720\n",
            "[INFO] epoch: 781, train loss: 0.0001508896, val loss: 0.0003122715\n",
            "[INFO] epoch: 782, train loss: 0.0001651215, val loss: 0.0007413545\n",
            "[INFO] epoch: 783, train loss: 0.0001506111, val loss: 0.0003128240\n",
            "[INFO] epoch: 784, train loss: 0.0001648194, val loss: 0.0007365881\n",
            "[INFO] epoch: 785, train loss: 0.0001503336, val loss: 0.0003133782\n",
            "[INFO] epoch: 786, train loss: 0.0001645176, val loss: 0.0007318708\n",
            "[INFO] epoch: 787, train loss: 0.0001500568, val loss: 0.0003139329\n",
            "[INFO] epoch: 788, train loss: 0.0001642170, val loss: 0.0007272026\n",
            "[INFO] epoch: 789, train loss: 0.0001497808, val loss: 0.0003144878\n",
            "[INFO] epoch: 790, train loss: 0.0001639167, val loss: 0.0007225807\n",
            "[INFO] epoch: 791, train loss: 0.0001495056, val loss: 0.0003150439\n",
            "[INFO] epoch: 792, train loss: 0.0001636170, val loss: 0.0007180064\n",
            "[INFO] epoch: 793, train loss: 0.0001492303, val loss: 0.0003155989\n",
            "[INFO] epoch: 794, train loss: 0.0001633170, val loss: 0.0007134749\n",
            "[INFO] epoch: 795, train loss: 0.0001489559, val loss: 0.0003161542\n",
            "[INFO] epoch: 796, train loss: 0.0001630189, val loss: 0.0007089924\n",
            "[INFO] epoch: 797, train loss: 0.0001486831, val loss: 0.0003167100\n",
            "[INFO] epoch: 798, train loss: 0.0001627217, val loss: 0.0007045581\n",
            "[INFO] epoch: 799, train loss: 0.0001484105, val loss: 0.0003172650\n",
            "[INFO] epoch: 800, train loss: 0.0001624243, val loss: 0.0007001638\n",
            "[INFO] epoch: 801, train loss: 0.0001481382, val loss: 0.0003178193\n",
            "[INFO] epoch: 802, train loss: 0.0001621273, val loss: 0.0006958138\n",
            "[INFO] epoch: 803, train loss: 0.0001478665, val loss: 0.0003183727\n",
            "[INFO] epoch: 804, train loss: 0.0001618313, val loss: 0.0006915063\n",
            "[INFO] epoch: 805, train loss: 0.0001475963, val loss: 0.0003189260\n",
            "[INFO] epoch: 806, train loss: 0.0001615363, val loss: 0.0006872425\n",
            "[INFO] epoch: 807, train loss: 0.0001473270, val loss: 0.0003194776\n",
            "[INFO] epoch: 808, train loss: 0.0001612431, val loss: 0.0006830260\n",
            "[INFO] epoch: 809, train loss: 0.0001470586, val loss: 0.0003200285\n",
            "[INFO] epoch: 810, train loss: 0.0001609502, val loss: 0.0006788471\n",
            "[INFO] epoch: 811, train loss: 0.0001467914, val loss: 0.0003205775\n",
            "[INFO] epoch: 812, train loss: 0.0001606589, val loss: 0.0006747129\n",
            "[INFO] epoch: 813, train loss: 0.0001465253, val loss: 0.0003211252\n",
            "[INFO] epoch: 814, train loss: 0.0001603682, val loss: 0.0006706221\n",
            "[INFO] epoch: 815, train loss: 0.0001462598, val loss: 0.0003216705\n",
            "[INFO] epoch: 816, train loss: 0.0001600786, val loss: 0.0006665689\n",
            "[INFO] epoch: 817, train loss: 0.0001459957, val loss: 0.0003222141\n",
            "[INFO] epoch: 818, train loss: 0.0001597899, val loss: 0.0006625581\n",
            "[INFO] epoch: 819, train loss: 0.0001457320, val loss: 0.0003227550\n",
            "[INFO] epoch: 820, train loss: 0.0001595018, val loss: 0.0006585871\n",
            "[INFO] epoch: 821, train loss: 0.0001454691, val loss: 0.0003232938\n",
            "[INFO] epoch: 822, train loss: 0.0001592144, val loss: 0.0006546538\n",
            "[INFO] epoch: 823, train loss: 0.0001452069, val loss: 0.0003238293\n",
            "[INFO] epoch: 824, train loss: 0.0001589273, val loss: 0.0006507584\n",
            "[INFO] epoch: 825, train loss: 0.0001449450, val loss: 0.0003243616\n",
            "[INFO] epoch: 826, train loss: 0.0001586409, val loss: 0.0006469003\n",
            "[INFO] epoch: 827, train loss: 0.0001446843, val loss: 0.0003248919\n",
            "[INFO] epoch: 828, train loss: 0.0001583557, val loss: 0.0006430807\n",
            "[INFO] epoch: 829, train loss: 0.0001444244, val loss: 0.0003254190\n",
            "[INFO] epoch: 830, train loss: 0.0001580710, val loss: 0.0006392986\n",
            "[INFO] epoch: 831, train loss: 0.0001441652, val loss: 0.0003259431\n",
            "[INFO] epoch: 832, train loss: 0.0001577873, val loss: 0.0006355530\n",
            "[INFO] epoch: 833, train loss: 0.0001439069, val loss: 0.0003264632\n",
            "[INFO] epoch: 834, train loss: 0.0001575043, val loss: 0.0006318433\n",
            "[INFO] epoch: 835, train loss: 0.0001436493, val loss: 0.0003269807\n",
            "[INFO] epoch: 836, train loss: 0.0001572222, val loss: 0.0006281689\n",
            "[INFO] epoch: 837, train loss: 0.0001433927, val loss: 0.0003274949\n",
            "[INFO] epoch: 838, train loss: 0.0001569412, val loss: 0.0006245311\n",
            "[INFO] epoch: 839, train loss: 0.0001431374, val loss: 0.0003280049\n",
            "[INFO] epoch: 840, train loss: 0.0001566619, val loss: 0.0006209277\n",
            "[INFO] epoch: 841, train loss: 0.0001428837, val loss: 0.0003285129\n",
            "[INFO] epoch: 842, train loss: 0.0001563839, val loss: 0.0006173628\n",
            "[INFO] epoch: 843, train loss: 0.0001426304, val loss: 0.0003290155\n",
            "[INFO] epoch: 844, train loss: 0.0001561058, val loss: 0.0006138293\n",
            "[INFO] epoch: 845, train loss: 0.0001423773, val loss: 0.0003295138\n",
            "[INFO] epoch: 846, train loss: 0.0001558281, val loss: 0.0006103289\n",
            "[INFO] epoch: 847, train loss: 0.0001421250, val loss: 0.0003300080\n",
            "[INFO] epoch: 848, train loss: 0.0001555513, val loss: 0.0006068603\n",
            "[INFO] epoch: 849, train loss: 0.0001418736, val loss: 0.0003304987\n",
            "[INFO] epoch: 850, train loss: 0.0001552756, val loss: 0.0006034245\n",
            "[INFO] epoch: 851, train loss: 0.0001416234, val loss: 0.0003309851\n",
            "[INFO] epoch: 852, train loss: 0.0001550010, val loss: 0.0006000230\n",
            "[INFO] epoch: 853, train loss: 0.0001413739, val loss: 0.0003314673\n",
            "[INFO] epoch: 854, train loss: 0.0001547268, val loss: 0.0005966530\n",
            "[INFO] epoch: 855, train loss: 0.0001411249, val loss: 0.0003319443\n",
            "[INFO] epoch: 856, train loss: 0.0001544533, val loss: 0.0005933131\n",
            "[INFO] epoch: 857, train loss: 0.0001408768, val loss: 0.0003324168\n",
            "[INFO] epoch: 858, train loss: 0.0001541808, val loss: 0.0005900029\n",
            "[INFO] epoch: 859, train loss: 0.0001406297, val loss: 0.0003328851\n",
            "[INFO] epoch: 860, train loss: 0.0001539094, val loss: 0.0005867256\n",
            "[INFO] epoch: 861, train loss: 0.0001403837, val loss: 0.0003333486\n",
            "[INFO] epoch: 862, train loss: 0.0001536394, val loss: 0.0005834799\n",
            "[INFO] epoch: 863, train loss: 0.0001401387, val loss: 0.0003338083\n",
            "[INFO] epoch: 864, train loss: 0.0001533698, val loss: 0.0005802629\n",
            "[INFO] epoch: 865, train loss: 0.0001398941, val loss: 0.0003342625\n",
            "[INFO] epoch: 866, train loss: 0.0001531007, val loss: 0.0005770743\n",
            "[INFO] epoch: 867, train loss: 0.0001396504, val loss: 0.0003347111\n",
            "[INFO] epoch: 868, train loss: 0.0001528331, val loss: 0.0005739163\n",
            "[INFO] epoch: 869, train loss: 0.0001394078, val loss: 0.0003351563\n",
            "[INFO] epoch: 870, train loss: 0.0001525660, val loss: 0.0005707857\n",
            "[INFO] epoch: 871, train loss: 0.0001391662, val loss: 0.0003355956\n",
            "[INFO] epoch: 872, train loss: 0.0001523007, val loss: 0.0005676863\n",
            "[INFO] epoch: 873, train loss: 0.0001389259, val loss: 0.0003360303\n",
            "[INFO] epoch: 874, train loss: 0.0001520362, val loss: 0.0005646158\n",
            "[INFO] epoch: 875, train loss: 0.0001386863, val loss: 0.0003364596\n",
            "[INFO] epoch: 876, train loss: 0.0001517725, val loss: 0.0005615721\n",
            "[INFO] epoch: 877, train loss: 0.0001384473, val loss: 0.0003368832\n",
            "[INFO] epoch: 878, train loss: 0.0001515092, val loss: 0.0005585550\n",
            "[INFO] epoch: 879, train loss: 0.0001382090, val loss: 0.0003373022\n",
            "[INFO] epoch: 880, train loss: 0.0001512466, val loss: 0.0005555643\n",
            "[INFO] epoch: 881, train loss: 0.0001379713, val loss: 0.0003377152\n",
            "[INFO] epoch: 882, train loss: 0.0001509848, val loss: 0.0005526003\n",
            "[INFO] epoch: 883, train loss: 0.0001377343, val loss: 0.0003381225\n",
            "[INFO] epoch: 884, train loss: 0.0001507239, val loss: 0.0005496611\n",
            "[INFO] epoch: 885, train loss: 0.0001374987, val loss: 0.0003385251\n",
            "[INFO] epoch: 886, train loss: 0.0001504648, val loss: 0.0005467536\n",
            "[INFO] epoch: 887, train loss: 0.0001372643, val loss: 0.0003389230\n",
            "[INFO] epoch: 888, train loss: 0.0001502065, val loss: 0.0005438695\n",
            "[INFO] epoch: 889, train loss: 0.0001370305, val loss: 0.0003393155\n",
            "[INFO] epoch: 890, train loss: 0.0001499488, val loss: 0.0005410097\n",
            "[INFO] epoch: 891, train loss: 0.0001367977, val loss: 0.0003397018\n",
            "[INFO] epoch: 892, train loss: 0.0001496921, val loss: 0.0005381767\n",
            "[INFO] epoch: 893, train loss: 0.0001365656, val loss: 0.0003400819\n",
            "[INFO] epoch: 894, train loss: 0.0001494365, val loss: 0.0005353680\n",
            "[INFO] epoch: 895, train loss: 0.0001363343, val loss: 0.0003404582\n",
            "[INFO] epoch: 896, train loss: 0.0001491812, val loss: 0.0005325816\n",
            "[INFO] epoch: 897, train loss: 0.0001361038, val loss: 0.0003408274\n",
            "[INFO] epoch: 898, train loss: 0.0001489269, val loss: 0.0005298224\n",
            "[INFO] epoch: 899, train loss: 0.0001358740, val loss: 0.0003411908\n",
            "[INFO] epoch: 900, train loss: 0.0001486738, val loss: 0.0005270848\n",
            "[INFO] epoch: 901, train loss: 0.0001356455, val loss: 0.0003415496\n",
            "[INFO] epoch: 902, train loss: 0.0001484217, val loss: 0.0005243736\n",
            "[INFO] epoch: 903, train loss: 0.0001354175, val loss: 0.0003419018\n",
            "[INFO] epoch: 904, train loss: 0.0001481696, val loss: 0.0005216821\n",
            "[INFO] epoch: 905, train loss: 0.0001351899, val loss: 0.0003422476\n",
            "[INFO] epoch: 906, train loss: 0.0001479188, val loss: 0.0005190155\n",
            "[INFO] epoch: 907, train loss: 0.0001349636, val loss: 0.0003425895\n",
            "[INFO] epoch: 908, train loss: 0.0001476687, val loss: 0.0005163703\n",
            "[INFO] epoch: 909, train loss: 0.0001347377, val loss: 0.0003429238\n",
            "[INFO] epoch: 910, train loss: 0.0001474194, val loss: 0.0005137479\n",
            "[INFO] epoch: 911, train loss: 0.0001345127, val loss: 0.0003432534\n",
            "[INFO] epoch: 912, train loss: 0.0001471711, val loss: 0.0005111477\n",
            "[INFO] epoch: 913, train loss: 0.0001342888, val loss: 0.0003435780\n",
            "[INFO] epoch: 914, train loss: 0.0001469237, val loss: 0.0005085683\n",
            "[INFO] epoch: 915, train loss: 0.0001340655, val loss: 0.0003438952\n",
            "[INFO] epoch: 916, train loss: 0.0001466772, val loss: 0.0005060129\n",
            "[INFO] epoch: 917, train loss: 0.0001338432, val loss: 0.0003442070\n",
            "[INFO] epoch: 918, train loss: 0.0001464316, val loss: 0.0005034787\n",
            "[INFO] epoch: 919, train loss: 0.0001336216, val loss: 0.0003445136\n",
            "[INFO] epoch: 920, train loss: 0.0001461863, val loss: 0.0005009639\n",
            "[INFO] epoch: 921, train loss: 0.0001334004, val loss: 0.0003448128\n",
            "[INFO] epoch: 922, train loss: 0.0001459423, val loss: 0.0004984698\n",
            "[INFO] epoch: 923, train loss: 0.0001331807, val loss: 0.0003451079\n",
            "[INFO] epoch: 924, train loss: 0.0001456998, val loss: 0.0004959987\n",
            "[INFO] epoch: 925, train loss: 0.0001329618, val loss: 0.0003453975\n",
            "[INFO] epoch: 926, train loss: 0.0001454574, val loss: 0.0004935462\n",
            "[INFO] epoch: 927, train loss: 0.0001327434, val loss: 0.0003456790\n",
            "[INFO] epoch: 928, train loss: 0.0001452165, val loss: 0.0004911148\n",
            "[INFO] epoch: 929, train loss: 0.0001325263, val loss: 0.0003459567\n",
            "[INFO] epoch: 930, train loss: 0.0001449763, val loss: 0.0004887026\n",
            "[INFO] epoch: 931, train loss: 0.0001323096, val loss: 0.0003462275\n",
            "[INFO] epoch: 932, train loss: 0.0001447365, val loss: 0.0004863103\n",
            "[INFO] epoch: 933, train loss: 0.0001320935, val loss: 0.0003464916\n",
            "[INFO] epoch: 934, train loss: 0.0001444973, val loss: 0.0004839359\n",
            "[INFO] epoch: 935, train loss: 0.0001318779, val loss: 0.0003467503\n",
            "[INFO] epoch: 936, train loss: 0.0001442588, val loss: 0.0004815799\n",
            "[INFO] epoch: 937, train loss: 0.0001316636, val loss: 0.0003470030\n",
            "[INFO] epoch: 938, train loss: 0.0001440221, val loss: 0.0004792463\n",
            "[INFO] epoch: 939, train loss: 0.0001314503, val loss: 0.0003472498\n",
            "[INFO] epoch: 940, train loss: 0.0001437860, val loss: 0.0004769319\n",
            "[INFO] epoch: 941, train loss: 0.0001312376, val loss: 0.0003474901\n",
            "[INFO] epoch: 942, train loss: 0.0001435508, val loss: 0.0004746335\n",
            "[INFO] epoch: 943, train loss: 0.0001310256, val loss: 0.0003477261\n",
            "[INFO] epoch: 944, train loss: 0.0001433157, val loss: 0.0004723536\n",
            "[INFO] epoch: 945, train loss: 0.0001308137, val loss: 0.0003479531\n",
            "[INFO] epoch: 946, train loss: 0.0001430807, val loss: 0.0004700898\n",
            "[INFO] epoch: 947, train loss: 0.0001306020, val loss: 0.0003481754\n",
            "[INFO] epoch: 948, train loss: 0.0001428462, val loss: 0.0004678430\n",
            "[INFO] epoch: 949, train loss: 0.0001303914, val loss: 0.0003483916\n",
            "[INFO] epoch: 950, train loss: 0.0001426133, val loss: 0.0004656147\n",
            "[INFO] epoch: 951, train loss: 0.0001301818, val loss: 0.0003486028\n",
            "[INFO] epoch: 952, train loss: 0.0001423812, val loss: 0.0004634044\n",
            "[INFO] epoch: 953, train loss: 0.0001299732, val loss: 0.0003488078\n",
            "[INFO] epoch: 954, train loss: 0.0001421506, val loss: 0.0004612107\n",
            "[INFO] epoch: 955, train loss: 0.0001297661, val loss: 0.0003490086\n",
            "[INFO] epoch: 956, train loss: 0.0001419210, val loss: 0.0004590373\n",
            "[INFO] epoch: 957, train loss: 0.0001295589, val loss: 0.0003492016\n",
            "[INFO] epoch: 958, train loss: 0.0001416908, val loss: 0.0004568779\n",
            "[INFO] epoch: 959, train loss: 0.0001293522, val loss: 0.0003493880\n",
            "[INFO] epoch: 960, train loss: 0.0001414622, val loss: 0.0004547337\n",
            "[INFO] epoch: 961, train loss: 0.0001291465, val loss: 0.0003495700\n",
            "[INFO] epoch: 962, train loss: 0.0001412336, val loss: 0.0004526067\n",
            "[INFO] epoch: 963, train loss: 0.0001289411, val loss: 0.0003497445\n",
            "[INFO] epoch: 964, train loss: 0.0001410062, val loss: 0.0004504965\n",
            "[INFO] epoch: 965, train loss: 0.0001287371, val loss: 0.0003499138\n",
            "[INFO] epoch: 966, train loss: 0.0001407806, val loss: 0.0004484041\n",
            "[INFO] epoch: 967, train loss: 0.0001285340, val loss: 0.0003500784\n",
            "[INFO] epoch: 968, train loss: 0.0001405551, val loss: 0.0004463263\n",
            "[INFO] epoch: 969, train loss: 0.0001283315, val loss: 0.0003502345\n",
            "[INFO] epoch: 970, train loss: 0.0001403310, val loss: 0.0004442644\n",
            "[INFO] epoch: 971, train loss: 0.0001281302, val loss: 0.0003503875\n",
            "[INFO] epoch: 972, train loss: 0.0001401075, val loss: 0.0004422192\n",
            "[INFO] epoch: 973, train loss: 0.0001279288, val loss: 0.0003505335\n",
            "[INFO] epoch: 974, train loss: 0.0001398834, val loss: 0.0004401864\n",
            "[INFO] epoch: 975, train loss: 0.0001277274, val loss: 0.0003506714\n",
            "[INFO] epoch: 976, train loss: 0.0001396602, val loss: 0.0004381693\n",
            "[INFO] epoch: 977, train loss: 0.0001275274, val loss: 0.0003508051\n",
            "[INFO] epoch: 978, train loss: 0.0001394385, val loss: 0.0004361673\n",
            "[INFO] epoch: 979, train loss: 0.0001273285, val loss: 0.0003509342\n",
            "[INFO] epoch: 980, train loss: 0.0001392176, val loss: 0.0004341812\n",
            "[INFO] epoch: 981, train loss: 0.0001271298, val loss: 0.0003510560\n",
            "[INFO] epoch: 982, train loss: 0.0001389970, val loss: 0.0004322082\n",
            "[INFO] epoch: 983, train loss: 0.0001269316, val loss: 0.0003511715\n",
            "[INFO] epoch: 984, train loss: 0.0001387765, val loss: 0.0004302493\n",
            "[INFO] epoch: 985, train loss: 0.0001267335, val loss: 0.0003512810\n",
            "[INFO] epoch: 986, train loss: 0.0001385566, val loss: 0.0004283032\n",
            "[INFO] epoch: 987, train loss: 0.0001265366, val loss: 0.0003513855\n",
            "[INFO] epoch: 988, train loss: 0.0001383383, val loss: 0.0004263740\n",
            "[INFO] epoch: 989, train loss: 0.0001263409, val loss: 0.0003514843\n",
            "[INFO] epoch: 990, train loss: 0.0001381210, val loss: 0.0004244581\n",
            "[INFO] epoch: 991, train loss: 0.0001261461, val loss: 0.0003515787\n",
            "[INFO] epoch: 992, train loss: 0.0001379048, val loss: 0.0004225572\n",
            "[INFO] epoch: 993, train loss: 0.0001259519, val loss: 0.0003516661\n",
            "[INFO] epoch: 994, train loss: 0.0001376889, val loss: 0.0004206697\n",
            "[INFO] epoch: 995, train loss: 0.0001257583, val loss: 0.0003517486\n",
            "[INFO] epoch: 996, train loss: 0.0001374735, val loss: 0.0004187945\n",
            "[INFO] epoch: 997, train loss: 0.0001255650, val loss: 0.0003518245\n",
            "[INFO] epoch: 998, train loss: 0.0001372588, val loss: 0.0004169334\n",
            "[INFO] epoch: 999, train loss: 0.0001253722, val loss: 0.0003518943\n",
            "[INFO] epoch: 1000, train loss: 0.0001370445, val loss: 0.0004150842\n",
            "[INFO] epoch: 1001, train loss: 0.0001251803, val loss: 0.0003519584\n",
            "[INFO] epoch: 1002, train loss: 0.0001368310, val loss: 0.0004132501\n",
            "[INFO] epoch: 1003, train loss: 0.0001249887, val loss: 0.0003520171\n",
            "[INFO] epoch: 1004, train loss: 0.0001366179, val loss: 0.0004114253\n",
            "[INFO] epoch: 1005, train loss: 0.0001247979, val loss: 0.0003520704\n",
            "[INFO] epoch: 1006, train loss: 0.0001364061, val loss: 0.0004096156\n",
            "[INFO] epoch: 1007, train loss: 0.0001246080, val loss: 0.0003521177\n",
            "[INFO] epoch: 1008, train loss: 0.0001361947, val loss: 0.0004078182\n",
            "[INFO] epoch: 1009, train loss: 0.0001244189, val loss: 0.0003521607\n",
            "[INFO] epoch: 1010, train loss: 0.0001359846, val loss: 0.0004060345\n",
            "[INFO] epoch: 1011, train loss: 0.0001242307, val loss: 0.0003521974\n",
            "[INFO] epoch: 1012, train loss: 0.0001357752, val loss: 0.0004042638\n",
            "[INFO] epoch: 1013, train loss: 0.0001240428, val loss: 0.0003522286\n",
            "[INFO] epoch: 1014, train loss: 0.0001355661, val loss: 0.0004025034\n",
            "[INFO] epoch: 1015, train loss: 0.0001238554, val loss: 0.0003522541\n",
            "[INFO] epoch: 1016, train loss: 0.0001353573, val loss: 0.0004007556\n",
            "[INFO] epoch: 1017, train loss: 0.0001236685, val loss: 0.0003522741\n",
            "[INFO] epoch: 1018, train loss: 0.0001351493, val loss: 0.0003990192\n",
            "[INFO] epoch: 1019, train loss: 0.0001234820, val loss: 0.0003522878\n",
            "[INFO] epoch: 1020, train loss: 0.0001349417, val loss: 0.0003972948\n",
            "[INFO] epoch: 1021, train loss: 0.0001232964, val loss: 0.0003522978\n",
            "[INFO] epoch: 1022, train loss: 0.0001347352, val loss: 0.0003955817\n",
            "[INFO] epoch: 1023, train loss: 0.0001231112, val loss: 0.0003523012\n",
            "[INFO] epoch: 1024, train loss: 0.0001345285, val loss: 0.0003938793\n",
            "[INFO] epoch: 1025, train loss: 0.0001229264, val loss: 0.0003522992\n",
            "[INFO] epoch: 1026, train loss: 0.0001343230, val loss: 0.0003921877\n",
            "[INFO] epoch: 1027, train loss: 0.0001227426, val loss: 0.0003522929\n",
            "[INFO] epoch: 1028, train loss: 0.0001341182, val loss: 0.0003905097\n",
            "[INFO] epoch: 1029, train loss: 0.0001225591, val loss: 0.0003522809\n",
            "[INFO] epoch: 1030, train loss: 0.0001339136, val loss: 0.0003888407\n",
            "[INFO] epoch: 1031, train loss: 0.0001223761, val loss: 0.0003522632\n",
            "[INFO] epoch: 1032, train loss: 0.0001337097, val loss: 0.0003871825\n",
            "[INFO] epoch: 1033, train loss: 0.0001221939, val loss: 0.0003522412\n",
            "[INFO] epoch: 1034, train loss: 0.0001335069, val loss: 0.0003855352\n",
            "[INFO] epoch: 1035, train loss: 0.0001220128, val loss: 0.0003522138\n",
            "[INFO] epoch: 1036, train loss: 0.0001333054, val loss: 0.0003839011\n",
            "[INFO] epoch: 1037, train loss: 0.0001218324, val loss: 0.0003521820\n",
            "[INFO] epoch: 1038, train loss: 0.0001331043, val loss: 0.0003822768\n",
            "[INFO] epoch: 1039, train loss: 0.0001216526, val loss: 0.0003521449\n",
            "[INFO] epoch: 1040, train loss: 0.0001329038, val loss: 0.0003806650\n",
            "[INFO] epoch: 1041, train loss: 0.0001214732, val loss: 0.0003521019\n",
            "[INFO] epoch: 1042, train loss: 0.0001327037, val loss: 0.0003790608\n",
            "[INFO] epoch: 1043, train loss: 0.0001212943, val loss: 0.0003520534\n",
            "[INFO] epoch: 1044, train loss: 0.0001325044, val loss: 0.0003774675\n",
            "[INFO] epoch: 1045, train loss: 0.0001211162, val loss: 0.0003520018\n",
            "[INFO] epoch: 1046, train loss: 0.0001323055, val loss: 0.0003758846\n",
            "[INFO] epoch: 1047, train loss: 0.0001209381, val loss: 0.0003519430\n",
            "[INFO] epoch: 1048, train loss: 0.0001321066, val loss: 0.0003743107\n",
            "[INFO] epoch: 1049, train loss: 0.0001207603, val loss: 0.0003518800\n",
            "[INFO] epoch: 1050, train loss: 0.0001319081, val loss: 0.0003727460\n",
            "[INFO] epoch: 1051, train loss: 0.0001205826, val loss: 0.0003518107\n",
            "[INFO] epoch: 1052, train loss: 0.0001317097, val loss: 0.0003711901\n",
            "[INFO] epoch: 1053, train loss: 0.0001204056, val loss: 0.0003517378\n",
            "[INFO] epoch: 1054, train loss: 0.0001315121, val loss: 0.0003696443\n",
            "[INFO] epoch: 1055, train loss: 0.0001202291, val loss: 0.0003516605\n",
            "[INFO] epoch: 1056, train loss: 0.0001313149, val loss: 0.0003681072\n",
            "[INFO] epoch: 1057, train loss: 0.0001200528, val loss: 0.0003515775\n",
            "[INFO] epoch: 1058, train loss: 0.0001311180, val loss: 0.0003665786\n",
            "[INFO] epoch: 1059, train loss: 0.0001198775, val loss: 0.0003514906\n",
            "[INFO] epoch: 1060, train loss: 0.0001309227, val loss: 0.0003650615\n",
            "[INFO] epoch: 1061, train loss: 0.0001197033, val loss: 0.0003513993\n",
            "[INFO] epoch: 1062, train loss: 0.0001307282, val loss: 0.0003635527\n",
            "[INFO] epoch: 1063, train loss: 0.0001195297, val loss: 0.0003513042\n",
            "[INFO] epoch: 1064, train loss: 0.0001305347, val loss: 0.0003620551\n",
            "[INFO] epoch: 1065, train loss: 0.0001193570, val loss: 0.0003512044\n",
            "[INFO] epoch: 1066, train loss: 0.0001303419, val loss: 0.0003605658\n",
            "[INFO] epoch: 1067, train loss: 0.0001191853, val loss: 0.0003511017\n",
            "[INFO] epoch: 1068, train loss: 0.0001301504, val loss: 0.0003590877\n",
            "[INFO] epoch: 1069, train loss: 0.0001190140, val loss: 0.0003509920\n",
            "[INFO] epoch: 1070, train loss: 0.0001299591, val loss: 0.0003576181\n",
            "[INFO] epoch: 1071, train loss: 0.0001188434, val loss: 0.0003508790\n",
            "[INFO] epoch: 1072, train loss: 0.0001297684, val loss: 0.0003561569\n",
            "[INFO] epoch: 1073, train loss: 0.0001186730, val loss: 0.0003507603\n",
            "[INFO] epoch: 1074, train loss: 0.0001295779, val loss: 0.0003547047\n",
            "[INFO] epoch: 1075, train loss: 0.0001185025, val loss: 0.0003506367\n",
            "[INFO] epoch: 1076, train loss: 0.0001293867, val loss: 0.0003532575\n",
            "[INFO] epoch: 1077, train loss: 0.0001183322, val loss: 0.0003505074\n",
            "[INFO] epoch: 1078, train loss: 0.0001291965, val loss: 0.0003518204\n",
            "[INFO] epoch: 1079, train loss: 0.0001181624, val loss: 0.0003503747\n",
            "[INFO] epoch: 1080, train loss: 0.0001290068, val loss: 0.0003503914\n",
            "[INFO] epoch: 1081, train loss: 0.0001179930, val loss: 0.0003502392\n",
            "[INFO] epoch: 1082, train loss: 0.0001288169, val loss: 0.0003489692\n",
            "[INFO] epoch: 1083, train loss: 0.0001178238, val loss: 0.0003500965\n",
            "[INFO] epoch: 1084, train loss: 0.0001286279, val loss: 0.0003475556\n",
            "[INFO] epoch: 1085, train loss: 0.0001176555, val loss: 0.0003499510\n",
            "[INFO] epoch: 1086, train loss: 0.0001284396, val loss: 0.0003461502\n",
            "[INFO] epoch: 1087, train loss: 0.0001174875, val loss: 0.0003498019\n",
            "[INFO] epoch: 1088, train loss: 0.0001282515, val loss: 0.0003447525\n",
            "[INFO] epoch: 1089, train loss: 0.0001173195, val loss: 0.0003496483\n",
            "[INFO] epoch: 1090, train loss: 0.0001280634, val loss: 0.0003433610\n",
            "[INFO] epoch: 1091, train loss: 0.0001171522, val loss: 0.0003494897\n",
            "[INFO] epoch: 1092, train loss: 0.0001278762, val loss: 0.0003419772\n",
            "[INFO] epoch: 1093, train loss: 0.0001169856, val loss: 0.0003493285\n",
            "[INFO] epoch: 1094, train loss: 0.0001276899, val loss: 0.0003406030\n",
            "[INFO] epoch: 1095, train loss: 0.0001168197, val loss: 0.0003491630\n",
            "[INFO] epoch: 1096, train loss: 0.0001275047, val loss: 0.0003392359\n",
            "[INFO] epoch: 1097, train loss: 0.0001166547, val loss: 0.0003489948\n",
            "[INFO] epoch: 1098, train loss: 0.0001273199, val loss: 0.0003378780\n",
            "[INFO] epoch: 1099, train loss: 0.0001164900, val loss: 0.0003488215\n",
            "[INFO] epoch: 1100, train loss: 0.0001271353, val loss: 0.0003365262\n",
            "[INFO] epoch: 1101, train loss: 0.0001163256, val loss: 0.0003486435\n",
            "[INFO] epoch: 1102, train loss: 0.0001269514, val loss: 0.0003351817\n",
            "[INFO] epoch: 1103, train loss: 0.0001161621, val loss: 0.0003484633\n",
            "[INFO] epoch: 1104, train loss: 0.0001267686, val loss: 0.0003338456\n",
            "[INFO] epoch: 1105, train loss: 0.0001159994, val loss: 0.0003482787\n",
            "[INFO] epoch: 1106, train loss: 0.0001265867, val loss: 0.0003325176\n",
            "[INFO] epoch: 1107, train loss: 0.0001158373, val loss: 0.0003480899\n",
            "[INFO] epoch: 1108, train loss: 0.0001264052, val loss: 0.0003311963\n",
            "[INFO] epoch: 1109, train loss: 0.0001156755, val loss: 0.0003478977\n",
            "[INFO] epoch: 1110, train loss: 0.0001262239, val loss: 0.0003298824\n",
            "[INFO] epoch: 1111, train loss: 0.0001155143, val loss: 0.0003477010\n",
            "[INFO] epoch: 1112, train loss: 0.0001260432, val loss: 0.0003285756\n",
            "[INFO] epoch: 1113, train loss: 0.0001153526, val loss: 0.0003475004\n",
            "[INFO] epoch: 1114, train loss: 0.0001258614, val loss: 0.0003272738\n",
            "[INFO] epoch: 1115, train loss: 0.0001151910, val loss: 0.0003472946\n",
            "[INFO] epoch: 1116, train loss: 0.0001256807, val loss: 0.0003259778\n",
            "[INFO] epoch: 1117, train loss: 0.0001150306, val loss: 0.0003470869\n",
            "[INFO] epoch: 1118, train loss: 0.0001255011, val loss: 0.0003246911\n",
            "[INFO] epoch: 1119, train loss: 0.0001148705, val loss: 0.0003468759\n",
            "[INFO] epoch: 1120, train loss: 0.0001253215, val loss: 0.0003234099\n",
            "[INFO] epoch: 1121, train loss: 0.0001147108, val loss: 0.0003466603\n",
            "[INFO] epoch: 1122, train loss: 0.0001251424, val loss: 0.0003221351\n",
            "[INFO] epoch: 1123, train loss: 0.0001145511, val loss: 0.0003464405\n",
            "[INFO] epoch: 1124, train loss: 0.0001249631, val loss: 0.0003208658\n",
            "[INFO] epoch: 1125, train loss: 0.0001143915, val loss: 0.0003462180\n",
            "[INFO] epoch: 1126, train loss: 0.0001247841, val loss: 0.0003196033\n",
            "[INFO] epoch: 1127, train loss: 0.0001142329, val loss: 0.0003459914\n",
            "[INFO] epoch: 1128, train loss: 0.0001246067, val loss: 0.0003183482\n",
            "[INFO] epoch: 1129, train loss: 0.0001140752, val loss: 0.0003457621\n",
            "[INFO] epoch: 1130, train loss: 0.0001244298, val loss: 0.0003170998\n",
            "[INFO] epoch: 1131, train loss: 0.0001139177, val loss: 0.0003455291\n",
            "[INFO] epoch: 1132, train loss: 0.0001242535, val loss: 0.0003158567\n",
            "[INFO] epoch: 1133, train loss: 0.0001137612, val loss: 0.0003452956\n",
            "[INFO] epoch: 1134, train loss: 0.0001240775, val loss: 0.0003146216\n",
            "[INFO] epoch: 1135, train loss: 0.0001136045, val loss: 0.0003450553\n",
            "[INFO] epoch: 1136, train loss: 0.0001239017, val loss: 0.0003133920\n",
            "[INFO] epoch: 1137, train loss: 0.0001134483, val loss: 0.0003448117\n",
            "[INFO] epoch: 1138, train loss: 0.0001237270, val loss: 0.0003121679\n",
            "[INFO] epoch: 1139, train loss: 0.0001132936, val loss: 0.0003445673\n",
            "[INFO] epoch: 1140, train loss: 0.0001235540, val loss: 0.0003109527\n",
            "[INFO] epoch: 1141, train loss: 0.0001131398, val loss: 0.0003443198\n",
            "[INFO] epoch: 1142, train loss: 0.0001233814, val loss: 0.0003097438\n",
            "[INFO] epoch: 1143, train loss: 0.0001129864, val loss: 0.0003440680\n",
            "[INFO] epoch: 1144, train loss: 0.0001232094, val loss: 0.0003085419\n",
            "[INFO] epoch: 1145, train loss: 0.0001128330, val loss: 0.0003438128\n",
            "[INFO] epoch: 1146, train loss: 0.0001230365, val loss: 0.0003073439\n",
            "[INFO] epoch: 1147, train loss: 0.0001126790, val loss: 0.0003435520\n",
            "[INFO] epoch: 1148, train loss: 0.0001228634, val loss: 0.0003061511\n",
            "[INFO] epoch: 1149, train loss: 0.0001125250, val loss: 0.0003432883\n",
            "[INFO] epoch: 1150, train loss: 0.0001226906, val loss: 0.0003049621\n",
            "[INFO] epoch: 1151, train loss: 0.0001123717, val loss: 0.0003430219\n",
            "[INFO] epoch: 1152, train loss: 0.0001225184, val loss: 0.0003037803\n",
            "[INFO] epoch: 1153, train loss: 0.0001122187, val loss: 0.0003427516\n",
            "[INFO] epoch: 1154, train loss: 0.0001223468, val loss: 0.0003026032\n",
            "[INFO] epoch: 1155, train loss: 0.0001120661, val loss: 0.0003424803\n",
            "[INFO] epoch: 1156, train loss: 0.0001221751, val loss: 0.0003014313\n",
            "[INFO] epoch: 1157, train loss: 0.0001119138, val loss: 0.0003422041\n",
            "[INFO] epoch: 1158, train loss: 0.0001220046, val loss: 0.0003002665\n",
            "[INFO] epoch: 1159, train loss: 0.0001117627, val loss: 0.0003419273\n",
            "[INFO] epoch: 1160, train loss: 0.0001218350, val loss: 0.0002991073\n",
            "[INFO] epoch: 1161, train loss: 0.0001116120, val loss: 0.0003416474\n",
            "[INFO] epoch: 1162, train loss: 0.0001216657, val loss: 0.0002979538\n",
            "[INFO] epoch: 1163, train loss: 0.0001114614, val loss: 0.0003413633\n",
            "[INFO] epoch: 1164, train loss: 0.0001214967, val loss: 0.0002968042\n",
            "[INFO] epoch: 1165, train loss: 0.0001113118, val loss: 0.0003410778\n",
            "[INFO] epoch: 1166, train loss: 0.0001213288, val loss: 0.0002956629\n",
            "[INFO] epoch: 1167, train loss: 0.0001111625, val loss: 0.0003407893\n",
            "[INFO] epoch: 1168, train loss: 0.0001211611, val loss: 0.0002945250\n",
            "[INFO] epoch: 1169, train loss: 0.0001110137, val loss: 0.0003404967\n",
            "[INFO] epoch: 1170, train loss: 0.0001209941, val loss: 0.0002933939\n",
            "[INFO] epoch: 1171, train loss: 0.0001108652, val loss: 0.0003402019\n",
            "[INFO] epoch: 1172, train loss: 0.0001208272, val loss: 0.0002922673\n",
            "[INFO] epoch: 1173, train loss: 0.0001107171, val loss: 0.0003399044\n",
            "[INFO] epoch: 1174, train loss: 0.0001206610, val loss: 0.0002911462\n",
            "[INFO] epoch: 1175, train loss: 0.0001105697, val loss: 0.0003396046\n",
            "[INFO] epoch: 1176, train loss: 0.0001204951, val loss: 0.0002900298\n",
            "[INFO] epoch: 1177, train loss: 0.0001104225, val loss: 0.0003393007\n",
            "[INFO] epoch: 1178, train loss: 0.0001203303, val loss: 0.0002889201\n",
            "[INFO] epoch: 1179, train loss: 0.0001102763, val loss: 0.0003389955\n",
            "[INFO] epoch: 1180, train loss: 0.0001201660, val loss: 0.0002878145\n",
            "[INFO] epoch: 1181, train loss: 0.0001101304, val loss: 0.0003386872\n",
            "[INFO] epoch: 1182, train loss: 0.0001200019, val loss: 0.0002867143\n",
            "[INFO] epoch: 1183, train loss: 0.0001099844, val loss: 0.0003383758\n",
            "[INFO] epoch: 1184, train loss: 0.0001198379, val loss: 0.0002856187\n",
            "[INFO] epoch: 1185, train loss: 0.0001098389, val loss: 0.0003380609\n",
            "[INFO] epoch: 1186, train loss: 0.0001196741, val loss: 0.0002845268\n",
            "[INFO] epoch: 1187, train loss: 0.0001096933, val loss: 0.0003377438\n",
            "[INFO] epoch: 1188, train loss: 0.0001195104, val loss: 0.0002834396\n",
            "[INFO] epoch: 1189, train loss: 0.0001095481, val loss: 0.0003374244\n",
            "[INFO] epoch: 1190, train loss: 0.0001193470, val loss: 0.0002823573\n",
            "[INFO] epoch: 1191, train loss: 0.0001094028, val loss: 0.0003371014\n",
            "[INFO] epoch: 1192, train loss: 0.0001191837, val loss: 0.0002812788\n",
            "[INFO] epoch: 1193, train loss: 0.0001092579, val loss: 0.0003367772\n",
            "[INFO] epoch: 1194, train loss: 0.0001190204, val loss: 0.0002802044\n",
            "[INFO] epoch: 1195, train loss: 0.0001091130, val loss: 0.0003364485\n",
            "[INFO] epoch: 1196, train loss: 0.0001188576, val loss: 0.0002791339\n",
            "[INFO] epoch: 1197, train loss: 0.0001089689, val loss: 0.0003361186\n",
            "[INFO] epoch: 1198, train loss: 0.0001186958, val loss: 0.0002780693\n",
            "[INFO] epoch: 1199, train loss: 0.0001088255, val loss: 0.0003357874\n",
            "[INFO] epoch: 1200, train loss: 0.0001185348, val loss: 0.0002770088\n",
            "[INFO] epoch: 1201, train loss: 0.0001086827, val loss: 0.0003354536\n",
            "[INFO] epoch: 1202, train loss: 0.0001183747, val loss: 0.0002759544\n",
            "[INFO] epoch: 1203, train loss: 0.0001085408, val loss: 0.0003351188\n",
            "[INFO] epoch: 1204, train loss: 0.0001182151, val loss: 0.0002749038\n",
            "[INFO] epoch: 1205, train loss: 0.0001083991, val loss: 0.0003347810\n",
            "[INFO] epoch: 1206, train loss: 0.0001180559, val loss: 0.0002738582\n",
            "[INFO] epoch: 1207, train loss: 0.0001082576, val loss: 0.0003344405\n",
            "[INFO] epoch: 1208, train loss: 0.0001178968, val loss: 0.0002728162\n",
            "[INFO] epoch: 1209, train loss: 0.0001081166, val loss: 0.0003340979\n",
            "[INFO] epoch: 1210, train loss: 0.0001177380, val loss: 0.0002717784\n",
            "[INFO] epoch: 1211, train loss: 0.0001079757, val loss: 0.0003337516\n",
            "[INFO] epoch: 1212, train loss: 0.0001175799, val loss: 0.0002707443\n",
            "[INFO] epoch: 1213, train loss: 0.0001078355, val loss: 0.0003334052\n",
            "[INFO] epoch: 1214, train loss: 0.0001174222, val loss: 0.0002697163\n",
            "[INFO] epoch: 1215, train loss: 0.0001076949, val loss: 0.0003330543\n",
            "[INFO] epoch: 1216, train loss: 0.0001172636, val loss: 0.0002686879\n",
            "[INFO] epoch: 1217, train loss: 0.0001075545, val loss: 0.0003327026\n",
            "[INFO] epoch: 1218, train loss: 0.0001171056, val loss: 0.0002676656\n",
            "[INFO] epoch: 1219, train loss: 0.0001074141, val loss: 0.0003323465\n",
            "[INFO] epoch: 1220, train loss: 0.0001169479, val loss: 0.0002666464\n",
            "[INFO] epoch: 1221, train loss: 0.0001072743, val loss: 0.0003319911\n",
            "[INFO] epoch: 1222, train loss: 0.0001167906, val loss: 0.0002656312\n",
            "[INFO] epoch: 1223, train loss: 0.0001071345, val loss: 0.0003316315\n",
            "[INFO] epoch: 1224, train loss: 0.0001166331, val loss: 0.0002646180\n",
            "[INFO] epoch: 1225, train loss: 0.0001069948, val loss: 0.0003312718\n",
            "[INFO] epoch: 1226, train loss: 0.0001164762, val loss: 0.0002636090\n",
            "[INFO] epoch: 1227, train loss: 0.0001068560, val loss: 0.0003309073\n",
            "[INFO] epoch: 1228, train loss: 0.0001163202, val loss: 0.0002626036\n",
            "[INFO] epoch: 1229, train loss: 0.0001067176, val loss: 0.0003305469\n",
            "[INFO] epoch: 1230, train loss: 0.0001161651, val loss: 0.0002616038\n",
            "[INFO] epoch: 1231, train loss: 0.0001065803, val loss: 0.0003301759\n",
            "[INFO] epoch: 1232, train loss: 0.0001160105, val loss: 0.0002606072\n",
            "[INFO] epoch: 1233, train loss: 0.0001064426, val loss: 0.0003298178\n",
            "[INFO] epoch: 1234, train loss: 0.0001158555, val loss: 0.0002596134\n",
            "[INFO] epoch: 1235, train loss: 0.0001063057, val loss: 0.0003294311\n",
            "[INFO] epoch: 1236, train loss: 0.0001157022, val loss: 0.0002586226\n",
            "[INFO] epoch: 1237, train loss: 0.0001061695, val loss: 0.0003290875\n",
            "[INFO] epoch: 1238, train loss: 0.0001155492, val loss: 0.0002576379\n",
            "[INFO] epoch: 1239, train loss: 0.0001060357, val loss: 0.0003286735\n",
            "[INFO] epoch: 1240, train loss: 0.0001154016, val loss: 0.0002566547\n",
            "[INFO] epoch: 1241, train loss: 0.0001059062, val loss: 0.0003283641\n",
            "[INFO] epoch: 1242, train loss: 0.0001152574, val loss: 0.0002556830\n",
            "[INFO] epoch: 1243, train loss: 0.0001057816, val loss: 0.0003279260\n",
            "[INFO] epoch: 1244, train loss: 0.0001151225, val loss: 0.0002547325\n",
            "[INFO] epoch: 1245, train loss: 0.0001056592, val loss: 0.0003276340\n",
            "[INFO] epoch: 1246, train loss: 0.0001149789, val loss: 0.0002537716\n",
            "[INFO] epoch: 1247, train loss: 0.0001055273, val loss: 0.0003271953\n",
            "[INFO] epoch: 1248, train loss: 0.0001148289, val loss: 0.0002528276\n",
            "[INFO] epoch: 1249, train loss: 0.0001053866, val loss: 0.0003268322\n",
            "[INFO] epoch: 1250, train loss: 0.0001146602, val loss: 0.0002518458\n",
            "[INFO] epoch: 1251, train loss: 0.0001052356, val loss: 0.0003263976\n",
            "[INFO] epoch: 1252, train loss: 0.0001144904, val loss: 0.0002508615\n",
            "[INFO] epoch: 1253, train loss: 0.0001050844, val loss: 0.0003260063\n",
            "[INFO] epoch: 1254, train loss: 0.0001143176, val loss: 0.0002498681\n",
            "[INFO] epoch: 1255, train loss: 0.0001049340, val loss: 0.0003255923\n",
            "[INFO] epoch: 1256, train loss: 0.0001141507, val loss: 0.0002488892\n",
            "[INFO] epoch: 1257, train loss: 0.0001047872, val loss: 0.0003251962\n",
            "[INFO] epoch: 1258, train loss: 0.0001139851, val loss: 0.0002479080\n",
            "[INFO] epoch: 1259, train loss: 0.0001046427, val loss: 0.0003247964\n",
            "[INFO] epoch: 1260, train loss: 0.0001138247, val loss: 0.0002469303\n",
            "[INFO] epoch: 1261, train loss: 0.0001045034, val loss: 0.0003244138\n",
            "[INFO] epoch: 1262, train loss: 0.0001136708, val loss: 0.0002459630\n",
            "[INFO] epoch: 1263, train loss: 0.0001043691, val loss: 0.0003240303\n",
            "[INFO] epoch: 1264, train loss: 0.0001135220, val loss: 0.0002450134\n",
            "[INFO] epoch: 1265, train loss: 0.0001042390, val loss: 0.0003236472\n",
            "[INFO] epoch: 1266, train loss: 0.0001133783, val loss: 0.0002440734\n",
            "[INFO] epoch: 1267, train loss: 0.0001041121, val loss: 0.0003232615\n",
            "[INFO] epoch: 1268, train loss: 0.0001132369, val loss: 0.0002431337\n",
            "[INFO] epoch: 1269, train loss: 0.0001039874, val loss: 0.0003228883\n",
            "[INFO] epoch: 1270, train loss: 0.0001130982, val loss: 0.0002422012\n",
            "[INFO] epoch: 1271, train loss: 0.0001038637, val loss: 0.0003224985\n",
            "[INFO] epoch: 1272, train loss: 0.0001129601, val loss: 0.0002412809\n",
            "[INFO] epoch: 1273, train loss: 0.0001037411, val loss: 0.0003221108\n",
            "[INFO] epoch: 1274, train loss: 0.0001128232, val loss: 0.0002403736\n",
            "[INFO] epoch: 1275, train loss: 0.0001036170, val loss: 0.0003217010\n",
            "[INFO] epoch: 1276, train loss: 0.0001126811, val loss: 0.0002394441\n",
            "[INFO] epoch: 1277, train loss: 0.0001034904, val loss: 0.0003213188\n",
            "[INFO] epoch: 1278, train loss: 0.0001125397, val loss: 0.0002385256\n",
            "[INFO] epoch: 1279, train loss: 0.0001033625, val loss: 0.0003208982\n",
            "[INFO] epoch: 1280, train loss: 0.0001123935, val loss: 0.0002375953\n",
            "[INFO] epoch: 1281, train loss: 0.0001032341, val loss: 0.0003205076\n",
            "[INFO] epoch: 1282, train loss: 0.0001122524, val loss: 0.0002367141\n",
            "[INFO] epoch: 1283, train loss: 0.0001031054, val loss: 0.0003200450\n",
            "[INFO] epoch: 1284, train loss: 0.0001121018, val loss: 0.0002357623\n",
            "[INFO] epoch: 1285, train loss: 0.0001029755, val loss: 0.0003196912\n",
            "[INFO] epoch: 1286, train loss: 0.0001119643, val loss: 0.0002348745\n",
            "[INFO] epoch: 1287, train loss: 0.0001028517, val loss: 0.0003192181\n",
            "[INFO] epoch: 1288, train loss: 0.0001118206, val loss: 0.0002339174\n",
            "[INFO] epoch: 1289, train loss: 0.0001027309, val loss: 0.0003189046\n",
            "[INFO] epoch: 1290, train loss: 0.0001116987, val loss: 0.0002331158\n",
            "[INFO] epoch: 1291, train loss: 0.0001026173, val loss: 0.0003183449\n",
            "[INFO] epoch: 1292, train loss: 0.0001115564, val loss: 0.0002321773\n",
            "[INFO] epoch: 1293, train loss: 0.0001024920, val loss: 0.0003180765\n",
            "[INFO] epoch: 1294, train loss: 0.0001114238, val loss: 0.0002313046\n",
            "[INFO] epoch: 1295, train loss: 0.0001023632, val loss: 0.0003175378\n",
            "[INFO] epoch: 1296, train loss: 0.0001112575, val loss: 0.0002303163\n",
            "[INFO] epoch: 1297, train loss: 0.0001022139, val loss: 0.0003172305\n",
            "[INFO] epoch: 1298, train loss: 0.0001110954, val loss: 0.0002294566\n",
            "[INFO] epoch: 1299, train loss: 0.0001020616, val loss: 0.0003166026\n",
            "[INFO] epoch: 1300, train loss: 0.0001109045, val loss: 0.0002285253\n",
            "[INFO] epoch: 1301, train loss: 0.0001018929, val loss: 0.0003161992\n",
            "[INFO] epoch: 1302, train loss: 0.0001107171, val loss: 0.0002275197\n",
            "[INFO] epoch: 1303, train loss: 0.0001017281, val loss: 0.0003157483\n",
            "[INFO] epoch: 1304, train loss: 0.0001105286, val loss: 0.0002264370\n",
            "[INFO] epoch: 1305, train loss: 0.0001015725, val loss: 0.0003154374\n",
            "[INFO] epoch: 1306, train loss: 0.0001103693, val loss: 0.0002255419\n",
            "[INFO] epoch: 1307, train loss: 0.0001014377, val loss: 0.0003148880\n",
            "[INFO] epoch: 1308, train loss: 0.0001102192, val loss: 0.0002247784\n",
            "[INFO] epoch: 1309, train loss: 0.0001013046, val loss: 0.0003143730\n",
            "[INFO] epoch: 1310, train loss: 0.0001100676, val loss: 0.0002238560\n",
            "[INFO] epoch: 1311, train loss: 0.0001011719, val loss: 0.0003140426\n",
            "[INFO] epoch: 1312, train loss: 0.0001099213, val loss: 0.0002226568\n",
            "[INFO] epoch: 1313, train loss: 0.0001010536, val loss: 0.0003139703\n",
            "[INFO] epoch: 1314, train loss: 0.0001098108, val loss: 0.0002216772\n",
            "[INFO] epoch: 1315, train loss: 0.0001009668, val loss: 0.0003135274\n",
            "[INFO] epoch: 1316, train loss: 0.0001097298, val loss: 0.0002212760\n",
            "[INFO] epoch: 1317, train loss: 0.0001008837, val loss: 0.0003126748\n",
            "[INFO] epoch: 1318, train loss: 0.0001096125, val loss: 0.0002208077\n",
            "[INFO] epoch: 1319, train loss: 0.0001007552, val loss: 0.0003120794\n",
            "[INFO] epoch: 1320, train loss: 0.0001094460, val loss: 0.0002193339\n",
            "[INFO] epoch: 1321, train loss: 0.0001006220, val loss: 0.0003125112\n",
            "[INFO] epoch: 1322, train loss: 0.0001093215, val loss: 0.0002175542\n",
            "[INFO] epoch: 1323, train loss: 0.0001005273, val loss: 0.0003127214\n",
            "[INFO] epoch: 1324, train loss: 0.0001092738, val loss: 0.0002172969\n",
            "[INFO] epoch: 1325, train loss: 0.0001005204, val loss: 0.0003112318\n",
            "[INFO] epoch: 1326, train loss: 0.0001092713, val loss: 0.0002185774\n",
            "[INFO] epoch: 1327, train loss: 0.0001003802, val loss: 0.0003089970\n",
            "[INFO] epoch: 1328, train loss: 0.0001089352, val loss: 0.0002174578\n",
            "[INFO] epoch: 1329, train loss: 0.0001001280, val loss: 0.0003096177\n",
            "[INFO] epoch: 1330, train loss: 0.0001087707, val loss: 0.0002130670\n",
            "[INFO] epoch: 1331, train loss: 0.0000999772, val loss: 0.0003131781\n",
            "[INFO] epoch: 1332, train loss: 0.0001085703, val loss: 0.0002102618\n",
            "[INFO] epoch: 1333, train loss: 0.0000999385, val loss: 0.0003122837\n",
            "[INFO] epoch: 1334, train loss: 0.0001091163, val loss: 0.0002153715\n",
            "[INFO] epoch: 1335, train loss: 0.0001005156, val loss: 0.0003054955\n",
            "[INFO] epoch: 1336, train loss: 0.0001090532, val loss: 0.0002213397\n",
            "[INFO] epoch: 1337, train loss: 0.0000993391, val loss: 0.0003008374\n",
            "[INFO] epoch: 1338, train loss: 0.0001077263, val loss: 0.0002109992\n",
            "[INFO] epoch: 1339, train loss: 0.0000999518, val loss: 0.0003120786\n",
            "[INFO] epoch: 1340, train loss: 0.0001091984, val loss: 0.0002021811\n",
            "[INFO] epoch: 1341, train loss: 0.0000988402, val loss: 0.0003200225\n",
            "[INFO] epoch: 1342, train loss: 0.0001075969, val loss: 0.0002018990\n",
            "[INFO] epoch: 1343, train loss: 0.0001011905, val loss: 0.0003081160\n",
            "[INFO] epoch: 1344, train loss: 0.0001124931, val loss: 0.0002272662\n",
            "[INFO] epoch: 1345, train loss: 0.0001009909, val loss: 0.0002961938\n",
            "[INFO] epoch: 1346, train loss: 0.0001079450, val loss: 0.0002165214\n",
            "[INFO] epoch: 1347, train loss: 0.0000995365, val loss: 0.0003022096\n",
            "[INFO] epoch: 1348, train loss: 0.0001095227, val loss: 0.0002020962\n",
            "[INFO] epoch: 1349, train loss: 0.0000998286, val loss: 0.0003201668\n",
            "[INFO] epoch: 1350, train loss: 0.0001081411, val loss: 0.0001971957\n",
            "[INFO] epoch: 1351, train loss: 0.0000998135, val loss: 0.0003134635\n",
            "[INFO] epoch: 1352, train loss: 0.0001101571, val loss: 0.0002118981\n",
            "[INFO] epoch: 1353, train loss: 0.0001012092, val loss: 0.0003029179\n",
            "[INFO] epoch: 1354, train loss: 0.0001095597, val loss: 0.0002158653\n",
            "[INFO] epoch: 1355, train loss: 0.0000996805, val loss: 0.0003008053\n",
            "[INFO] epoch: 1356, train loss: 0.0001078381, val loss: 0.0002044998\n",
            "[INFO] epoch: 1357, train loss: 0.0000989503, val loss: 0.0003087756\n",
            "[INFO] epoch: 1358, train loss: 0.0001072786, val loss: 0.0001990651\n",
            "[INFO] epoch: 1359, train loss: 0.0000983343, val loss: 0.0003098100\n",
            "[INFO] epoch: 1360, train loss: 0.0001069008, val loss: 0.0002014656\n",
            "[INFO] epoch: 1361, train loss: 0.0000985041, val loss: 0.0003040506\n",
            "[INFO] epoch: 1362, train loss: 0.0001069800, val loss: 0.0002059722\n",
            "[INFO] epoch: 1363, train loss: 0.0000979859, val loss: 0.0003004197\n",
            "[INFO] epoch: 1364, train loss: 0.0001058797, val loss: 0.0002018696\n",
            "[INFO] epoch: 1365, train loss: 0.0000972257, val loss: 0.0003023865\n",
            "[INFO] epoch: 1366, train loss: 0.0001052598, val loss: 0.0001969520\n",
            "[INFO] epoch: 1367, train loss: 0.0000967791, val loss: 0.0003051031\n",
            "[INFO] epoch: 1368, train loss: 0.0001049306, val loss: 0.0001954339\n",
            "[INFO] epoch: 1369, train loss: 0.0000967394, val loss: 0.0003035427\n",
            "[INFO] epoch: 1370, train loss: 0.0001050787, val loss: 0.0001977580\n",
            "[INFO] epoch: 1371, train loss: 0.0000967997, val loss: 0.0003010128\n",
            "[INFO] epoch: 1372, train loss: 0.0001049546, val loss: 0.0001976389\n",
            "[INFO] epoch: 1373, train loss: 0.0000966103, val loss: 0.0003005832\n",
            "[INFO] epoch: 1374, train loss: 0.0001047297, val loss: 0.0001949372\n",
            "[INFO] epoch: 1375, train loss: 0.0000964930, val loss: 0.0003028561\n",
            "[INFO] epoch: 1376, train loss: 0.0001047393, val loss: 0.0001926318\n",
            "[INFO] epoch: 1377, train loss: 0.0000965903, val loss: 0.0003034993\n",
            "[INFO] epoch: 1378, train loss: 0.0001049865, val loss: 0.0001933135\n",
            "[INFO] epoch: 1379, train loss: 0.0000968410, val loss: 0.0003021545\n",
            "[INFO] epoch: 1380, train loss: 0.0001052533, val loss: 0.0001946117\n",
            "[INFO] epoch: 1381, train loss: 0.0000969549, val loss: 0.0003004375\n",
            "[INFO] epoch: 1382, train loss: 0.0001052383, val loss: 0.0001939852\n",
            "[INFO] epoch: 1383, train loss: 0.0000968801, val loss: 0.0003010545\n",
            "[INFO] epoch: 1384, train loss: 0.0001051621, val loss: 0.0001916554\n",
            "[INFO] epoch: 1385, train loss: 0.0000968447, val loss: 0.0003021935\n",
            "[INFO] epoch: 1386, train loss: 0.0001051871, val loss: 0.0001906732\n",
            "[INFO] epoch: 1387, train loss: 0.0000968811, val loss: 0.0003018413\n",
            "[INFO] epoch: 1388, train loss: 0.0001052615, val loss: 0.0001914372\n",
            "[INFO] epoch: 1389, train loss: 0.0000969173, val loss: 0.0002995581\n",
            "[INFO] epoch: 1390, train loss: 0.0001051930, val loss: 0.0001921170\n",
            "[INFO] epoch: 1391, train loss: 0.0000967183, val loss: 0.0002982890\n",
            "[INFO] epoch: 1392, train loss: 0.0001048422, val loss: 0.0001902756\n",
            "[INFO] epoch: 1393, train loss: 0.0000964277, val loss: 0.0002988728\n",
            "[INFO] epoch: 1394, train loss: 0.0001045625, val loss: 0.0001878314\n",
            "[INFO] epoch: 1395, train loss: 0.0000962088, val loss: 0.0002998125\n",
            "[INFO] epoch: 1396, train loss: 0.0001043713, val loss: 0.0001868911\n",
            "[INFO] epoch: 1397, train loss: 0.0000961118, val loss: 0.0002983837\n",
            "[INFO] epoch: 1398, train loss: 0.0001043107, val loss: 0.0001880663\n",
            "[INFO] epoch: 1399, train loss: 0.0000959934, val loss: 0.0002958468\n",
            "[INFO] epoch: 1400, train loss: 0.0001040156, val loss: 0.0001879980\n",
            "[INFO] epoch: 1401, train loss: 0.0000956461, val loss: 0.0002948963\n",
            "[INFO] epoch: 1402, train loss: 0.0001035883, val loss: 0.0001853408\n",
            "[INFO] epoch: 1403, train loss: 0.0000953527, val loss: 0.0002966807\n",
            "[INFO] epoch: 1404, train loss: 0.0001033602, val loss: 0.0001823132\n",
            "[INFO] epoch: 1405, train loss: 0.0000952047, val loss: 0.0002979127\n",
            "[INFO] epoch: 1406, train loss: 0.0001033195, val loss: 0.0001822210\n",
            "[INFO] epoch: 1407, train loss: 0.0000952945, val loss: 0.0002958061\n",
            "[INFO] epoch: 1408, train loss: 0.0001034824, val loss: 0.0001846547\n",
            "[INFO] epoch: 1409, train loss: 0.0000952819, val loss: 0.0002922831\n",
            "[INFO] epoch: 1410, train loss: 0.0001032059, val loss: 0.0001848140\n",
            "[INFO] epoch: 1411, train loss: 0.0000949381, val loss: 0.0002920808\n",
            "[INFO] epoch: 1412, train loss: 0.0001028644, val loss: 0.0001804891\n",
            "[INFO] epoch: 1413, train loss: 0.0000948173, val loss: 0.0002962402\n",
            "[INFO] epoch: 1414, train loss: 0.0001029056, val loss: 0.0001766728\n",
            "[INFO] epoch: 1415, train loss: 0.0000948062, val loss: 0.0002987042\n",
            "[INFO] epoch: 1416, train loss: 0.0001030479, val loss: 0.0001778040\n",
            "[INFO] epoch: 1417, train loss: 0.0000952813, val loss: 0.0002939013\n",
            "[INFO] epoch: 1418, train loss: 0.0001037790, val loss: 0.0001844679\n",
            "[INFO] epoch: 1419, train loss: 0.0000953143, val loss: 0.0002875704\n",
            "[INFO] epoch: 1420, train loss: 0.0001029420, val loss: 0.0001843537\n",
            "[INFO] epoch: 1421, train loss: 0.0000944916, val loss: 0.0002875248\n",
            "[INFO] epoch: 1422, train loss: 0.0001025231, val loss: 0.0001756394\n",
            "[INFO] epoch: 1423, train loss: 0.0000946383, val loss: 0.0002986579\n",
            "[INFO] epoch: 1424, train loss: 0.0001027880, val loss: 0.0001688355\n",
            "[INFO] epoch: 1425, train loss: 0.0000941546, val loss: 0.0003023543\n",
            "[INFO] epoch: 1426, train loss: 0.0001025768, val loss: 0.0001720696\n",
            "[INFO] epoch: 1427, train loss: 0.0000957504, val loss: 0.0002916678\n",
            "[INFO] epoch: 1428, train loss: 0.0001053342, val loss: 0.0001899881\n",
            "[INFO] epoch: 1429, train loss: 0.0000954992, val loss: 0.0002783599\n",
            "[INFO] epoch: 1430, train loss: 0.0001019475, val loss: 0.0001856708\n",
            "[INFO] epoch: 1431, train loss: 0.0000934474, val loss: 0.0002809050\n",
            "[INFO] epoch: 1432, train loss: 0.0001025077, val loss: 0.0001687985\n",
            "[INFO] epoch: 1433, train loss: 0.0000947543, val loss: 0.0003055132\n",
            "[INFO] epoch: 1434, train loss: 0.0001027372, val loss: 0.0001598022\n",
            "[INFO] epoch: 1435, train loss: 0.0000932379, val loss: 0.0003071938\n",
            "[INFO] epoch: 1436, train loss: 0.0001027695, val loss: 0.0001692454\n",
            "[INFO] epoch: 1437, train loss: 0.0000971066, val loss: 0.0002892354\n",
            "[INFO] epoch: 1438, train loss: 0.0001071780, val loss: 0.0001949267\n",
            "[INFO] epoch: 1439, train loss: 0.0000956966, val loss: 0.0002764209\n",
            "[INFO] epoch: 1440, train loss: 0.0001019607, val loss: 0.0001796692\n",
            "[INFO] epoch: 1441, train loss: 0.0000941755, val loss: 0.0002857443\n",
            "[INFO] epoch: 1442, train loss: 0.0001032879, val loss: 0.0001654106\n",
            "[INFO] epoch: 1443, train loss: 0.0000941772, val loss: 0.0003056253\n",
            "[INFO] epoch: 1444, train loss: 0.0001018221, val loss: 0.0001602059\n",
            "[INFO] epoch: 1445, train loss: 0.0000939729, val loss: 0.0002997362\n",
            "[INFO] epoch: 1446, train loss: 0.0001034305, val loss: 0.0001739663\n",
            "[INFO] epoch: 1447, train loss: 0.0000956015, val loss: 0.0002869981\n",
            "[INFO] epoch: 1448, train loss: 0.0001035461, val loss: 0.0001815634\n",
            "[INFO] epoch: 1449, train loss: 0.0000940776, val loss: 0.0002815770\n",
            "[INFO] epoch: 1450, train loss: 0.0001012512, val loss: 0.0001707727\n",
            "[INFO] epoch: 1451, train loss: 0.0000931362, val loss: 0.0002900205\n",
            "[INFO] epoch: 1452, train loss: 0.0001008942, val loss: 0.0001626217\n",
            "[INFO] epoch: 1453, train loss: 0.0000924804, val loss: 0.0002961481\n",
            "[INFO] epoch: 1454, train loss: 0.0001001762, val loss: 0.0001623452\n",
            "[INFO] epoch: 1455, train loss: 0.0000926296, val loss: 0.0002911846\n",
            "[INFO] epoch: 1456, train loss: 0.0001008000, val loss: 0.0001694615\n",
            "[INFO] epoch: 1457, train loss: 0.0000927603, val loss: 0.0002838921\n",
            "[INFO] epoch: 1458, train loss: 0.0001001281, val loss: 0.0001699672\n",
            "[INFO] epoch: 1459, train loss: 0.0000918930, val loss: 0.0002831626\n",
            "[INFO] epoch: 1460, train loss: 0.0000991912, val loss: 0.0001634695\n",
            "[INFO] epoch: 1461, train loss: 0.0000914008, val loss: 0.0002883902\n",
            "[INFO] epoch: 1462, train loss: 0.0000989295, val loss: 0.0001589744\n",
            "[INFO] epoch: 1463, train loss: 0.0000912248, val loss: 0.0002913124\n",
            "[INFO] epoch: 1464, train loss: 0.0000989615, val loss: 0.0001599501\n",
            "[INFO] epoch: 1465, train loss: 0.0000915840, val loss: 0.0002874914\n",
            "[INFO] epoch: 1466, train loss: 0.0000994763, val loss: 0.0001642815\n",
            "[INFO] epoch: 1467, train loss: 0.0000917339, val loss: 0.0002835752\n",
            "[INFO] epoch: 1468, train loss: 0.0000992247, val loss: 0.0001639729\n",
            "[INFO] epoch: 1469, train loss: 0.0000914102, val loss: 0.0002836111\n",
            "[INFO] epoch: 1470, train loss: 0.0000989366, val loss: 0.0001595994\n",
            "[INFO] epoch: 1471, train loss: 0.0000913089, val loss: 0.0002883608\n",
            "[INFO] epoch: 1472, train loss: 0.0000990153, val loss: 0.0001566332\n",
            "[INFO] epoch: 1473, train loss: 0.0000914196, val loss: 0.0002897059\n",
            "[INFO] epoch: 1474, train loss: 0.0000992951, val loss: 0.0001581022\n",
            "[INFO] epoch: 1475, train loss: 0.0000918352, val loss: 0.0002867331\n",
            "[INFO] epoch: 1476, train loss: 0.0000997776, val loss: 0.0001620141\n",
            "[INFO] epoch: 1477, train loss: 0.0000919401, val loss: 0.0002821945\n",
            "[INFO] epoch: 1478, train loss: 0.0000994671, val loss: 0.0001616010\n",
            "[INFO] epoch: 1479, train loss: 0.0000915641, val loss: 0.0002829700\n",
            "[INFO] epoch: 1480, train loss: 0.0000991323, val loss: 0.0001572854\n",
            "[INFO] epoch: 1481, train loss: 0.0000913928, val loss: 0.0002868137\n",
            "[INFO] epoch: 1482, train loss: 0.0000990529, val loss: 0.0001539043\n",
            "[INFO] epoch: 1483, train loss: 0.0000913275, val loss: 0.0002889206\n",
            "[INFO] epoch: 1484, train loss: 0.0000991581, val loss: 0.0001555554\n",
            "[INFO] epoch: 1485, train loss: 0.0000916453, val loss: 0.0002840211\n",
            "[INFO] epoch: 1486, train loss: 0.0000995024, val loss: 0.0001599902\n",
            "[INFO] epoch: 1487, train loss: 0.0000915455, val loss: 0.0002790789\n",
            "[INFO] epoch: 1488, train loss: 0.0000988726, val loss: 0.0001598829\n",
            "[INFO] epoch: 1489, train loss: 0.0000908916, val loss: 0.0002786581\n",
            "[INFO] epoch: 1490, train loss: 0.0000982687, val loss: 0.0001536436\n",
            "[INFO] epoch: 1491, train loss: 0.0000906166, val loss: 0.0002853818\n",
            "[INFO] epoch: 1492, train loss: 0.0000981744, val loss: 0.0001490211\n",
            "[INFO] epoch: 1493, train loss: 0.0000903985, val loss: 0.0002885624\n",
            "[INFO] epoch: 1494, train loss: 0.0000980624, val loss: 0.0001500292\n",
            "[INFO] epoch: 1495, train loss: 0.0000908817, val loss: 0.0002826013\n",
            "[INFO] epoch: 1496, train loss: 0.0000989729, val loss: 0.0001584975\n",
            "[INFO] epoch: 1497, train loss: 0.0000909937, val loss: 0.0002739996\n",
            "[INFO] epoch: 1498, train loss: 0.0000979767, val loss: 0.0001594313\n",
            "[INFO] epoch: 1499, train loss: 0.0000897896, val loss: 0.0002723851\n",
            "[INFO] epoch: 1500, train loss: 0.0000970022, val loss: 0.0001501773\n",
            "[INFO] epoch: 1501, train loss: 0.0000897891, val loss: 0.0002843747\n",
            "[INFO] epoch: 1502, train loss: 0.0000975653, val loss: 0.0001418992\n",
            "[INFO] epoch: 1503, train loss: 0.0000893941, val loss: 0.0002933250\n",
            "[INFO] epoch: 1504, train loss: 0.0000968652, val loss: 0.0001417111\n",
            "[INFO] epoch: 1505, train loss: 0.0000903805, val loss: 0.0002848620\n",
            "[INFO] epoch: 1506, train loss: 0.0000996993, val loss: 0.0001582517\n",
            "[INFO] epoch: 1507, train loss: 0.0000918332, val loss: 0.0002698082\n",
            "[INFO] epoch: 1508, train loss: 0.0000986659, val loss: 0.0001651504\n",
            "[INFO] epoch: 1509, train loss: 0.0000891703, val loss: 0.0002635896\n",
            "[INFO] epoch: 1510, train loss: 0.0000960819, val loss: 0.0001482822\n",
            "[INFO] epoch: 1511, train loss: 0.0000898850, val loss: 0.0002833317\n",
            "[INFO] epoch: 1512, train loss: 0.0000988638, val loss: 0.0001370811\n",
            "[INFO] epoch: 1513, train loss: 0.0000893164, val loss: 0.0003028299\n",
            "[INFO] epoch: 1514, train loss: 0.0000962835, val loss: 0.0001331685\n",
            "[INFO] epoch: 1515, train loss: 0.0000903321, val loss: 0.0002899809\n",
            "[INFO] epoch: 1516, train loss: 0.0001013193, val loss: 0.0001585138\n",
            "[INFO] epoch: 1517, train loss: 0.0000935572, val loss: 0.0002713507\n",
            "[INFO] epoch: 1518, train loss: 0.0001005819, val loss: 0.0001688059\n",
            "[INFO] epoch: 1519, train loss: 0.0000900449, val loss: 0.0002629030\n",
            "[INFO] epoch: 1520, train loss: 0.0000968639, val loss: 0.0001478982\n",
            "[INFO] epoch: 1521, train loss: 0.0000903025, val loss: 0.0002835690\n",
            "[INFO] epoch: 1522, train loss: 0.0000990145, val loss: 0.0001367337\n",
            "[INFO] epoch: 1523, train loss: 0.0000895505, val loss: 0.0003002420\n",
            "[INFO] epoch: 1524, train loss: 0.0000968098, val loss: 0.0001352222\n",
            "[INFO] epoch: 1525, train loss: 0.0000903876, val loss: 0.0002888781\n",
            "[INFO] epoch: 1526, train loss: 0.0000999652, val loss: 0.0001544991\n",
            "[INFO] epoch: 1527, train loss: 0.0000919112, val loss: 0.0002739033\n",
            "[INFO] epoch: 1528, train loss: 0.0000988597, val loss: 0.0001586478\n",
            "[INFO] epoch: 1529, train loss: 0.0000897417, val loss: 0.0002694973\n",
            "[INFO] epoch: 1530, train loss: 0.0000965599, val loss: 0.0001451136\n",
            "[INFO] epoch: 1531, train loss: 0.0000890284, val loss: 0.0002816797\n",
            "[INFO] epoch: 1532, train loss: 0.0000964524, val loss: 0.0001366254\n",
            "[INFO] epoch: 1533, train loss: 0.0000882708, val loss: 0.0002896357\n",
            "[INFO] epoch: 1534, train loss: 0.0000954983, val loss: 0.0001369676\n",
            "[INFO] epoch: 1535, train loss: 0.0000885912, val loss: 0.0002825932\n",
            "[INFO] epoch: 1536, train loss: 0.0000965518, val loss: 0.0001467415\n",
            "[INFO] epoch: 1537, train loss: 0.0000889287, val loss: 0.0002732992\n",
            "[INFO] epoch: 1538, train loss: 0.0000957616, val loss: 0.0001480891\n",
            "[INFO] epoch: 1539, train loss: 0.0000877677, val loss: 0.0002709843\n",
            "[INFO] epoch: 1540, train loss: 0.0000944885, val loss: 0.0001402063\n",
            "[INFO] epoch: 1541, train loss: 0.0000871810, val loss: 0.0002784822\n",
            "[INFO] epoch: 1542, train loss: 0.0000942643, val loss: 0.0001340815\n",
            "[INFO] epoch: 1543, train loss: 0.0000868880, val loss: 0.0002836949\n",
            "[INFO] epoch: 1544, train loss: 0.0000940726, val loss: 0.0001343684\n",
            "[INFO] epoch: 1545, train loss: 0.0000872617, val loss: 0.0002802838\n",
            "[INFO] epoch: 1546, train loss: 0.0000948654, val loss: 0.0001405135\n",
            "[INFO] epoch: 1547, train loss: 0.0000877201, val loss: 0.0002735093\n",
            "[INFO] epoch: 1548, train loss: 0.0000947813, val loss: 0.0001426861\n",
            "[INFO] epoch: 1549, train loss: 0.0000872513, val loss: 0.0002718561\n",
            "[INFO] epoch: 1550, train loss: 0.0000941554, val loss: 0.0001375204\n",
            "[INFO] epoch: 1551, train loss: 0.0000870190, val loss: 0.0002771323\n",
            "[INFO] epoch: 1552, train loss: 0.0000942513, val loss: 0.0001325445\n",
            "[INFO] epoch: 1553, train loss: 0.0000870426, val loss: 0.0002829911\n",
            "[INFO] epoch: 1554, train loss: 0.0000943700, val loss: 0.0001321111\n",
            "[INFO] epoch: 1555, train loss: 0.0000874653, val loss: 0.0002804644\n",
            "[INFO] epoch: 1556, train loss: 0.0000951497, val loss: 0.0001377259\n",
            "[INFO] epoch: 1557, train loss: 0.0000880607, val loss: 0.0002743291\n",
            "[INFO] epoch: 1558, train loss: 0.0000953587, val loss: 0.0001413605\n",
            "[INFO] epoch: 1559, train loss: 0.0000876962, val loss: 0.0002704285\n",
            "[INFO] epoch: 1560, train loss: 0.0000945938, val loss: 0.0001374219\n",
            "[INFO] epoch: 1561, train loss: 0.0000873010, val loss: 0.0002751264\n",
            "[INFO] epoch: 1562, train loss: 0.0000945725, val loss: 0.0001312765\n",
            "[INFO] epoch: 1563, train loss: 0.0000872071, val loss: 0.0002820758\n",
            "[INFO] epoch: 1564, train loss: 0.0000944109, val loss: 0.0001290420\n",
            "[INFO] epoch: 1565, train loss: 0.0000872458, val loss: 0.0002814665\n",
            "[INFO] epoch: 1566, train loss: 0.0000948692, val loss: 0.0001343078\n",
            "[INFO] epoch: 1567, train loss: 0.0000879026, val loss: 0.0002729811\n",
            "[INFO] epoch: 1568, train loss: 0.0000952757, val loss: 0.0001411032\n",
            "[INFO] epoch: 1569, train loss: 0.0000873173, val loss: 0.0002659745\n",
            "[INFO] epoch: 1570, train loss: 0.0000937789, val loss: 0.0001376514\n",
            "[INFO] epoch: 1571, train loss: 0.0000863519, val loss: 0.0002688596\n",
            "[INFO] epoch: 1572, train loss: 0.0000934432, val loss: 0.0001285115\n",
            "[INFO] epoch: 1573, train loss: 0.0000862640, val loss: 0.0002809088\n",
            "[INFO] epoch: 1574, train loss: 0.0000933264, val loss: 0.0001228688\n",
            "[INFO] epoch: 1575, train loss: 0.0000858243, val loss: 0.0002846370\n",
            "[INFO] epoch: 1576, train loss: 0.0000931819, val loss: 0.0001264221\n",
            "[INFO] epoch: 1577, train loss: 0.0000870321, val loss: 0.0002744254\n",
            "[INFO] epoch: 1578, train loss: 0.0000952924, val loss: 0.0001408350\n",
            "[INFO] epoch: 1579, train loss: 0.0000873061, val loss: 0.0002616033\n",
            "[INFO] epoch: 1580, train loss: 0.0000933540, val loss: 0.0001415085\n",
            "[INFO] epoch: 1581, train loss: 0.0000851944, val loss: 0.0002592584\n",
            "[INFO] epoch: 1582, train loss: 0.0000920282, val loss: 0.0001274030\n",
            "[INFO] epoch: 1583, train loss: 0.0000858111, val loss: 0.0002785498\n",
            "[INFO] epoch: 1584, train loss: 0.0000937353, val loss: 0.0001174955\n",
            "[INFO] epoch: 1585, train loss: 0.0000851686, val loss: 0.0002938274\n",
            "[INFO] epoch: 1586, train loss: 0.0000918516, val loss: 0.0001158794\n",
            "[INFO] epoch: 1587, train loss: 0.0000861843, val loss: 0.0002828337\n",
            "[INFO] epoch: 1588, train loss: 0.0000961604, val loss: 0.0001379558\n",
            "[INFO] epoch: 1589, train loss: 0.0000892363, val loss: 0.0002641523\n",
            "[INFO] epoch: 1590, train loss: 0.0000961248, val loss: 0.0001503598\n",
            "[INFO] epoch: 1591, train loss: 0.0000858921, val loss: 0.0002536781\n",
            "[INFO] epoch: 1592, train loss: 0.0000917558, val loss: 0.0001305875\n",
            "[INFO] epoch: 1593, train loss: 0.0000859593, val loss: 0.0002724244\n",
            "[INFO] epoch: 1594, train loss: 0.0000949507, val loss: 0.0001177993\n",
            "[INFO] epoch: 1595, train loss: 0.0000860760, val loss: 0.0002979451\n",
            "[INFO] epoch: 1596, train loss: 0.0000925468, val loss: 0.0001126061\n",
            "[INFO] epoch: 1597, train loss: 0.0000858148, val loss: 0.0002897446\n",
            "[INFO] epoch: 1598, train loss: 0.0000953950, val loss: 0.0001316834\n",
            "[INFO] epoch: 1599, train loss: 0.0000892977, val loss: 0.0002715578\n",
            "[INFO] epoch: 1600, train loss: 0.0000971646, val loss: 0.0001476756\n",
            "[INFO] epoch: 1601, train loss: 0.0000872375, val loss: 0.0002597370\n",
            "[INFO] epoch: 1602, train loss: 0.0000929867, val loss: 0.0001329476\n",
            "[INFO] epoch: 1603, train loss: 0.0000859534, val loss: 0.0002703691\n",
            "[INFO] epoch: 1604, train loss: 0.0000937258, val loss: 0.0001201252\n",
            "[INFO] epoch: 1605, train loss: 0.0000857127, val loss: 0.0002885671\n",
            "[INFO] epoch: 1606, train loss: 0.0000924078, val loss: 0.0001157981\n",
            "[INFO] epoch: 1607, train loss: 0.0000851760, val loss: 0.0002860291\n",
            "[INFO] epoch: 1608, train loss: 0.0000930689, val loss: 0.0001261049\n",
            "[INFO] epoch: 1609, train loss: 0.0000866439, val loss: 0.0002727270\n",
            "[INFO] epoch: 1610, train loss: 0.0000940440, val loss: 0.0001365804\n",
            "[INFO] epoch: 1611, train loss: 0.0000857129, val loss: 0.0002638656\n",
            "[INFO] epoch: 1612, train loss: 0.0000916825, val loss: 0.0001294646\n",
            "[INFO] epoch: 1613, train loss: 0.0000843679, val loss: 0.0002680567\n",
            "[INFO] epoch: 1614, train loss: 0.0000911359, val loss: 0.0001195204\n",
            "[INFO] epoch: 1615, train loss: 0.0000839379, val loss: 0.0002796440\n",
            "[INFO] epoch: 1616, train loss: 0.0000905684, val loss: 0.0001152873\n",
            "[INFO] epoch: 1617, train loss: 0.0000835750, val loss: 0.0002806848\n",
            "[INFO] epoch: 1618, train loss: 0.0000906534, val loss: 0.0001201590\n",
            "[INFO] epoch: 1619, train loss: 0.0000843368, val loss: 0.0002729117\n",
            "[INFO] epoch: 1620, train loss: 0.0000915310, val loss: 0.0001278243\n",
            "[INFO] epoch: 1621, train loss: 0.0000842640, val loss: 0.0002655133\n",
            "[INFO] epoch: 1622, train loss: 0.0000905458, val loss: 0.0001259939\n",
            "[INFO] epoch: 1623, train loss: 0.0000834379, val loss: 0.0002667055\n",
            "[INFO] epoch: 1624, train loss: 0.0000900265, val loss: 0.0001184417\n",
            "[INFO] epoch: 1625, train loss: 0.0000832973, val loss: 0.0002755108\n",
            "[INFO] epoch: 1626, train loss: 0.0000901043, val loss: 0.0001139660\n",
            "[INFO] epoch: 1627, train loss: 0.0000832560, val loss: 0.0002801008\n",
            "[INFO] epoch: 1628, train loss: 0.0000902175, val loss: 0.0001159300\n",
            "[INFO] epoch: 1629, train loss: 0.0000839113, val loss: 0.0002750277\n",
            "[INFO] epoch: 1630, train loss: 0.0000912730, val loss: 0.0001234031\n",
            "[INFO] epoch: 1631, train loss: 0.0000844199, val loss: 0.0002676434\n",
            "[INFO] epoch: 1632, train loss: 0.0000910754, val loss: 0.0001255460\n",
            "[INFO] epoch: 1633, train loss: 0.0000837960, val loss: 0.0002652621\n",
            "[INFO] epoch: 1634, train loss: 0.0000902916, val loss: 0.0001194699\n",
            "[INFO] epoch: 1635, train loss: 0.0000835414, val loss: 0.0002724235\n",
            "[INFO] epoch: 1636, train loss: 0.0000904742, val loss: 0.0001133856\n",
            "[INFO] epoch: 1637, train loss: 0.0000834680, val loss: 0.0002802305\n",
            "[INFO] epoch: 1638, train loss: 0.0000902757, val loss: 0.0001121457\n",
            "[INFO] epoch: 1639, train loss: 0.0000836817, val loss: 0.0002781664\n",
            "[INFO] epoch: 1640, train loss: 0.0000910772, val loss: 0.0001193317\n",
            "[INFO] epoch: 1641, train loss: 0.0000845540, val loss: 0.0002688121\n",
            "[INFO] epoch: 1642, train loss: 0.0000915650, val loss: 0.0001263466\n",
            "[INFO] epoch: 1643, train loss: 0.0000839137, val loss: 0.0002615321\n",
            "[INFO] epoch: 1644, train loss: 0.0000899679, val loss: 0.0001220002\n",
            "[INFO] epoch: 1645, train loss: 0.0000829749, val loss: 0.0002655061\n",
            "[INFO] epoch: 1646, train loss: 0.0000898017, val loss: 0.0001124800\n",
            "[INFO] epoch: 1647, train loss: 0.0000829981, val loss: 0.0002786924\n",
            "[INFO] epoch: 1648, train loss: 0.0000897258, val loss: 0.0001070489\n",
            "[INFO] epoch: 1649, train loss: 0.0000825014, val loss: 0.0002831979\n",
            "[INFO] epoch: 1650, train loss: 0.0000894526, val loss: 0.0001107598\n",
            "[INFO] epoch: 1651, train loss: 0.0000837080, val loss: 0.0002727670\n",
            "[INFO] epoch: 1652, train loss: 0.0000917510, val loss: 0.0001255938\n",
            "[INFO] epoch: 1653, train loss: 0.0000843204, val loss: 0.0002593887\n",
            "[INFO] epoch: 1654, train loss: 0.0000902052, val loss: 0.0001280944\n",
            "[INFO] epoch: 1655, train loss: 0.0000821088, val loss: 0.0002549169\n",
            "[INFO] epoch: 1656, train loss: 0.0000882545, val loss: 0.0001142580\n",
            "[INFO] epoch: 1657, train loss: 0.0000823879, val loss: 0.0002722644\n",
            "[INFO] epoch: 1658, train loss: 0.0000901301, val loss: 0.0001037194\n",
            "[INFO] epoch: 1659, train loss: 0.0000821626, val loss: 0.0002910941\n",
            "[INFO] epoch: 1660, train loss: 0.0000882833, val loss: 0.0001003302\n",
            "[INFO] epoch: 1661, train loss: 0.0000820630, val loss: 0.0002842567\n",
            "[INFO] epoch: 1662, train loss: 0.0000909638, val loss: 0.0001175199\n",
            "[INFO] epoch: 1663, train loss: 0.0000856249, val loss: 0.0002657881\n",
            "[INFO] epoch: 1664, train loss: 0.0000934702, val loss: 0.0001365884\n",
            "[INFO] epoch: 1665, train loss: 0.0000835147, val loss: 0.0002514458\n",
            "[INFO] epoch: 1666, train loss: 0.0000882773, val loss: 0.0001218832\n",
            "[INFO] epoch: 1667, train loss: 0.0000819016, val loss: 0.0002610160\n",
            "[INFO] epoch: 1668, train loss: 0.0000901973, val loss: 0.0001057346\n",
            "[INFO] epoch: 1669, train loss: 0.0000831938, val loss: 0.0002904688\n",
            "[INFO] epoch: 1670, train loss: 0.0000898787, val loss: 0.0000984657\n",
            "[INFO] epoch: 1671, train loss: 0.0000816908, val loss: 0.0002936741\n",
            "[INFO] epoch: 1672, train loss: 0.0000892843, val loss: 0.0001079025\n",
            "[INFO] epoch: 1673, train loss: 0.0000847193, val loss: 0.0002761483\n",
            "[INFO] epoch: 1674, train loss: 0.0000938976, val loss: 0.0001314207\n",
            "[INFO] epoch: 1675, train loss: 0.0000851706, val loss: 0.0002600741\n",
            "[INFO] epoch: 1676, train loss: 0.0000905179, val loss: 0.0001270014\n",
            "[INFO] epoch: 1677, train loss: 0.0000825420, val loss: 0.0002595377\n",
            "[INFO] epoch: 1678, train loss: 0.0000893231, val loss: 0.0001102509\n",
            "[INFO] epoch: 1679, train loss: 0.0000827675, val loss: 0.0002798473\n",
            "[INFO] epoch: 1680, train loss: 0.0000897898, val loss: 0.0001022457\n",
            "[INFO] epoch: 1681, train loss: 0.0000818253, val loss: 0.0002892641\n",
            "[INFO] epoch: 1682, train loss: 0.0000883757, val loss: 0.0001049114\n",
            "[INFO] epoch: 1683, train loss: 0.0000825679, val loss: 0.0002786886\n",
            "[INFO] epoch: 1684, train loss: 0.0000905055, val loss: 0.0001197258\n",
            "[INFO] epoch: 1685, train loss: 0.0000834795, val loss: 0.0002655695\n",
            "[INFO] epoch: 1686, train loss: 0.0000896364, val loss: 0.0001225731\n",
            "[INFO] epoch: 1687, train loss: 0.0000817868, val loss: 0.0002609725\n",
            "[INFO] epoch: 1688, train loss: 0.0000876638, val loss: 0.0001117203\n",
            "[INFO] epoch: 1689, train loss: 0.0000810635, val loss: 0.0002709314\n",
            "[INFO] epoch: 1690, train loss: 0.0000876457, val loss: 0.0001033222\n",
            "[INFO] epoch: 1691, train loss: 0.0000805887, val loss: 0.0002811822\n",
            "[INFO] epoch: 1692, train loss: 0.0000868358, val loss: 0.0001021541\n",
            "[INFO] epoch: 1693, train loss: 0.0000805460, val loss: 0.0002782880\n",
            "[INFO] epoch: 1694, train loss: 0.0000875625, val loss: 0.0001102204\n",
            "[INFO] epoch: 1695, train loss: 0.0000814559, val loss: 0.0002685028\n",
            "[INFO] epoch: 1696, train loss: 0.0000880958, val loss: 0.0001166748\n",
            "[INFO] epoch: 1697, train loss: 0.0000809368, val loss: 0.0002623630\n",
            "[INFO] epoch: 1698, train loss: 0.0000867693, val loss: 0.0001118357\n",
            "[INFO] epoch: 1699, train loss: 0.0000801758, val loss: 0.0002662097\n",
            "[INFO] epoch: 1700, train loss: 0.0000865802, val loss: 0.0001038011\n",
            "[INFO] epoch: 1701, train loss: 0.0000801387, val loss: 0.0002767725\n",
            "[INFO] epoch: 1702, train loss: 0.0000865775, val loss: 0.0001002933\n",
            "[INFO] epoch: 1703, train loss: 0.0000800708, val loss: 0.0002797667\n",
            "[INFO] epoch: 1704, train loss: 0.0000867704, val loss: 0.0001039456\n",
            "[INFO] epoch: 1705, train loss: 0.0000809424, val loss: 0.0002732229\n",
            "[INFO] epoch: 1706, train loss: 0.0000880698, val loss: 0.0001128288\n",
            "[INFO] epoch: 1707, train loss: 0.0000813962, val loss: 0.0002648560\n",
            "[INFO] epoch: 1708, train loss: 0.0000875749, val loss: 0.0001138486\n",
            "[INFO] epoch: 1709, train loss: 0.0000805786, val loss: 0.0002629981\n",
            "[INFO] epoch: 1710, train loss: 0.0000867333, val loss: 0.0001066231\n",
            "[INFO] epoch: 1711, train loss: 0.0000803864, val loss: 0.0002720365\n",
            "[INFO] epoch: 1712, train loss: 0.0000870331, val loss: 0.0000998925\n",
            "[INFO] epoch: 1713, train loss: 0.0000802780, val loss: 0.0002810648\n",
            "[INFO] epoch: 1714, train loss: 0.0000866671, val loss: 0.0000991377\n",
            "[INFO] epoch: 1715, train loss: 0.0000804073, val loss: 0.0002785196\n",
            "[INFO] epoch: 1716, train loss: 0.0000875229, val loss: 0.0001072167\n",
            "[INFO] epoch: 1717, train loss: 0.0000815066, val loss: 0.0002677321\n",
            "[INFO] epoch: 1718, train loss: 0.0000883265, val loss: 0.0001161701\n",
            "[INFO] epoch: 1719, train loss: 0.0000808396, val loss: 0.0002589837\n",
            "[INFO] epoch: 1720, train loss: 0.0000863794, val loss: 0.0001113314\n",
            "[INFO] epoch: 1721, train loss: 0.0000796600, val loss: 0.0002624061\n",
            "[INFO] epoch: 1722, train loss: 0.0000861678, val loss: 0.0001006841\n",
            "[INFO] epoch: 1723, train loss: 0.0000798826, val loss: 0.0002781967\n",
            "[INFO] epoch: 1724, train loss: 0.0000864099, val loss: 0.0000941199\n",
            "[INFO] epoch: 1725, train loss: 0.0000792148, val loss: 0.0002856501\n",
            "[INFO] epoch: 1726, train loss: 0.0000855408, val loss: 0.0000967602\n",
            "[INFO] epoch: 1727, train loss: 0.0000801924, val loss: 0.0002757136\n",
            "[INFO] epoch: 1728, train loss: 0.0000882511, val loss: 0.0001128985\n",
            "[INFO] epoch: 1729, train loss: 0.0000817378, val loss: 0.0002604180\n",
            "[INFO] epoch: 1730, train loss: 0.0000877940, val loss: 0.0001203738\n",
            "[INFO] epoch: 1731, train loss: 0.0000793827, val loss: 0.0002521859\n",
            "[INFO] epoch: 1732, train loss: 0.0000846116, val loss: 0.0001061074\n",
            "[INFO] epoch: 1733, train loss: 0.0000790255, val loss: 0.0002663052\n",
            "[INFO] epoch: 1734, train loss: 0.0000867340, val loss: 0.0000936374\n",
            "[INFO] epoch: 1735, train loss: 0.0000796306, val loss: 0.0002908634\n",
            "[INFO] epoch: 1736, train loss: 0.0000855741, val loss: 0.0000882419\n",
            "[INFO] epoch: 1737, train loss: 0.0000784407, val loss: 0.0002899958\n",
            "[INFO] epoch: 1738, train loss: 0.0000859886, val loss: 0.0001000694\n",
            "[INFO] epoch: 1739, train loss: 0.0000818493, val loss: 0.0002721061\n",
            "[INFO] epoch: 1740, train loss: 0.0000907324, val loss: 0.0001239249\n",
            "[INFO] epoch: 1741, train loss: 0.0000819703, val loss: 0.0002553250\n",
            "[INFO] epoch: 1742, train loss: 0.0000866120, val loss: 0.0001180681\n",
            "[INFO] epoch: 1743, train loss: 0.0000789765, val loss: 0.0002553225\n",
            "[INFO] epoch: 1744, train loss: 0.0000857570, val loss: 0.0000988857\n",
            "[INFO] epoch: 1745, train loss: 0.0000801787, val loss: 0.0002814024\n",
            "[INFO] epoch: 1746, train loss: 0.0000875375, val loss: 0.0000901194\n",
            "[INFO] epoch: 1747, train loss: 0.0000791464, val loss: 0.0002961142\n",
            "[INFO] epoch: 1748, train loss: 0.0000851486, val loss: 0.0000920492\n",
            "[INFO] epoch: 1749, train loss: 0.0000800150, val loss: 0.0002830695\n",
            "[INFO] epoch: 1750, train loss: 0.0000887583, val loss: 0.0001125876\n",
            "[INFO] epoch: 1751, train loss: 0.0000823990, val loss: 0.0002662567\n",
            "[INFO] epoch: 1752, train loss: 0.0000887660, val loss: 0.0001205059\n",
            "[INFO] epoch: 1753, train loss: 0.0000802014, val loss: 0.0002578187\n",
            "[INFO] epoch: 1754, train loss: 0.0000855679, val loss: 0.0001057812\n",
            "[INFO] epoch: 1755, train loss: 0.0000793657, val loss: 0.0002698659\n",
            "[INFO] epoch: 1756, train loss: 0.0000863101, val loss: 0.0000945417\n",
            "[INFO] epoch: 1757, train loss: 0.0000791429, val loss: 0.0002867246\n",
            "[INFO] epoch: 1758, train loss: 0.0000850931, val loss: 0.0000916930\n",
            "[INFO] epoch: 1759, train loss: 0.0000785350, val loss: 0.0002846447\n",
            "[INFO] epoch: 1760, train loss: 0.0000854752, val loss: 0.0001014587\n",
            "[INFO] epoch: 1761, train loss: 0.0000799395, val loss: 0.0002717276\n",
            "[INFO] epoch: 1762, train loss: 0.0000868890, val loss: 0.0001124539\n",
            "[INFO] epoch: 1763, train loss: 0.0000795530, val loss: 0.0002619359\n",
            "[INFO] epoch: 1764, train loss: 0.0000849377, val loss: 0.0001078713\n",
            "[INFO] epoch: 1765, train loss: 0.0000781499, val loss: 0.0002637058\n",
            "[INFO] epoch: 1766, train loss: 0.0000841748, val loss: 0.0000970720\n",
            "[INFO] epoch: 1767, train loss: 0.0000779531, val loss: 0.0002764032\n",
            "[INFO] epoch: 1768, train loss: 0.0000841396, val loss: 0.0000917824\n",
            "[INFO] epoch: 1769, train loss: 0.0000775058, val loss: 0.0002827088\n",
            "[INFO] epoch: 1770, train loss: 0.0000836169, val loss: 0.0000940865\n",
            "[INFO] epoch: 1771, train loss: 0.0000780168, val loss: 0.0002757871\n",
            "[INFO] epoch: 1772, train loss: 0.0000849148, val loss: 0.0001040117\n",
            "[INFO] epoch: 1773, train loss: 0.0000787696, val loss: 0.0002664025\n",
            "[INFO] epoch: 1774, train loss: 0.0000847805, val loss: 0.0001073592\n",
            "[INFO] epoch: 1775, train loss: 0.0000778836, val loss: 0.0002619148\n",
            "[INFO] epoch: 1776, train loss: 0.0000834573, val loss: 0.0001004042\n",
            "[INFO] epoch: 1777, train loss: 0.0000773318, val loss: 0.0002688919\n",
            "[INFO] epoch: 1778, train loss: 0.0000835430, val loss: 0.0000925833\n",
            "[INFO] epoch: 1779, train loss: 0.0000772653, val loss: 0.0002798230\n",
            "[INFO] epoch: 1780, train loss: 0.0000832934, val loss: 0.0000902253\n",
            "[INFO] epoch: 1781, train loss: 0.0000771362, val loss: 0.0002806497\n",
            "[INFO] epoch: 1782, train loss: 0.0000835848, val loss: 0.0000959192\n",
            "[INFO] epoch: 1783, train loss: 0.0000781107, val loss: 0.0002719883\n",
            "[INFO] epoch: 1784, train loss: 0.0000848982, val loss: 0.0001054197\n",
            "[INFO] epoch: 1785, train loss: 0.0000783264, val loss: 0.0002628705\n",
            "[INFO] epoch: 1786, train loss: 0.0000839462, val loss: 0.0001050443\n",
            "[INFO] epoch: 1787, train loss: 0.0000772505, val loss: 0.0002621528\n",
            "[INFO] epoch: 1788, train loss: 0.0000830828, val loss: 0.0000960408\n",
            "[INFO] epoch: 1789, train loss: 0.0000772229, val loss: 0.0002735334\n",
            "[INFO] epoch: 1790, train loss: 0.0000836416, val loss: 0.0000889527\n",
            "[INFO] epoch: 1791, train loss: 0.0000770921, val loss: 0.0002844915\n",
            "[INFO] epoch: 1792, train loss: 0.0000830554, val loss: 0.0000884106\n",
            "[INFO] epoch: 1793, train loss: 0.0000771977, val loss: 0.0002808271\n",
            "[INFO] epoch: 1794, train loss: 0.0000841949, val loss: 0.0000984899\n",
            "[INFO] epoch: 1795, train loss: 0.0000787833, val loss: 0.0002683783\n",
            "[INFO] epoch: 1796, train loss: 0.0000855963, val loss: 0.0001098298\n",
            "[INFO] epoch: 1797, train loss: 0.0000781605, val loss: 0.0002578466\n",
            "[INFO] epoch: 1798, train loss: 0.0000831917, val loss: 0.0001044881\n",
            "[INFO] epoch: 1799, train loss: 0.0000766611, val loss: 0.0002609545\n",
            "[INFO] epoch: 1800, train loss: 0.0000829909, val loss: 0.0000916487\n",
            "[INFO] epoch: 1801, train loss: 0.0000772989, val loss: 0.0002803909\n",
            "[INFO] epoch: 1802, train loss: 0.0000838736, val loss: 0.0000842403\n",
            "[INFO] epoch: 1803, train loss: 0.0000765174, val loss: 0.0002915666\n",
            "[INFO] epoch: 1804, train loss: 0.0000822678, val loss: 0.0000859912\n",
            "[INFO] epoch: 1805, train loss: 0.0000771924, val loss: 0.0002807358\n",
            "[INFO] epoch: 1806, train loss: 0.0000853880, val loss: 0.0001042982\n",
            "[INFO] epoch: 1807, train loss: 0.0000797382, val loss: 0.0002634738\n",
            "[INFO] epoch: 1808, train loss: 0.0000861568, val loss: 0.0001162194\n",
            "[INFO] epoch: 1809, train loss: 0.0000773740, val loss: 0.0002523021\n",
            "[INFO] epoch: 1810, train loss: 0.0000818641, val loss: 0.0001013364\n",
            "[INFO] epoch: 1811, train loss: 0.0000762491, val loss: 0.0002635850\n",
            "[INFO] epoch: 1812, train loss: 0.0000837391, val loss: 0.0000864952\n",
            "[INFO] epoch: 1813, train loss: 0.0000774167, val loss: 0.0002914210\n",
            "[INFO] epoch: 1814, train loss: 0.0000834713, val loss: 0.0000805525\n",
            "[INFO] epoch: 1815, train loss: 0.0000758287, val loss: 0.0002951815\n",
            "[INFO] epoch: 1816, train loss: 0.0000822711, val loss: 0.0000891383\n",
            "[INFO] epoch: 1817, train loss: 0.0000782987, val loss: 0.0002776847\n",
            "[INFO] epoch: 1818, train loss: 0.0000871309, val loss: 0.0001127237\n",
            "[INFO] epoch: 1819, train loss: 0.0000797728, val loss: 0.0002607371\n",
            "[INFO] epoch: 1820, train loss: 0.0000848614, val loss: 0.0001129198\n",
            "[INFO] epoch: 1821, train loss: 0.0000768064, val loss: 0.0002564988\n",
            "[INFO] epoch: 1822, train loss: 0.0000824182, val loss: 0.0000945788\n",
            "[INFO] epoch: 1823, train loss: 0.0000770637, val loss: 0.0002759534\n",
            "[INFO] epoch: 1824, train loss: 0.0000841874, val loss: 0.0000844189\n",
            "[INFO] epoch: 1825, train loss: 0.0000767333, val loss: 0.0002940137\n",
            "[INFO] epoch: 1826, train loss: 0.0000823117, val loss: 0.0000839257\n",
            "[INFO] epoch: 1827, train loss: 0.0000764323, val loss: 0.0002871389\n",
            "[INFO] epoch: 1828, train loss: 0.0000838223, val loss: 0.0000984088\n",
            "[INFO] epoch: 1829, train loss: 0.0000786308, val loss: 0.0002710965\n",
            "[INFO] epoch: 1830, train loss: 0.0000854851, val loss: 0.0001105867\n",
            "[INFO] epoch: 1831, train loss: 0.0000777282, val loss: 0.0002611314\n",
            "[INFO] epoch: 1832, train loss: 0.0000826385, val loss: 0.0001016458\n",
            "[INFO] epoch: 1833, train loss: 0.0000761728, val loss: 0.0002654906\n",
            "[INFO] epoch: 1834, train loss: 0.0000822793, val loss: 0.0000894356\n",
            "[INFO] epoch: 1835, train loss: 0.0000761120, val loss: 0.0002818089\n",
            "[INFO] epoch: 1836, train loss: 0.0000819958, val loss: 0.0000842664\n",
            "[INFO] epoch: 1837, train loss: 0.0000753193, val loss: 0.0002861905\n",
            "[INFO] epoch: 1838, train loss: 0.0000811880, val loss: 0.0000887132\n",
            "[INFO] epoch: 1839, train loss: 0.0000759284, val loss: 0.0002771430\n",
            "[INFO] epoch: 1840, train loss: 0.0000827104, val loss: 0.0001002712\n",
            "[INFO] epoch: 1841, train loss: 0.0000766051, val loss: 0.0002656475\n",
            "[INFO] epoch: 1842, train loss: 0.0000821893, val loss: 0.0001026800\n",
            "[INFO] epoch: 1843, train loss: 0.0000753646, val loss: 0.0002620220\n",
            "[INFO] epoch: 1844, train loss: 0.0000805964, val loss: 0.0000935273\n",
            "[INFO] epoch: 1845, train loss: 0.0000748161, val loss: 0.0002706595\n",
            "[INFO] epoch: 1846, train loss: 0.0000808222, val loss: 0.0000854817\n",
            "[INFO] epoch: 1847, train loss: 0.0000747360, val loss: 0.0002825256\n",
            "[INFO] epoch: 1848, train loss: 0.0000804205, val loss: 0.0000837212\n",
            "[INFO] epoch: 1849, train loss: 0.0000745565, val loss: 0.0002821815\n",
            "[INFO] epoch: 1850, train loss: 0.0000807670, val loss: 0.0000904277\n",
            "[INFO] epoch: 1851, train loss: 0.0000756217, val loss: 0.0002730846\n",
            "[INFO] epoch: 1852, train loss: 0.0000821494, val loss: 0.0000999478\n",
            "[INFO] epoch: 1853, train loss: 0.0000758480, val loss: 0.0002640124\n",
            "[INFO] epoch: 1854, train loss: 0.0000812120, val loss: 0.0000989385\n",
            "[INFO] epoch: 1855, train loss: 0.0000748364, val loss: 0.0002639904\n",
            "[INFO] epoch: 1856, train loss: 0.0000804371, val loss: 0.0000899124\n",
            "[INFO] epoch: 1857, train loss: 0.0000748498, val loss: 0.0002751768\n",
            "[INFO] epoch: 1858, train loss: 0.0000809983, val loss: 0.0000832734\n",
            "[INFO] epoch: 1859, train loss: 0.0000747785, val loss: 0.0002857357\n",
            "[INFO] epoch: 1860, train loss: 0.0000805161, val loss: 0.0000832621\n",
            "[INFO] epoch: 1861, train loss: 0.0000748690, val loss: 0.0002821767\n",
            "[INFO] epoch: 1862, train loss: 0.0000814559, val loss: 0.0000925162\n",
            "[INFO] epoch: 1863, train loss: 0.0000762480, val loss: 0.0002706916\n",
            "[INFO] epoch: 1864, train loss: 0.0000827712, val loss: 0.0001027637\n",
            "[INFO] epoch: 1865, train loss: 0.0000758789, val loss: 0.0002609611\n",
            "[INFO] epoch: 1866, train loss: 0.0000808539, val loss: 0.0000985816\n",
            "[INFO] epoch: 1867, train loss: 0.0000745047, val loss: 0.0002631121\n",
            "[INFO] epoch: 1868, train loss: 0.0000803586, val loss: 0.0000870727\n",
            "[INFO] epoch: 1869, train loss: 0.0000748722, val loss: 0.0002795789\n",
            "[INFO] epoch: 1870, train loss: 0.0000811357, val loss: 0.0000799477\n",
            "[INFO] epoch: 1871, train loss: 0.0000743716, val loss: 0.0002905378\n",
            "[INFO] epoch: 1872, train loss: 0.0000798714, val loss: 0.0000810823\n",
            "[INFO] epoch: 1873, train loss: 0.0000745740, val loss: 0.0002828800\n",
            "[INFO] epoch: 1874, train loss: 0.0000818308, val loss: 0.0000954035\n",
            "[INFO] epoch: 1875, train loss: 0.0000767638, val loss: 0.0002668045\n",
            "[INFO] epoch: 1876, train loss: 0.0000833176, val loss: 0.0001081457\n",
            "[INFO] epoch: 1877, train loss: 0.0000753876, val loss: 0.0002558498\n",
            "[INFO] epoch: 1878, train loss: 0.0000796841, val loss: 0.0000980501\n",
            "[INFO] epoch: 1879, train loss: 0.0000736742, val loss: 0.0002616223\n",
            "[INFO] epoch: 1880, train loss: 0.0000801923, val loss: 0.0000829918\n",
            "[INFO] epoch: 1881, train loss: 0.0000748256, val loss: 0.0002868601\n",
            "[INFO] epoch: 1882, train loss: 0.0000811390, val loss: 0.0000759866\n",
            "[INFO] epoch: 1883, train loss: 0.0000736006, val loss: 0.0002965649\n",
            "[INFO] epoch: 1884, train loss: 0.0000790069, val loss: 0.0000799080\n",
            "[INFO] epoch: 1885, train loss: 0.0000746268, val loss: 0.0002829928\n",
            "[INFO] epoch: 1886, train loss: 0.0000829827, val loss: 0.0001010988\n",
            "[INFO] epoch: 1887, train loss: 0.0000774359, val loss: 0.0002645144\n",
            "[INFO] epoch: 1888, train loss: 0.0000834378, val loss: 0.0001111121\n",
            "[INFO] epoch: 1889, train loss: 0.0000748616, val loss: 0.0002551071\n",
            "[INFO] epoch: 1890, train loss: 0.0000792160, val loss: 0.0000941810\n",
            "[INFO] epoch: 1891, train loss: 0.0000739223, val loss: 0.0002678073\n",
            "[INFO] epoch: 1892, train loss: 0.0000810330, val loss: 0.0000801725\n",
            "[INFO] epoch: 1893, train loss: 0.0000747912, val loss: 0.0002936654\n",
            "[INFO] epoch: 1894, train loss: 0.0000804558, val loss: 0.0000758948\n",
            "[INFO] epoch: 1895, train loss: 0.0000734738, val loss: 0.0002948218\n",
            "[INFO] epoch: 1896, train loss: 0.0000796522, val loss: 0.0000854726\n",
            "[INFO] epoch: 1897, train loss: 0.0000754822, val loss: 0.0002787956\n",
            "[INFO] epoch: 1898, train loss: 0.0000832963, val loss: 0.0001045967\n",
            "[INFO] epoch: 1899, train loss: 0.0000764973, val loss: 0.0002638042\n",
            "[INFO] epoch: 1900, train loss: 0.0000815012, val loss: 0.0001038123\n",
            "[INFO] epoch: 1901, train loss: 0.0000742105, val loss: 0.0002611363\n",
            "[INFO] epoch: 1902, train loss: 0.0000794782, val loss: 0.0000885897\n",
            "[INFO] epoch: 1903, train loss: 0.0000740896, val loss: 0.0002766142\n",
            "[INFO] epoch: 1904, train loss: 0.0000804423, val loss: 0.0000794924\n",
            "[INFO] epoch: 1905, train loss: 0.0000737816, val loss: 0.0002915697\n",
            "[INFO] epoch: 1906, train loss: 0.0000791273, val loss: 0.0000794130\n",
            "[INFO] epoch: 1907, train loss: 0.0000733911, val loss: 0.0002860520\n",
            "[INFO] epoch: 1908, train loss: 0.0000798591, val loss: 0.0000905678\n",
            "[INFO] epoch: 1909, train loss: 0.0000748698, val loss: 0.0002728050\n",
            "[INFO] epoch: 1910, train loss: 0.0000812428, val loss: 0.0001010437\n",
            "[INFO] epoch: 1911, train loss: 0.0000744079, val loss: 0.0002631101\n",
            "[INFO] epoch: 1912, train loss: 0.0000791797, val loss: 0.0000952559\n",
            "[INFO] epoch: 1913, train loss: 0.0000729954, val loss: 0.0002659022\n",
            "[INFO] epoch: 1914, train loss: 0.0000785328, val loss: 0.0000842563\n",
            "[INFO] epoch: 1915, train loss: 0.0000729767, val loss: 0.0002797422\n",
            "[INFO] epoch: 1916, train loss: 0.0000786945, val loss: 0.0000785786\n",
            "[INFO] epoch: 1917, train loss: 0.0000725029, val loss: 0.0002870602\n",
            "[INFO] epoch: 1918, train loss: 0.0000779481, val loss: 0.0000810829\n",
            "[INFO] epoch: 1919, train loss: 0.0000727870, val loss: 0.0002806811\n",
            "[INFO] epoch: 1920, train loss: 0.0000791577, val loss: 0.0000913546\n",
            "[INFO] epoch: 1921, train loss: 0.0000738680, val loss: 0.0002692257\n",
            "[INFO] epoch: 1922, train loss: 0.0000796617, val loss: 0.0000974801\n",
            "[INFO] epoch: 1923, train loss: 0.0000731506, val loss: 0.0002634555\n",
            "[INFO] epoch: 1924, train loss: 0.0000780152, val loss: 0.0000907835\n",
            "[INFO] epoch: 1925, train loss: 0.0000723219, val loss: 0.0002684333\n",
            "[INFO] epoch: 1926, train loss: 0.0000780267, val loss: 0.0000815259\n",
            "[INFO] epoch: 1927, train loss: 0.0000725146, val loss: 0.0002825799\n",
            "[INFO] epoch: 1928, train loss: 0.0000781663, val loss: 0.0000774142\n",
            "[INFO] epoch: 1929, train loss: 0.0000721773, val loss: 0.0002868886\n",
            "[INFO] epoch: 1930, train loss: 0.0000777810, val loss: 0.0000812974\n",
            "[INFO] epoch: 1931, train loss: 0.0000728587, val loss: 0.0002796369\n",
            "[INFO] epoch: 1932, train loss: 0.0000794237, val loss: 0.0000926274\n",
            "[INFO] epoch: 1933, train loss: 0.0000739486, val loss: 0.0002673791\n",
            "[INFO] epoch: 1934, train loss: 0.0000795683, val loss: 0.0000978761\n",
            "[INFO] epoch: 1935, train loss: 0.0000728704, val loss: 0.0002621843\n",
            "[INFO] epoch: 1936, train loss: 0.0000776666, val loss: 0.0000895012\n",
            "[INFO] epoch: 1937, train loss: 0.0000721557, val loss: 0.0002694153\n",
            "[INFO] epoch: 1938, train loss: 0.0000781207, val loss: 0.0000793434\n",
            "[INFO] epoch: 1939, train loss: 0.0000725365, val loss: 0.0002864642\n",
            "[INFO] epoch: 1940, train loss: 0.0000781723, val loss: 0.0000752130\n",
            "[INFO] epoch: 1941, train loss: 0.0000719079, val loss: 0.0002901803\n",
            "[INFO] epoch: 1942, train loss: 0.0000774991, val loss: 0.0000803977\n",
            "[INFO] epoch: 1943, train loss: 0.0000729058, val loss: 0.0002794034\n",
            "[INFO] epoch: 1944, train loss: 0.0000799883, val loss: 0.0000955811\n",
            "[INFO] epoch: 1945, train loss: 0.0000743042, val loss: 0.0002645852\n",
            "[INFO] epoch: 1946, train loss: 0.0000797052, val loss: 0.0001012966\n",
            "[INFO] epoch: 1947, train loss: 0.0000723590, val loss: 0.0002581299\n",
            "[INFO] epoch: 1948, train loss: 0.0000768289, val loss: 0.0000887547\n",
            "[INFO] epoch: 1949, train loss: 0.0000716814, val loss: 0.0002692491\n",
            "[INFO] epoch: 1950, train loss: 0.0000781927, val loss: 0.0000763817\n",
            "[INFO] epoch: 1951, train loss: 0.0000724711, val loss: 0.0002923427\n",
            "[INFO] epoch: 1952, train loss: 0.0000780170, val loss: 0.0000719733\n",
            "[INFO] epoch: 1953, train loss: 0.0000712637, val loss: 0.0002952815\n",
            "[INFO] epoch: 1954, train loss: 0.0000768978, val loss: 0.0000795260\n",
            "[INFO] epoch: 1955, train loss: 0.0000729819, val loss: 0.0002795333\n",
            "[INFO] epoch: 1956, train loss: 0.0000809011, val loss: 0.0001000111\n",
            "[INFO] epoch: 1957, train loss: 0.0000747983, val loss: 0.0002630012\n",
            "[INFO] epoch: 1958, train loss: 0.0000798411, val loss: 0.0001041624\n",
            "[INFO] epoch: 1959, train loss: 0.0000719828, val loss: 0.0002565650\n",
            "[INFO] epoch: 1960, train loss: 0.0000764744, val loss: 0.0000867922\n",
            "[INFO] epoch: 1961, train loss: 0.0000717421, val loss: 0.0002728830\n",
            "[INFO] epoch: 1962, train loss: 0.0000786884, val loss: 0.0000743802\n",
            "[INFO] epoch: 1963, train loss: 0.0000724232, val loss: 0.0002971862\n",
            "[INFO] epoch: 1964, train loss: 0.0000776692, val loss: 0.0000714806\n",
            "[INFO] epoch: 1965, train loss: 0.0000711772, val loss: 0.0002959936\n",
            "[INFO] epoch: 1966, train loss: 0.0000773222, val loss: 0.0000827990\n",
            "[INFO] epoch: 1967, train loss: 0.0000735051, val loss: 0.0002778979\n",
            "[INFO] epoch: 1968, train loss: 0.0000811550, val loss: 0.0001020614\n",
            "[INFO] epoch: 1969, train loss: 0.0000743696, val loss: 0.0002640402\n",
            "[INFO] epoch: 1970, train loss: 0.0000789779, val loss: 0.0000999118\n",
            "[INFO] epoch: 1971, train loss: 0.0000718940, val loss: 0.0002613390\n",
            "[INFO] epoch: 1972, train loss: 0.0000769764, val loss: 0.0000836297\n",
            "[INFO] epoch: 1973, train loss: 0.0000719725, val loss: 0.0002793580\n",
            "[INFO] epoch: 1974, train loss: 0.0000782606, val loss: 0.0000745011\n",
            "[INFO] epoch: 1975, train loss: 0.0000717325, val loss: 0.0002946484\n",
            "[INFO] epoch: 1976, train loss: 0.0000767877, val loss: 0.0000745699\n",
            "[INFO] epoch: 1977, train loss: 0.0000712080, val loss: 0.0002893660\n",
            "[INFO] epoch: 1978, train loss: 0.0000775132, val loss: 0.0000867074\n",
            "[INFO] epoch: 1979, train loss: 0.0000729129, val loss: 0.0002738444\n",
            "[INFO] epoch: 1980, train loss: 0.0000792621, val loss: 0.0000983483\n",
            "[INFO] epoch: 1981, train loss: 0.0000725514, val loss: 0.0002643270\n",
            "[INFO] epoch: 1982, train loss: 0.0000770399, val loss: 0.0000927025\n",
            "[INFO] epoch: 1983, train loss: 0.0000709148, val loss: 0.0002657452\n",
            "[INFO] epoch: 1984, train loss: 0.0000761591, val loss: 0.0000804244\n",
            "[INFO] epoch: 1985, train loss: 0.0000709313, val loss: 0.0002813666\n",
            "[INFO] epoch: 1986, train loss: 0.0000765595, val loss: 0.0000742866\n",
            "[INFO] epoch: 1987, train loss: 0.0000704892, val loss: 0.0002899075\n",
            "[INFO] epoch: 1988, train loss: 0.0000755865, val loss: 0.0000762328\n",
            "[INFO] epoch: 1989, train loss: 0.0000705287, val loss: 0.0002837983\n",
            "[INFO] epoch: 1990, train loss: 0.0000766588, val loss: 0.0000869571\n",
            "[INFO] epoch: 1991, train loss: 0.0000717630, val loss: 0.0002713621\n",
            "[INFO] epoch: 1992, train loss: 0.0000774954, val loss: 0.0000943838\n",
            "[INFO] epoch: 1993, train loss: 0.0000711847, val loss: 0.0002645855\n",
            "[INFO] epoch: 1994, train loss: 0.0000757712, val loss: 0.0000885063\n",
            "[INFO] epoch: 1995, train loss: 0.0000701443, val loss: 0.0002681772\n",
            "[INFO] epoch: 1996, train loss: 0.0000754855, val loss: 0.0000783593\n",
            "[INFO] epoch: 1997, train loss: 0.0000703438, val loss: 0.0002827055\n",
            "[INFO] epoch: 1998, train loss: 0.0000759006, val loss: 0.0000735277\n",
            "[INFO] epoch: 1999, train loss: 0.0000700826, val loss: 0.0002896934\n",
            "[INFO] epoch: 2000, train loss: 0.0000752948, val loss: 0.0000761967\n",
            "[INFO] epoch: 2001, train loss: 0.0000704264, val loss: 0.0002828725\n",
            "[INFO] epoch: 2002, train loss: 0.0000766610, val loss: 0.0000870666\n",
            "[INFO] epoch: 2003, train loss: 0.0000717300, val loss: 0.0002710621\n",
            "[INFO] epoch: 2004, train loss: 0.0000774265, val loss: 0.0000943227\n",
            "[INFO] epoch: 2005, train loss: 0.0000710393, val loss: 0.0002638323\n",
            "[INFO] epoch: 2006, train loss: 0.0000755856, val loss: 0.0000879837\n",
            "[INFO] epoch: 2007, train loss: 0.0000700249, val loss: 0.0002687763\n",
            "[INFO] epoch: 2008, train loss: 0.0000755134, val loss: 0.0000772531\n",
            "[INFO] epoch: 2009, train loss: 0.0000704280, val loss: 0.0002846224\n",
            "[INFO] epoch: 2010, train loss: 0.0000760977, val loss: 0.0000720250\n",
            "[INFO] epoch: 2011, train loss: 0.0000700344, val loss: 0.0002932545\n",
            "[INFO] epoch: 2012, train loss: 0.0000751666, val loss: 0.0000748409\n",
            "[INFO] epoch: 2013, train loss: 0.0000703808, val loss: 0.0002837147\n",
            "[INFO] epoch: 2014, train loss: 0.0000769369, val loss: 0.0000879646\n",
            "[INFO] epoch: 2015, train loss: 0.0000721280, val loss: 0.0002705373\n",
            "[INFO] epoch: 2016, train loss: 0.0000779975, val loss: 0.0000976265\n",
            "[INFO] epoch: 2017, train loss: 0.0000710326, val loss: 0.0002603850\n",
            "[INFO] epoch: 2018, train loss: 0.0000751580, val loss: 0.0000891998\n",
            "[INFO] epoch: 2019, train loss: 0.0000695255, val loss: 0.0002666686\n",
            "[INFO] epoch: 2020, train loss: 0.0000751952, val loss: 0.0000757675\n",
            "[INFO] epoch: 2021, train loss: 0.0000703168, val loss: 0.0002868935\n",
            "[INFO] epoch: 2022, train loss: 0.0000761728, val loss: 0.0000693394\n",
            "[INFO] epoch: 2023, train loss: 0.0000695574, val loss: 0.0002984733\n",
            "[INFO] epoch: 2024, train loss: 0.0000743798, val loss: 0.0000723429\n",
            "[INFO] epoch: 2025, train loss: 0.0000697706, val loss: 0.0002856833\n",
            "[INFO] epoch: 2026, train loss: 0.0000767896, val loss: 0.0000889212\n",
            "[INFO] epoch: 2027, train loss: 0.0000723163, val loss: 0.0002698126\n",
            "[INFO] epoch: 2028, train loss: 0.0000784532, val loss: 0.0001015104\n",
            "[INFO] epoch: 2029, train loss: 0.0000708169, val loss: 0.0002580572\n",
            "[INFO] epoch: 2030, train loss: 0.0000745102, val loss: 0.0000897565\n",
            "[INFO] epoch: 2031, train loss: 0.0000689606, val loss: 0.0002656168\n",
            "[INFO] epoch: 2032, train loss: 0.0000749279, val loss: 0.0000741700\n",
            "[INFO] epoch: 2033, train loss: 0.0000702038, val loss: 0.0002900616\n",
            "[INFO] epoch: 2034, train loss: 0.0000761685, val loss: 0.0000678452\n",
            "[INFO] epoch: 2035, train loss: 0.0000692727, val loss: 0.0003015194\n",
            "[INFO] epoch: 2036, train loss: 0.0000740891, val loss: 0.0000721876\n",
            "[INFO] epoch: 2037, train loss: 0.0000697330, val loss: 0.0002870704\n",
            "[INFO] epoch: 2038, train loss: 0.0000769790, val loss: 0.0000903434\n",
            "[INFO] epoch: 2039, train loss: 0.0000723364, val loss: 0.0002706995\n",
            "[INFO] epoch: 2040, train loss: 0.0000782688, val loss: 0.0001008124\n",
            "[INFO] epoch: 2041, train loss: 0.0000707421, val loss: 0.0002609067\n",
            "[INFO] epoch: 2042, train loss: 0.0000746744, val loss: 0.0000877451\n",
            "[INFO] epoch: 2043, train loss: 0.0000693114, val loss: 0.0002698726\n",
            "[INFO] epoch: 2044, train loss: 0.0000752679, val loss: 0.0000739511\n",
            "[INFO] epoch: 2045, train loss: 0.0000700964, val loss: 0.0002914675\n",
            "[INFO] epoch: 2046, train loss: 0.0000756266, val loss: 0.0000692023\n",
            "[INFO] epoch: 2047, train loss: 0.0000691929, val loss: 0.0002976845\n",
            "[INFO] epoch: 2048, train loss: 0.0000742891, val loss: 0.0000751345\n",
            "[INFO] epoch: 2049, train loss: 0.0000698964, val loss: 0.0002845177\n",
            "[INFO] epoch: 2050, train loss: 0.0000765915, val loss: 0.0000903042\n",
            "[INFO] epoch: 2051, train loss: 0.0000714011, val loss: 0.0002702785\n",
            "[INFO] epoch: 2052, train loss: 0.0000766976, val loss: 0.0000956494\n",
            "[INFO] epoch: 2053, train loss: 0.0000698925, val loss: 0.0002641280\n",
            "[INFO] epoch: 2054, train loss: 0.0000741448, val loss: 0.0000842801\n",
            "[INFO] epoch: 2055, train loss: 0.0000689191, val loss: 0.0002728873\n",
            "[INFO] epoch: 2056, train loss: 0.0000744944, val loss: 0.0000735822\n",
            "[INFO] epoch: 2057, train loss: 0.0000691501, val loss: 0.0002897555\n",
            "[INFO] epoch: 2058, train loss: 0.0000742970, val loss: 0.0000703182\n",
            "[INFO] epoch: 2059, train loss: 0.0000684841, val loss: 0.0002918804\n",
            "[INFO] epoch: 2060, train loss: 0.0000736539, val loss: 0.0000762993\n",
            "[INFO] epoch: 2061, train loss: 0.0000692079, val loss: 0.0002818192\n",
            "[INFO] epoch: 2062, train loss: 0.0000753776, val loss: 0.0000883056\n",
            "[INFO] epoch: 2063, train loss: 0.0000701348, val loss: 0.0002691386\n",
            "[INFO] epoch: 2064, train loss: 0.0000751460, val loss: 0.0000914818\n",
            "[INFO] epoch: 2065, train loss: 0.0000688861, val loss: 0.0002657269\n",
            "[INFO] epoch: 2066, train loss: 0.0000732671, val loss: 0.0000820358\n",
            "[INFO] epoch: 2067, train loss: 0.0000681976, val loss: 0.0002734556\n",
            "[INFO] epoch: 2068, train loss: 0.0000735933, val loss: 0.0000727801\n",
            "[INFO] epoch: 2069, train loss: 0.0000684039, val loss: 0.0002889682\n",
            "[INFO] epoch: 2070, train loss: 0.0000735086, val loss: 0.0000700150\n",
            "[INFO] epoch: 2071, train loss: 0.0000679527, val loss: 0.0002905809\n",
            "[INFO] epoch: 2072, train loss: 0.0000730955, val loss: 0.0000754813\n",
            "[INFO] epoch: 2073, train loss: 0.0000686974, val loss: 0.0002815165\n",
            "[INFO] epoch: 2074, train loss: 0.0000748021, val loss: 0.0000871078\n",
            "[INFO] epoch: 2075, train loss: 0.0000696755, val loss: 0.0002691119\n",
            "[INFO] epoch: 2076, train loss: 0.0000746937, val loss: 0.0000908612\n",
            "[INFO] epoch: 2077, train loss: 0.0000684890, val loss: 0.0002651924\n",
            "[INFO] epoch: 2078, train loss: 0.0000728304, val loss: 0.0000819212\n",
            "[INFO] epoch: 2079, train loss: 0.0000678434, val loss: 0.0002730790\n",
            "[INFO] epoch: 2080, train loss: 0.0000733226, val loss: 0.0000720323\n",
            "[INFO] epoch: 2081, train loss: 0.0000682882, val loss: 0.0002900609\n",
            "[INFO] epoch: 2082, train loss: 0.0000735289, val loss: 0.0000685489\n",
            "[INFO] epoch: 2083, train loss: 0.0000677902, val loss: 0.0002939445\n",
            "[INFO] epoch: 2084, train loss: 0.0000728306, val loss: 0.0000735955\n",
            "[INFO] epoch: 2085, train loss: 0.0000685220, val loss: 0.0002832812\n",
            "[INFO] epoch: 2086, train loss: 0.0000749301, val loss: 0.0000872358\n",
            "[INFO] epoch: 2087, train loss: 0.0000699994, val loss: 0.0002694967\n",
            "[INFO] epoch: 2088, train loss: 0.0000752408, val loss: 0.0000937759\n",
            "[INFO] epoch: 2089, train loss: 0.0000685607, val loss: 0.0002625211\n",
            "[INFO] epoch: 2090, train loss: 0.0000725714, val loss: 0.0000836452\n",
            "[INFO] epoch: 2091, train loss: 0.0000675130, val loss: 0.0002709145\n",
            "[INFO] epoch: 2092, train loss: 0.0000731723, val loss: 0.0000713095\n",
            "[INFO] epoch: 2093, train loss: 0.0000683567, val loss: 0.0002917333\n",
            "[INFO] epoch: 2094, train loss: 0.0000738449, val loss: 0.0000663548\n",
            "[INFO] epoch: 2095, train loss: 0.0000675742, val loss: 0.0002996805\n",
            "[INFO] epoch: 2096, train loss: 0.0000723337, val loss: 0.0000708827\n",
            "[INFO] epoch: 2097, train loss: 0.0000681124, val loss: 0.0002861168\n",
            "[INFO] epoch: 2098, train loss: 0.0000749531, val loss: 0.0000874598\n",
            "[INFO] epoch: 2099, train loss: 0.0000704200, val loss: 0.0002705267\n",
            "[INFO] epoch: 2100, train loss: 0.0000760844, val loss: 0.0000976917\n",
            "[INFO] epoch: 2101, train loss: 0.0000687793, val loss: 0.0002603150\n",
            "[INFO] epoch: 2102, train loss: 0.0000723846, val loss: 0.0000856762\n",
            "[INFO] epoch: 2103, train loss: 0.0000671578, val loss: 0.0002689150\n",
            "[INFO] epoch: 2104, train loss: 0.0000729208, val loss: 0.0000709017\n",
            "[INFO] epoch: 2105, train loss: 0.0000683203, val loss: 0.0002925707\n",
            "[INFO] epoch: 2106, train loss: 0.0000740154, val loss: 0.0000651825\n",
            "[INFO] epoch: 2107, train loss: 0.0000674245, val loss: 0.0003032572\n",
            "[INFO] epoch: 2108, train loss: 0.0000720296, val loss: 0.0000697563\n",
            "[INFO] epoch: 2109, train loss: 0.0000677859, val loss: 0.0002881808\n",
            "[INFO] epoch: 2110, train loss: 0.0000746651, val loss: 0.0000870643\n",
            "[INFO] epoch: 2111, train loss: 0.0000702723, val loss: 0.0002724417\n",
            "[INFO] epoch: 2112, train loss: 0.0000760394, val loss: 0.0000975266\n",
            "[INFO] epoch: 2113, train loss: 0.0000688076, val loss: 0.0002621687\n",
            "[INFO] epoch: 2114, train loss: 0.0000724799, val loss: 0.0000853879\n",
            "[INFO] epoch: 2115, train loss: 0.0000671846, val loss: 0.0002705794\n",
            "[INFO] epoch: 2116, train loss: 0.0000727576, val loss: 0.0000713149\n",
            "[INFO] epoch: 2117, train loss: 0.0000679970, val loss: 0.0002916413\n",
            "[INFO] epoch: 2118, train loss: 0.0000734533, val loss: 0.0000661169\n",
            "[INFO] epoch: 2119, train loss: 0.0000672206, val loss: 0.0003004703\n",
            "[INFO] epoch: 2120, train loss: 0.0000719407, val loss: 0.0000709981\n",
            "[INFO] epoch: 2121, train loss: 0.0000675727, val loss: 0.0002873287\n",
            "[INFO] epoch: 2122, train loss: 0.0000739255, val loss: 0.0000855760\n",
            "[INFO] epoch: 2123, train loss: 0.0000693124, val loss: 0.0002733138\n",
            "[INFO] epoch: 2124, train loss: 0.0000747284, val loss: 0.0000932674\n",
            "[INFO] epoch: 2125, train loss: 0.0000681481, val loss: 0.0002651113\n",
            "[INFO] epoch: 2126, train loss: 0.0000720838, val loss: 0.0000832917\n",
            "[INFO] epoch: 2127, train loss: 0.0000668612, val loss: 0.0002725144\n",
            "[INFO] epoch: 2128, train loss: 0.0000720991, val loss: 0.0000716993\n",
            "[INFO] epoch: 2129, train loss: 0.0000672494, val loss: 0.0002893291\n",
            "[INFO] epoch: 2130, train loss: 0.0000724201, val loss: 0.0000671644\n",
            "[INFO] epoch: 2131, train loss: 0.0000666904, val loss: 0.0002960356\n",
            "[INFO] epoch: 2132, train loss: 0.0000714676, val loss: 0.0000714777\n",
            "[INFO] epoch: 2133, train loss: 0.0000670168, val loss: 0.0002861021\n",
            "[INFO] epoch: 2134, train loss: 0.0000729146, val loss: 0.0000832295\n",
            "[INFO] epoch: 2135, train loss: 0.0000683035, val loss: 0.0002734805\n",
            "[INFO] epoch: 2136, train loss: 0.0000735512, val loss: 0.0000898098\n",
            "[INFO] epoch: 2137, train loss: 0.0000674476, val loss: 0.0002668674\n",
            "[INFO] epoch: 2138, train loss: 0.0000715031, val loss: 0.0000821701\n",
            "[INFO] epoch: 2139, train loss: 0.0000663291, val loss: 0.0002722238\n",
            "[INFO] epoch: 2140, train loss: 0.0000713447, val loss: 0.0000718104\n",
            "[INFO] epoch: 2141, train loss: 0.0000666183, val loss: 0.0002878818\n",
            "[INFO] epoch: 2142, train loss: 0.0000717550, val loss: 0.0000671305\n",
            "[INFO] epoch: 2143, train loss: 0.0000662557, val loss: 0.0002944838\n",
            "[INFO] epoch: 2144, train loss: 0.0000709720, val loss: 0.0000702852\n",
            "[INFO] epoch: 2145, train loss: 0.0000664471, val loss: 0.0002872613\n",
            "[INFO] epoch: 2146, train loss: 0.0000721788, val loss: 0.0000811060\n",
            "[INFO] epoch: 2147, train loss: 0.0000677780, val loss: 0.0002739623\n",
            "[INFO] epoch: 2148, train loss: 0.0000731568, val loss: 0.0000890326\n",
            "[INFO] epoch: 2149, train loss: 0.0000671890, val loss: 0.0002670661\n",
            "[INFO] epoch: 2150, train loss: 0.0000712175, val loss: 0.0000831814\n",
            "[INFO] epoch: 2151, train loss: 0.0000659180, val loss: 0.0002701184\n",
            "[INFO] epoch: 2152, train loss: 0.0000707857, val loss: 0.0000721136\n",
            "[INFO] epoch: 2153, train loss: 0.0000662981, val loss: 0.0002869147\n",
            "[INFO] epoch: 2154, train loss: 0.0000716599, val loss: 0.0000662176\n",
            "[INFO] epoch: 2155, train loss: 0.0000661264, val loss: 0.0002967993\n",
            "[INFO] epoch: 2156, train loss: 0.0000707218, val loss: 0.0000677877\n",
            "[INFO] epoch: 2157, train loss: 0.0000659807, val loss: 0.0002908275\n",
            "[INFO] epoch: 2158, train loss: 0.0000716863, val loss: 0.0000789580\n",
            "[INFO] epoch: 2159, train loss: 0.0000676840, val loss: 0.0002759053\n",
            "[INFO] epoch: 2160, train loss: 0.0000735535, val loss: 0.0000905455\n",
            "[INFO] epoch: 2161, train loss: 0.0000675095, val loss: 0.0002660756\n",
            "[INFO] epoch: 2162, train loss: 0.0000714320, val loss: 0.0000866516\n",
            "[INFO] epoch: 2163, train loss: 0.0000656403, val loss: 0.0002665206\n",
            "[INFO] epoch: 2164, train loss: 0.0000702284, val loss: 0.0000731710\n",
            "[INFO] epoch: 2165, train loss: 0.0000660485, val loss: 0.0002847513\n",
            "[INFO] epoch: 2166, train loss: 0.0000718554, val loss: 0.0000650838\n",
            "[INFO] epoch: 2167, train loss: 0.0000662346, val loss: 0.0003010923\n",
            "[INFO] epoch: 2168, train loss: 0.0000707384, val loss: 0.0000647793\n",
            "[INFO] epoch: 2169, train loss: 0.0000654782, val loss: 0.0002961857\n",
            "[INFO] epoch: 2170, train loss: 0.0000710138, val loss: 0.0000760963\n",
            "[INFO] epoch: 2171, train loss: 0.0000675109, val loss: 0.0002794583\n",
            "[INFO] epoch: 2172, train loss: 0.0000741007, val loss: 0.0000920959\n",
            "[INFO] epoch: 2173, train loss: 0.0000681526, val loss: 0.0002661396\n",
            "[INFO] epoch: 2174, train loss: 0.0000721582, val loss: 0.0000908944\n",
            "[INFO] epoch: 2175, train loss: 0.0000656597, val loss: 0.0002640048\n",
            "[INFO] epoch: 2176, train loss: 0.0000698252, val loss: 0.0000750010\n",
            "[INFO] epoch: 2177, train loss: 0.0000657578, val loss: 0.0002815290\n",
            "[INFO] epoch: 2178, train loss: 0.0000718464, val loss: 0.0000648787\n",
            "[INFO] epoch: 2179, train loss: 0.0000663723, val loss: 0.0003031271\n",
            "[INFO] epoch: 2180, train loss: 0.0000709913, val loss: 0.0000634099\n",
            "[INFO] epoch: 2181, train loss: 0.0000652831, val loss: 0.0002998317\n",
            "[INFO] epoch: 2182, train loss: 0.0000705103, val loss: 0.0000738838\n",
            "[INFO] epoch: 2183, train loss: 0.0000670759, val loss: 0.0002833121\n",
            "[INFO] epoch: 2184, train loss: 0.0000738279, val loss: 0.0000908212\n",
            "[INFO] epoch: 2185, train loss: 0.0000682719, val loss: 0.0002684435\n",
            "[INFO] epoch: 2186, train loss: 0.0000725859, val loss: 0.0000916371\n",
            "[INFO] epoch: 2187, train loss: 0.0000659506, val loss: 0.0002656639\n",
            "[INFO] epoch: 2188, train loss: 0.0000699269, val loss: 0.0000765231\n",
            "[INFO] epoch: 2189, train loss: 0.0000655641, val loss: 0.0002796185\n",
            "[INFO] epoch: 2190, train loss: 0.0000712990, val loss: 0.0000660064\n",
            "[INFO] epoch: 2191, train loss: 0.0000660720, val loss: 0.0003002207\n",
            "[INFO] epoch: 2192, train loss: 0.0000707951, val loss: 0.0000642582\n",
            "[INFO] epoch: 2193, train loss: 0.0000651749, val loss: 0.0002986132\n",
            "[INFO] epoch: 2194, train loss: 0.0000701283, val loss: 0.0000728518\n",
            "[INFO] epoch: 2195, train loss: 0.0000663368, val loss: 0.0002847087\n",
            "[INFO] epoch: 2196, train loss: 0.0000725531, val loss: 0.0000871191\n",
            "[INFO] epoch: 2197, train loss: 0.0000673942, val loss: 0.0002707950\n",
            "[INFO] epoch: 2198, train loss: 0.0000718809, val loss: 0.0000888755\n",
            "[INFO] epoch: 2199, train loss: 0.0000656613, val loss: 0.0002679219\n",
            "[INFO] epoch: 2200, train loss: 0.0000696129, val loss: 0.0000770036\n",
            "[INFO] epoch: 2201, train loss: 0.0000650018, val loss: 0.0002782413\n",
            "[INFO] epoch: 2202, train loss: 0.0000702326, val loss: 0.0000671204\n",
            "[INFO] epoch: 2203, train loss: 0.0000653092, val loss: 0.0002955572\n",
            "[INFO] epoch: 2204, train loss: 0.0000700565, val loss: 0.0000649500\n",
            "[INFO] epoch: 2205, train loss: 0.0000646858, val loss: 0.0002965004\n",
            "[INFO] epoch: 2206, train loss: 0.0000694066, val loss: 0.0000712837\n",
            "[INFO] epoch: 2207, train loss: 0.0000653870, val loss: 0.0002851926\n",
            "[INFO] epoch: 2208, train loss: 0.0000711657, val loss: 0.0000832746\n",
            "[INFO] epoch: 2209, train loss: 0.0000664125, val loss: 0.0002727638\n",
            "[INFO] epoch: 2210, train loss: 0.0000710762, val loss: 0.0000867540\n",
            "[INFO] epoch: 2211, train loss: 0.0000651954, val loss: 0.0002685663\n",
            "[INFO] epoch: 2212, train loss: 0.0000691132, val loss: 0.0000776917\n",
            "[INFO] epoch: 2213, train loss: 0.0000644050, val loss: 0.0002766042\n",
            "[INFO] epoch: 2214, train loss: 0.0000693849, val loss: 0.0000679350\n",
            "[INFO] epoch: 2215, train loss: 0.0000647919, val loss: 0.0002924747\n",
            "[INFO] epoch: 2216, train loss: 0.0000696839, val loss: 0.0000646153\n",
            "[INFO] epoch: 2217, train loss: 0.0000643935, val loss: 0.0002974644\n",
            "[INFO] epoch: 2218, train loss: 0.0000689651, val loss: 0.0000689658\n",
            "[INFO] epoch: 2219, train loss: 0.0000647999, val loss: 0.0002875284\n",
            "[INFO] epoch: 2220, train loss: 0.0000704606, val loss: 0.0000803563\n",
            "[INFO] epoch: 2221, train loss: 0.0000661274, val loss: 0.0002754214\n",
            "[INFO] epoch: 2222, train loss: 0.0000711803, val loss: 0.0000870963\n",
            "[INFO] epoch: 2223, train loss: 0.0000653262, val loss: 0.0002680056\n",
            "[INFO] epoch: 2224, train loss: 0.0000691317, val loss: 0.0000801440\n",
            "[INFO] epoch: 2225, train loss: 0.0000641427, val loss: 0.0002739432\n",
            "[INFO] epoch: 2226, train loss: 0.0000689403, val loss: 0.0000691632\n",
            "[INFO] epoch: 2227, train loss: 0.0000646449, val loss: 0.0002900136\n",
            "[INFO] epoch: 2228, train loss: 0.0000698299, val loss: 0.0000637546\n",
            "[INFO] epoch: 2229, train loss: 0.0000644164, val loss: 0.0003009917\n",
            "[INFO] epoch: 2230, train loss: 0.0000688175, val loss: 0.0000659166\n",
            "[INFO] epoch: 2231, train loss: 0.0000642947, val loss: 0.0002919899\n",
            "[INFO] epoch: 2232, train loss: 0.0000698266, val loss: 0.0000772750\n",
            "[INFO] epoch: 2233, train loss: 0.0000660100, val loss: 0.0002786238\n",
            "[INFO] epoch: 2234, train loss: 0.0000717129, val loss: 0.0000887632\n",
            "[INFO] epoch: 2235, train loss: 0.0000658237, val loss: 0.0002670876\n",
            "[INFO] epoch: 2236, train loss: 0.0000695065, val loss: 0.0000845533\n",
            "[INFO] epoch: 2237, train loss: 0.0000638671, val loss: 0.0002692507\n",
            "[INFO] epoch: 2238, train loss: 0.0000682330, val loss: 0.0000711878\n",
            "[INFO] epoch: 2239, train loss: 0.0000642456, val loss: 0.0002860726\n",
            "[INFO] epoch: 2240, train loss: 0.0000698519, val loss: 0.0000629367\n",
            "[INFO] epoch: 2241, train loss: 0.0000644959, val loss: 0.0003040815\n",
            "[INFO] epoch: 2242, train loss: 0.0000688669, val loss: 0.0000626850\n",
            "[INFO] epoch: 2243, train loss: 0.0000636745, val loss: 0.0002986986\n",
            "[INFO] epoch: 2244, train loss: 0.0000688096, val loss: 0.0000730879\n",
            "[INFO] epoch: 2245, train loss: 0.0000654734, val loss: 0.0002828477\n",
            "[INFO] epoch: 2246, train loss: 0.0000718885, val loss: 0.0000889042\n",
            "[INFO] epoch: 2247, train loss: 0.0000664313, val loss: 0.0002688019\n",
            "[INFO] epoch: 2248, train loss: 0.0000704406, val loss: 0.0000891461\n",
            "[INFO] epoch: 2249, train loss: 0.0000640437, val loss: 0.0002661396\n",
            "[INFO] epoch: 2250, train loss: 0.0000678337, val loss: 0.0000742683\n",
            "[INFO] epoch: 2251, train loss: 0.0000637964, val loss: 0.0002815012\n",
            "[INFO] epoch: 2252, train loss: 0.0000695376, val loss: 0.0000634603\n",
            "[INFO] epoch: 2253, train loss: 0.0000646094, val loss: 0.0003034392\n",
            "[INFO] epoch: 2254, train loss: 0.0000692836, val loss: 0.0000614230\n",
            "[INFO] epoch: 2255, train loss: 0.0000635817, val loss: 0.0003037770\n",
            "[INFO] epoch: 2256, train loss: 0.0000682619, val loss: 0.0000697852\n",
            "[INFO] epoch: 2257, train loss: 0.0000648047, val loss: 0.0002873071\n",
            "[INFO] epoch: 2258, train loss: 0.0000712342, val loss: 0.0000859630\n",
            "[INFO] epoch: 2259, train loss: 0.0000664555, val loss: 0.0002730173\n",
            "[INFO] epoch: 2260, train loss: 0.0000710450, val loss: 0.0000900512\n",
            "[INFO] epoch: 2261, train loss: 0.0000645525, val loss: 0.0002672074\n",
            "[INFO] epoch: 2262, train loss: 0.0000681071, val loss: 0.0000769996\n",
            "[INFO] epoch: 2263, train loss: 0.0000636120, val loss: 0.0002789046\n",
            "[INFO] epoch: 2264, train loss: 0.0000689035, val loss: 0.0000653814\n",
            "[INFO] epoch: 2265, train loss: 0.0000643123, val loss: 0.0002985296\n",
            "[INFO] epoch: 2266, train loss: 0.0000691583, val loss: 0.0000621321\n",
            "[INFO] epoch: 2267, train loss: 0.0000635438, val loss: 0.0003035475\n",
            "[INFO] epoch: 2268, train loss: 0.0000679680, val loss: 0.0000680795\n",
            "[INFO] epoch: 2269, train loss: 0.0000640504, val loss: 0.0002894450\n",
            "[INFO] epoch: 2270, train loss: 0.0000698829, val loss: 0.0000815005\n",
            "[INFO] epoch: 2271, train loss: 0.0000655530, val loss: 0.0002765007\n",
            "[INFO] epoch: 2272, train loss: 0.0000704338, val loss: 0.0000876333\n",
            "[INFO] epoch: 2273, train loss: 0.0000644009, val loss: 0.0002690609\n",
            "[INFO] epoch: 2274, train loss: 0.0000679740, val loss: 0.0000783205\n",
            "[INFO] epoch: 2275, train loss: 0.0000631544, val loss: 0.0002769368\n",
            "[INFO] epoch: 2276, train loss: 0.0000679158, val loss: 0.0000672831\n",
            "[INFO] epoch: 2277, train loss: 0.0000635668, val loss: 0.0002930881\n",
            "[INFO] epoch: 2278, train loss: 0.0000684308, val loss: 0.0000627869\n",
            "[INFO] epoch: 2279, train loss: 0.0000631442, val loss: 0.0003013816\n",
            "[INFO] epoch: 2280, train loss: 0.0000674513, val loss: 0.0000663189\n",
            "[INFO] epoch: 2281, train loss: 0.0000631997, val loss: 0.0002914794\n",
            "[INFO] epoch: 2282, train loss: 0.0000685162, val loss: 0.0000771082\n",
            "[INFO] epoch: 2283, train loss: 0.0000645455, val loss: 0.0002791755\n",
            "[INFO] epoch: 2284, train loss: 0.0000696718, val loss: 0.0000852738\n",
            "[INFO] epoch: 2285, train loss: 0.0000641227, val loss: 0.0002706557\n",
            "[INFO] epoch: 2286, train loss: 0.0000678084, val loss: 0.0000799254\n",
            "[INFO] epoch: 2287, train loss: 0.0000627353, val loss: 0.0002743687\n",
            "[INFO] epoch: 2288, train loss: 0.0000671041, val loss: 0.0000692045\n",
            "[INFO] epoch: 2289, train loss: 0.0000629708, val loss: 0.0002889414\n",
            "[INFO] epoch: 2290, train loss: 0.0000679614, val loss: 0.0000630562\n",
            "[INFO] epoch: 2291, train loss: 0.0000629638, val loss: 0.0003008309\n",
            "[INFO] epoch: 2292, train loss: 0.0000672834, val loss: 0.0000640685\n",
            "[INFO] epoch: 2293, train loss: 0.0000626658, val loss: 0.0002957505\n",
            "[INFO] epoch: 2294, train loss: 0.0000676257, val loss: 0.0000730047\n",
            "[INFO] epoch: 2295, train loss: 0.0000639466, val loss: 0.0002828076\n",
            "[INFO] epoch: 2296, train loss: 0.0000694574, val loss: 0.0000839138\n",
            "[INFO] epoch: 2297, train loss: 0.0000643528, val loss: 0.0002724199\n",
            "[INFO] epoch: 2298, train loss: 0.0000683142, val loss: 0.0000832371\n",
            "[INFO] epoch: 2299, train loss: 0.0000627613, val loss: 0.0002710078\n",
            "[INFO] epoch: 2300, train loss: 0.0000666628, val loss: 0.0000722467\n",
            "[INFO] epoch: 2301, train loss: 0.0000625557, val loss: 0.0002842712\n",
            "[INFO] epoch: 2302, train loss: 0.0000677110, val loss: 0.0000635314\n",
            "[INFO] epoch: 2303, train loss: 0.0000630681, val loss: 0.0003007023\n",
            "[INFO] epoch: 2304, train loss: 0.0000676025, val loss: 0.0000616818\n",
            "[INFO] epoch: 2305, train loss: 0.0000624033, val loss: 0.0003022019\n",
            "[INFO] epoch: 2306, train loss: 0.0000669054, val loss: 0.0000683714\n",
            "[INFO] epoch: 2307, train loss: 0.0000632826, val loss: 0.0002880297\n",
            "[INFO] epoch: 2308, train loss: 0.0000691407, val loss: 0.0000818998\n",
            "[INFO] epoch: 2309, train loss: 0.0000647442, val loss: 0.0002751347\n",
            "[INFO] epoch: 2310, train loss: 0.0000693708, val loss: 0.0000875363\n",
            "[INFO] epoch: 2311, train loss: 0.0000632254, val loss: 0.0002679663\n",
            "[INFO] epoch: 2312, train loss: 0.0000665501, val loss: 0.0000770363\n",
            "[INFO] epoch: 2313, train loss: 0.0000620178, val loss: 0.0002776373\n",
            "[INFO] epoch: 2314, train loss: 0.0000670212, val loss: 0.0000649424\n",
            "[INFO] epoch: 2315, train loss: 0.0000629729, val loss: 0.0002980026\n",
            "[INFO] epoch: 2316, train loss: 0.0000680537, val loss: 0.0000600534\n",
            "[INFO] epoch: 2317, train loss: 0.0000624264, val loss: 0.0003081605\n",
            "[INFO] epoch: 2318, train loss: 0.0000665010, val loss: 0.0000638332\n",
            "[INFO] epoch: 2319, train loss: 0.0000623918, val loss: 0.0002952191\n",
            "[INFO] epoch: 2320, train loss: 0.0000681039, val loss: 0.0000776100\n",
            "[INFO] epoch: 2321, train loss: 0.0000646054, val loss: 0.0002796999\n",
            "[INFO] epoch: 2322, train loss: 0.0000702081, val loss: 0.0000894465\n",
            "[INFO] epoch: 2323, train loss: 0.0000640827, val loss: 0.0002688330\n",
            "[INFO] epoch: 2324, train loss: 0.0000673122, val loss: 0.0000823329\n",
            "[INFO] epoch: 2325, train loss: 0.0000619362, val loss: 0.0002724889\n",
            "[INFO] epoch: 2326, train loss: 0.0000663003, val loss: 0.0000679098\n",
            "[INFO] epoch: 2327, train loss: 0.0000625619, val loss: 0.0002923487\n",
            "[INFO] epoch: 2328, train loss: 0.0000680111, val loss: 0.0000604338\n",
            "[INFO] epoch: 2329, train loss: 0.0000626556, val loss: 0.0003086856\n",
            "[INFO] epoch: 2330, train loss: 0.0000667621, val loss: 0.0000615742\n",
            "[INFO] epoch: 2331, train loss: 0.0000619303, val loss: 0.0003012452\n",
            "[INFO] epoch: 2332, train loss: 0.0000669843, val loss: 0.0000727953\n",
            "[INFO] epoch: 2333, train loss: 0.0000637173, val loss: 0.0002846984\n",
            "[INFO] epoch: 2334, train loss: 0.0000696342, val loss: 0.0000865256\n",
            "[INFO] epoch: 2335, train loss: 0.0000643028, val loss: 0.0002728244\n",
            "[INFO] epoch: 2336, train loss: 0.0000680501, val loss: 0.0000847373\n",
            "[INFO] epoch: 2337, train loss: 0.0000622317, val loss: 0.0002714376\n",
            "[INFO] epoch: 2338, train loss: 0.0000660010, val loss: 0.0000713577\n",
            "[INFO] epoch: 2339, train loss: 0.0000620045, val loss: 0.0002866919\n",
            "[INFO] epoch: 2340, train loss: 0.0000671922, val loss: 0.0000623146\n",
            "[INFO] epoch: 2341, train loss: 0.0000624423, val loss: 0.0003037451\n",
            "[INFO] epoch: 2342, train loss: 0.0000668063, val loss: 0.0000612564\n",
            "[INFO] epoch: 2343, train loss: 0.0000616939, val loss: 0.0003033633\n",
            "[INFO] epoch: 2344, train loss: 0.0000661805, val loss: 0.0000689708\n",
            "[INFO] epoch: 2345, train loss: 0.0000626158, val loss: 0.0002885711\n",
            "[INFO] epoch: 2346, train loss: 0.0000682089, val loss: 0.0000814494\n",
            "[INFO] epoch: 2347, train loss: 0.0000636963, val loss: 0.0002769153\n",
            "[INFO] epoch: 2348, train loss: 0.0000680043, val loss: 0.0000844331\n",
            "[INFO] epoch: 2349, train loss: 0.0000622926, val loss: 0.0002718842\n",
            "[INFO] epoch: 2350, train loss: 0.0000657867, val loss: 0.0000743093\n",
            "[INFO] epoch: 2351, train loss: 0.0000614175, val loss: 0.0002820815\n",
            "[INFO] epoch: 2352, train loss: 0.0000661515, val loss: 0.0000645383\n",
            "[INFO] epoch: 2353, train loss: 0.0000618939, val loss: 0.0002979329\n",
            "[INFO] epoch: 2354, train loss: 0.0000664850, val loss: 0.0000612240\n",
            "[INFO] epoch: 2355, train loss: 0.0000614253, val loss: 0.0003038713\n",
            "[INFO] epoch: 2356, train loss: 0.0000656156, val loss: 0.0000656814\n",
            "[INFO] epoch: 2357, train loss: 0.0000616697, val loss: 0.0002927589\n",
            "[INFO] epoch: 2358, train loss: 0.0000668710, val loss: 0.0000764772\n",
            "[INFO] epoch: 2359, train loss: 0.0000629844, val loss: 0.0002806582\n",
            "[INFO] epoch: 2360, train loss: 0.0000678243, val loss: 0.0000836800\n",
            "[INFO] epoch: 2361, train loss: 0.0000624062, val loss: 0.0002728869\n",
            "[INFO] epoch: 2362, train loss: 0.0000658782, val loss: 0.0000776790\n",
            "[INFO] epoch: 2363, train loss: 0.0000610625, val loss: 0.0002774790\n",
            "[INFO] epoch: 2364, train loss: 0.0000653242, val loss: 0.0000671836\n",
            "[INFO] epoch: 2365, train loss: 0.0000613916, val loss: 0.0002927590\n",
            "[INFO] epoch: 2366, train loss: 0.0000662478, val loss: 0.0000612989\n",
            "[INFO] epoch: 2367, train loss: 0.0000613949, val loss: 0.0003044924\n",
            "[INFO] epoch: 2368, train loss: 0.0000655358, val loss: 0.0000625531\n",
            "[INFO] epoch: 2369, train loss: 0.0000610554, val loss: 0.0002988708\n",
            "[INFO] epoch: 2370, train loss: 0.0000658257, val loss: 0.0000714812\n",
            "[INFO] epoch: 2371, train loss: 0.0000623507, val loss: 0.0002853688\n",
            "[INFO] epoch: 2372, train loss: 0.0000677396, val loss: 0.0000825688\n",
            "[INFO] epoch: 2373, train loss: 0.0000628377, val loss: 0.0002747892\n",
            "[INFO] epoch: 2374, train loss: 0.0000666420, val loss: 0.0000822718\n",
            "[INFO] epoch: 2375, train loss: 0.0000611652, val loss: 0.0002728595\n",
            "[INFO] epoch: 2376, train loss: 0.0000647924, val loss: 0.0000712515\n",
            "[INFO] epoch: 2377, train loss: 0.0000608443, val loss: 0.0002861761\n",
            "[INFO] epoch: 2378, train loss: 0.0000658473, val loss: 0.0000621487\n",
            "[INFO] epoch: 2379, train loss: 0.0000614880, val loss: 0.0003035508\n",
            "[INFO] epoch: 2380, train loss: 0.0000659530, val loss: 0.0000599372\n",
            "[INFO] epoch: 2381, train loss: 0.0000607955, val loss: 0.0003064091\n",
            "[INFO] epoch: 2382, train loss: 0.0000649868, val loss: 0.0000659481\n",
            "[INFO] epoch: 2383, train loss: 0.0000614331, val loss: 0.0002921774\n",
            "[INFO] epoch: 2384, train loss: 0.0000670689, val loss: 0.0000792524\n",
            "[INFO] epoch: 2385, train loss: 0.0000631220, val loss: 0.0002786636\n",
            "[INFO] epoch: 2386, train loss: 0.0000678452, val loss: 0.0000864848\n",
            "[INFO] epoch: 2387, train loss: 0.0000618860, val loss: 0.0002705364\n",
            "[INFO] epoch: 2388, train loss: 0.0000649690, val loss: 0.0000772447\n",
            "[INFO] epoch: 2389, train loss: 0.0000603314, val loss: 0.0002782146\n",
            "[INFO] epoch: 2390, train loss: 0.0000649045, val loss: 0.0000646622\n",
            "[INFO] epoch: 2391, train loss: 0.0000612053, val loss: 0.0002982771\n",
            "[INFO] epoch: 2392, train loss: 0.0000663102, val loss: 0.0000589214\n",
            "[INFO] epoch: 2393, train loss: 0.0000609856, val loss: 0.0003113447\n",
            "[INFO] epoch: 2394, train loss: 0.0000648819, val loss: 0.0000614035\n",
            "[INFO] epoch: 2395, train loss: 0.0000605445, val loss: 0.0003007160\n",
            "[INFO] epoch: 2396, train loss: 0.0000656928, val loss: 0.0000734604\n",
            "[INFO] epoch: 2397, train loss: 0.0000625569, val loss: 0.0002847056\n",
            "[INFO] epoch: 2398, train loss: 0.0000682434, val loss: 0.0000864072\n",
            "[INFO] epoch: 2399, train loss: 0.0000627895, val loss: 0.0002730634\n",
            "[INFO] epoch: 2400, train loss: 0.0000661568, val loss: 0.0000828788\n",
            "[INFO] epoch: 2401, train loss: 0.0000605777, val loss: 0.0002736208\n",
            "[INFO] epoch: 2402, train loss: 0.0000643188, val loss: 0.0000690250\n",
            "[INFO] epoch: 2403, train loss: 0.0000606498, val loss: 0.0002904552\n",
            "[INFO] epoch: 2404, train loss: 0.0000658897, val loss: 0.0000602531\n",
            "[INFO] epoch: 2405, train loss: 0.0000611800, val loss: 0.0003091586\n",
            "[INFO] epoch: 2406, train loss: 0.0000653860, val loss: 0.0000596864\n",
            "[INFO] epoch: 2407, train loss: 0.0000603343, val loss: 0.0003066662\n",
            "[INFO] epoch: 2408, train loss: 0.0000647241, val loss: 0.0000681556\n",
            "[INFO] epoch: 2409, train loss: 0.0000614511, val loss: 0.0002909513\n",
            "[INFO] epoch: 2410, train loss: 0.0000671419, val loss: 0.0000816831\n",
            "[INFO] epoch: 2411, train loss: 0.0000627192, val loss: 0.0002779811\n",
            "[INFO] epoch: 2412, train loss: 0.0000668715, val loss: 0.0000845971\n",
            "[INFO] epoch: 2413, train loss: 0.0000610897, val loss: 0.0002734085\n",
            "[INFO] epoch: 2414, train loss: 0.0000643697, val loss: 0.0000734825\n",
            "[INFO] epoch: 2415, train loss: 0.0000601637, val loss: 0.0002841777\n",
            "[INFO] epoch: 2416, train loss: 0.0000648720, val loss: 0.0000630474\n",
            "[INFO] epoch: 2417, train loss: 0.0000607798, val loss: 0.0003019975\n",
            "[INFO] epoch: 2418, train loss: 0.0000653368, val loss: 0.0000598235\n",
            "[INFO] epoch: 2419, train loss: 0.0000602538, val loss: 0.0003079996\n",
            "[INFO] epoch: 2420, train loss: 0.0000642503, val loss: 0.0000644459\n",
            "[INFO] epoch: 2421, train loss: 0.0000604146, val loss: 0.0002958744\n",
            "[INFO] epoch: 2422, train loss: 0.0000655374, val loss: 0.0000757468\n",
            "[INFO] epoch: 2423, train loss: 0.0000618336, val loss: 0.0002829977\n",
            "[INFO] epoch: 2424, train loss: 0.0000665931, val loss: 0.0000832111\n",
            "[INFO] epoch: 2425, train loss: 0.0000612576, val loss: 0.0002747853\n",
            "[INFO] epoch: 2426, train loss: 0.0000645639, val loss: 0.0000771284\n",
            "[INFO] epoch: 2427, train loss: 0.0000598092, val loss: 0.0002796496\n",
            "[INFO] epoch: 2428, train loss: 0.0000638666, val loss: 0.0000664326\n",
            "[INFO] epoch: 2429, train loss: 0.0000600573, val loss: 0.0002943620\n",
            "[INFO] epoch: 2430, train loss: 0.0000647732, val loss: 0.0000605085\n",
            "[INFO] epoch: 2431, train loss: 0.0000600854, val loss: 0.0003069521\n",
            "[INFO] epoch: 2432, train loss: 0.0000640957, val loss: 0.0000616773\n",
            "[INFO] epoch: 2433, train loss: 0.0000597004, val loss: 0.0003011022\n",
            "[INFO] epoch: 2434, train loss: 0.0000642236, val loss: 0.0000700456\n",
            "[INFO] epoch: 2435, train loss: 0.0000608390, val loss: 0.0002884497\n",
            "[INFO] epoch: 2436, train loss: 0.0000660051, val loss: 0.0000805156\n",
            "[INFO] epoch: 2437, train loss: 0.0000614066, val loss: 0.0002776209\n",
            "[INFO] epoch: 2438, train loss: 0.0000651780, val loss: 0.0000807193\n",
            "[INFO] epoch: 2439, train loss: 0.0000599380, val loss: 0.0002760801\n",
            "[INFO] epoch: 2440, train loss: 0.0000634148, val loss: 0.0000709152\n",
            "[INFO] epoch: 2441, train loss: 0.0000594682, val loss: 0.0002871762\n",
            "[INFO] epoch: 2442, train loss: 0.0000641270, val loss: 0.0000621088\n",
            "[INFO] epoch: 2443, train loss: 0.0000600388, val loss: 0.0003039007\n",
            "[INFO] epoch: 2444, train loss: 0.0000644372, val loss: 0.0000597754\n",
            "[INFO] epoch: 2445, train loss: 0.0000595532, val loss: 0.0003073507\n",
            "[INFO] epoch: 2446, train loss: 0.0000635513, val loss: 0.0000646460\n",
            "[INFO] epoch: 2447, train loss: 0.0000599074, val loss: 0.0002955838\n",
            "[INFO] epoch: 2448, train loss: 0.0000650855, val loss: 0.0000761255\n",
            "[INFO] epoch: 2449, train loss: 0.0000614050, val loss: 0.0002822832\n",
            "[INFO] epoch: 2450, train loss: 0.0000660775, val loss: 0.0000835883\n",
            "[INFO] epoch: 2451, train loss: 0.0000606537, val loss: 0.0002743526\n",
            "[INFO] epoch: 2452, train loss: 0.0000637944, val loss: 0.0000771346\n",
            "[INFO] epoch: 2453, train loss: 0.0000590948, val loss: 0.0002794934\n",
            "[INFO] epoch: 2454, train loss: 0.0000631763, val loss: 0.0000654939\n",
            "[INFO] epoch: 2455, train loss: 0.0000596191, val loss: 0.0002967373\n",
            "[INFO] epoch: 2456, train loss: 0.0000645724, val loss: 0.0000591817\n",
            "[INFO] epoch: 2457, train loss: 0.0000597527, val loss: 0.0003114519\n",
            "[INFO] epoch: 2458, train loss: 0.0000636403, val loss: 0.0000600798\n",
            "[INFO] epoch: 2459, train loss: 0.0000591103, val loss: 0.0003046255\n",
            "[INFO] epoch: 2460, train loss: 0.0000636621, val loss: 0.0000696359\n",
            "[INFO] epoch: 2461, train loss: 0.0000606004, val loss: 0.0002894681\n",
            "[INFO] epoch: 2462, train loss: 0.0000661593, val loss: 0.0000826616\n",
            "[INFO] epoch: 2463, train loss: 0.0000614939, val loss: 0.0002766675\n",
            "[INFO] epoch: 2464, train loss: 0.0000651760, val loss: 0.0000835267\n",
            "[INFO] epoch: 2465, train loss: 0.0000594941, val loss: 0.0002743620\n",
            "[INFO] epoch: 2466, train loss: 0.0000626716, val loss: 0.0000712743\n",
            "[INFO] epoch: 2467, train loss: 0.0000588813, val loss: 0.0002868678\n",
            "[INFO] epoch: 2468, train loss: 0.0000638040, val loss: 0.0000607903\n",
            "[INFO] epoch: 2469, train loss: 0.0000598028, val loss: 0.0003084061\n",
            "[INFO] epoch: 2470, train loss: 0.0000642965, val loss: 0.0000580792\n",
            "[INFO] epoch: 2471, train loss: 0.0000590833, val loss: 0.0003121525\n",
            "[INFO] epoch: 2472, train loss: 0.0000629278, val loss: 0.0000636301\n",
            "[INFO] epoch: 2473, train loss: 0.0000594291, val loss: 0.0002982531\n",
            "[INFO] epoch: 2474, train loss: 0.0000648692, val loss: 0.0000770956\n",
            "[INFO] epoch: 2475, train loss: 0.0000613773, val loss: 0.0002827794\n",
            "[INFO] epoch: 2476, train loss: 0.0000661826, val loss: 0.0000854105\n",
            "[INFO] epoch: 2477, train loss: 0.0000605168, val loss: 0.0002750118\n",
            "[INFO] epoch: 2478, train loss: 0.0000634995, val loss: 0.0000775379\n",
            "[INFO] epoch: 2479, train loss: 0.0000587722, val loss: 0.0002801910\n",
            "[INFO] epoch: 2480, train loss: 0.0000628726, val loss: 0.0000648283\n",
            "[INFO] epoch: 2481, train loss: 0.0000593394, val loss: 0.0002993024\n",
            "[INFO] epoch: 2482, train loss: 0.0000642554, val loss: 0.0000588976\n",
            "[INFO] epoch: 2483, train loss: 0.0000593737, val loss: 0.0003126603\n",
            "[INFO] epoch: 2484, train loss: 0.0000631924, val loss: 0.0000605345\n",
            "[INFO] epoch: 2485, train loss: 0.0000588181, val loss: 0.0003050191\n",
            "[INFO] epoch: 2486, train loss: 0.0000634113, val loss: 0.0000705545\n",
            "[INFO] epoch: 2487, train loss: 0.0000602550, val loss: 0.0002897402\n",
            "[INFO] epoch: 2488, train loss: 0.0000654996, val loss: 0.0000818957\n",
            "[INFO] epoch: 2489, train loss: 0.0000607873, val loss: 0.0002785870\n",
            "[INFO] epoch: 2490, train loss: 0.0000643503, val loss: 0.0000809876\n",
            "[INFO] epoch: 2491, train loss: 0.0000590681, val loss: 0.0002779916\n",
            "[INFO] epoch: 2492, train loss: 0.0000624326, val loss: 0.0000698432\n",
            "[INFO] epoch: 2493, train loss: 0.0000586379, val loss: 0.0002898453\n",
            "[INFO] epoch: 2494, train loss: 0.0000632818, val loss: 0.0000612060\n",
            "[INFO] epoch: 2495, train loss: 0.0000591331, val loss: 0.0003075364\n",
            "[INFO] epoch: 2496, train loss: 0.0000633246, val loss: 0.0000595559\n",
            "[INFO] epoch: 2497, train loss: 0.0000585651, val loss: 0.0003082067\n",
            "[INFO] epoch: 2498, train loss: 0.0000625071, val loss: 0.0000652270\n",
            "[INFO] epoch: 2499, train loss: 0.0000589952, val loss: 0.0002965569\n",
            "[INFO] epoch: 2500, train loss: 0.0000639932, val loss: 0.0000762204\n",
            "[INFO] epoch: 2501, train loss: 0.0000602375, val loss: 0.0002831943\n",
            "[INFO] epoch: 2502, train loss: 0.0000645501, val loss: 0.0000815747\n",
            "[INFO] epoch: 2503, train loss: 0.0000593858, val loss: 0.0002779803\n",
            "[INFO] epoch: 2504, train loss: 0.0000625315, val loss: 0.0000748757\n",
            "[INFO] epoch: 2505, train loss: 0.0000581261, val loss: 0.0002828353\n",
            "[INFO] epoch: 2506, train loss: 0.0000621230, val loss: 0.0000645454\n",
            "[INFO] epoch: 2507, train loss: 0.0000584876, val loss: 0.0002996402\n",
            "[INFO] epoch: 2508, train loss: 0.0000630362, val loss: 0.0000596691\n",
            "[INFO] epoch: 2509, train loss: 0.0000584650, val loss: 0.0003094622\n",
            "[INFO] epoch: 2510, train loss: 0.0000622637, val loss: 0.0000611953\n",
            "[INFO] epoch: 2511, train loss: 0.0000581187, val loss: 0.0003036135\n",
            "[INFO] epoch: 2512, train loss: 0.0000625606, val loss: 0.0000699528\n",
            "[INFO] epoch: 2513, train loss: 0.0000593363, val loss: 0.0002897918\n",
            "[INFO] epoch: 2514, train loss: 0.0000642890, val loss: 0.0000798754\n",
            "[INFO] epoch: 2515, train loss: 0.0000598172, val loss: 0.0002799108\n",
            "[INFO] epoch: 2516, train loss: 0.0000634002, val loss: 0.0000798990\n",
            "[INFO] epoch: 2517, train loss: 0.0000583174, val loss: 0.0002784850\n",
            "[INFO] epoch: 2518, train loss: 0.0000615981, val loss: 0.0000698405\n",
            "[INFO] epoch: 2519, train loss: 0.0000578731, val loss: 0.0002900628\n",
            "[INFO] epoch: 2520, train loss: 0.0000624430, val loss: 0.0000612718\n",
            "[INFO] epoch: 2521, train loss: 0.0000585236, val loss: 0.0003076180\n",
            "[INFO] epoch: 2522, train loss: 0.0000627844, val loss: 0.0000588205\n",
            "[INFO] epoch: 2523, train loss: 0.0000580231, val loss: 0.0003109215\n",
            "[INFO] epoch: 2524, train loss: 0.0000618435, val loss: 0.0000637349\n",
            "[INFO] epoch: 2525, train loss: 0.0000583148, val loss: 0.0002989864\n",
            "[INFO] epoch: 2526, train loss: 0.0000633215, val loss: 0.0000751240\n",
            "[INFO] epoch: 2527, train loss: 0.0000598966, val loss: 0.0002848026\n",
            "[INFO] epoch: 2528, train loss: 0.0000645211, val loss: 0.0000830960\n",
            "[INFO] epoch: 2529, train loss: 0.0000592686, val loss: 0.0002773235\n",
            "[INFO] epoch: 2530, train loss: 0.0000622374, val loss: 0.0000772145\n",
            "[INFO] epoch: 2531, train loss: 0.0000575698, val loss: 0.0002806040\n",
            "[INFO] epoch: 2532, train loss: 0.0000613842, val loss: 0.0000652765\n",
            "[INFO] epoch: 2533, train loss: 0.0000580238, val loss: 0.0002993944\n",
            "[INFO] epoch: 2534, train loss: 0.0000628931, val loss: 0.0000586508\n",
            "[INFO] epoch: 2535, train loss: 0.0000583298, val loss: 0.0003139843\n",
            "[INFO] epoch: 2536, train loss: 0.0000621255, val loss: 0.0000590899\n",
            "[INFO] epoch: 2537, train loss: 0.0000575777, val loss: 0.0003095153\n",
            "[INFO] epoch: 2538, train loss: 0.0000618274, val loss: 0.0000679913\n",
            "[INFO] epoch: 2539, train loss: 0.0000588866, val loss: 0.0002929217\n",
            "[INFO] epoch: 2540, train loss: 0.0000642776, val loss: 0.0000808199\n",
            "[INFO] epoch: 2541, train loss: 0.0000600489, val loss: 0.0002809067\n",
            "[INFO] epoch: 2542, train loss: 0.0000638359, val loss: 0.0000833628\n",
            "[INFO] epoch: 2543, train loss: 0.0000582659, val loss: 0.0002764997\n",
            "[INFO] epoch: 2544, train loss: 0.0000611657, val loss: 0.0000718893\n",
            "[INFO] epoch: 2545, train loss: 0.0000573332, val loss: 0.0002882576\n",
            "[INFO] epoch: 2546, train loss: 0.0000619172, val loss: 0.0000612661\n",
            "[INFO] epoch: 2547, train loss: 0.0000582446, val loss: 0.0003085595\n",
            "[INFO] epoch: 2548, train loss: 0.0000627228, val loss: 0.0000577260\n",
            "[INFO] epoch: 2549, train loss: 0.0000577349, val loss: 0.0003157020\n",
            "[INFO] epoch: 2550, train loss: 0.0000613863, val loss: 0.0000622301\n",
            "[INFO] epoch: 2551, train loss: 0.0000577287, val loss: 0.0003025593\n",
            "[INFO] epoch: 2552, train loss: 0.0000626745, val loss: 0.0000740941\n",
            "[INFO] epoch: 2553, train loss: 0.0000595176, val loss: 0.0002876362\n",
            "[INFO] epoch: 2554, train loss: 0.0000643773, val loss: 0.0000834599\n",
            "[INFO] epoch: 2555, train loss: 0.0000592226, val loss: 0.0002788019\n",
            "[INFO] epoch: 2556, train loss: 0.0000622207, val loss: 0.0000781625\n",
            "[INFO] epoch: 2557, train loss: 0.0000574066, val loss: 0.0002814103\n",
            "[INFO] epoch: 2558, train loss: 0.0000610336, val loss: 0.0000661609\n",
            "[INFO] epoch: 2559, train loss: 0.0000576001, val loss: 0.0002985720\n",
            "[INFO] epoch: 2560, train loss: 0.0000622994, val loss: 0.0000592401\n",
            "[INFO] epoch: 2561, train loss: 0.0000579205, val loss: 0.0003132495\n",
            "[INFO] epoch: 2562, train loss: 0.0000617562, val loss: 0.0000595802\n",
            "[INFO] epoch: 2563, train loss: 0.0000572698, val loss: 0.0003097830\n",
            "[INFO] epoch: 2564, train loss: 0.0000613504, val loss: 0.0000674723\n",
            "[INFO] epoch: 2565, train loss: 0.0000582214, val loss: 0.0002948439\n",
            "[INFO] epoch: 2566, train loss: 0.0000632307, val loss: 0.0000785552\n",
            "[INFO] epoch: 2567, train loss: 0.0000591843, val loss: 0.0002837142\n",
            "[INFO] epoch: 2568, train loss: 0.0000629863, val loss: 0.0000809886\n",
            "[INFO] epoch: 2569, train loss: 0.0000578393, val loss: 0.0002794147\n",
            "[INFO] epoch: 2570, train loss: 0.0000608571, val loss: 0.0000717012\n",
            "[INFO] epoch: 2571, train loss: 0.0000569267, val loss: 0.0002895407\n",
            "[INFO] epoch: 2572, train loss: 0.0000611241, val loss: 0.0000624225\n",
            "[INFO] epoch: 2573, train loss: 0.0000574531, val loss: 0.0003055091\n",
            "[INFO] epoch: 2574, train loss: 0.0000617145, val loss: 0.0000589782\n",
            "[INFO] epoch: 2575, train loss: 0.0000571310, val loss: 0.0003128687\n",
            "[INFO] epoch: 2576, train loss: 0.0000608001, val loss: 0.0000625178\n",
            "[INFO] epoch: 2577, train loss: 0.0000570767, val loss: 0.0003023905\n",
            "[INFO] epoch: 2578, train loss: 0.0000615941, val loss: 0.0000720480\n",
            "[INFO] epoch: 2579, train loss: 0.0000583663, val loss: 0.0002898536\n",
            "[INFO] epoch: 2580, train loss: 0.0000629675, val loss: 0.0000802987\n",
            "[INFO] epoch: 2581, train loss: 0.0000583110, val loss: 0.0002809954\n",
            "[INFO] epoch: 2582, train loss: 0.0000614753, val loss: 0.0000771611\n",
            "[INFO] epoch: 2583, train loss: 0.0000568142, val loss: 0.0002829287\n",
            "[INFO] epoch: 2584, train loss: 0.0000602394, val loss: 0.0000672005\n",
            "[INFO] epoch: 2585, train loss: 0.0000567495, val loss: 0.0002963398\n",
            "[INFO] epoch: 2586, train loss: 0.0000611855, val loss: 0.0000600799\n",
            "[INFO] epoch: 2587, train loss: 0.0000571557, val loss: 0.0003114936\n",
            "[INFO] epoch: 2588, train loss: 0.0000610673, val loss: 0.0000593963\n",
            "[INFO] epoch: 2589, train loss: 0.0000566395, val loss: 0.0003105812\n",
            "[INFO] epoch: 2590, train loss: 0.0000604850, val loss: 0.0000654931\n",
            "[INFO] epoch: 2591, train loss: 0.0000572766, val loss: 0.0002980159\n",
            "[INFO] epoch: 2592, train loss: 0.0000621504, val loss: 0.0000762474\n",
            "[INFO] epoch: 2593, train loss: 0.0000585246, val loss: 0.0002856754\n",
            "[INFO] epoch: 2594, train loss: 0.0000626025, val loss: 0.0000812694\n",
            "[INFO] epoch: 2595, train loss: 0.0000575465, val loss: 0.0002799102\n",
            "[INFO] epoch: 2596, train loss: 0.0000604421, val loss: 0.0000740104\n",
            "[INFO] epoch: 2597, train loss: 0.0000562907, val loss: 0.0002868409\n",
            "[INFO] epoch: 2598, train loss: 0.0000602239, val loss: 0.0000636188\n",
            "[INFO] epoch: 2599, train loss: 0.0000568817, val loss: 0.0003040650\n",
            "[INFO] epoch: 2600, train loss: 0.0000614235, val loss: 0.0000585007\n",
            "[INFO] epoch: 2601, train loss: 0.0000569112, val loss: 0.0003162158\n",
            "[INFO] epoch: 2602, train loss: 0.0000605238, val loss: 0.0000601525\n",
            "[INFO] epoch: 2603, train loss: 0.0000564288, val loss: 0.0003083755\n",
            "[INFO] epoch: 2604, train loss: 0.0000607065, val loss: 0.0000692945\n",
            "[INFO] epoch: 2605, train loss: 0.0000578035, val loss: 0.0002937610\n",
            "[INFO] epoch: 2606, train loss: 0.0000628584, val loss: 0.0000806350\n",
            "[INFO] epoch: 2607, train loss: 0.0000585334, val loss: 0.0002819741\n",
            "[INFO] epoch: 2608, train loss: 0.0000619348, val loss: 0.0000811176\n",
            "[INFO] epoch: 2609, train loss: 0.0000567405, val loss: 0.0002804372\n",
            "[INFO] epoch: 2610, train loss: 0.0000596859, val loss: 0.0000701824\n",
            "[INFO] epoch: 2611, train loss: 0.0000561281, val loss: 0.0002921556\n",
            "[INFO] epoch: 2612, train loss: 0.0000606370, val loss: 0.0000604402\n",
            "[INFO] epoch: 2613, train loss: 0.0000569933, val loss: 0.0003129515\n",
            "[INFO] epoch: 2614, train loss: 0.0000612474, val loss: 0.0000577538\n",
            "[INFO] epoch: 2615, train loss: 0.0000564287, val loss: 0.0003169457\n",
            "[INFO] epoch: 2616, train loss: 0.0000599646, val loss: 0.0000625096\n",
            "[INFO] epoch: 2617, train loss: 0.0000565302, val loss: 0.0003044049\n",
            "[INFO] epoch: 2618, train loss: 0.0000614081, val loss: 0.0000743502\n",
            "[INFO] epoch: 2619, train loss: 0.0000583281, val loss: 0.0002887404\n",
            "[INFO] epoch: 2620, train loss: 0.0000629696, val loss: 0.0000831133\n",
            "[INFO] epoch: 2621, train loss: 0.0000578962, val loss: 0.0002807974\n",
            "[INFO] epoch: 2622, train loss: 0.0000607193, val loss: 0.0000776037\n",
            "[INFO] epoch: 2623, train loss: 0.0000560591, val loss: 0.0002837787\n",
            "[INFO] epoch: 2624, train loss: 0.0000595631, val loss: 0.0000654830\n",
            "[INFO] epoch: 2625, train loss: 0.0000563307, val loss: 0.0003014873\n",
            "[INFO] epoch: 2626, train loss: 0.0000609824, val loss: 0.0000587154\n",
            "[INFO] epoch: 2627, train loss: 0.0000566966, val loss: 0.0003170858\n",
            "[INFO] epoch: 2628, train loss: 0.0000603928, val loss: 0.0000590105\n",
            "[INFO] epoch: 2629, train loss: 0.0000559963, val loss: 0.0003130346\n",
            "[INFO] epoch: 2630, train loss: 0.0000599449, val loss: 0.0000670392\n",
            "[INFO] epoch: 2631, train loss: 0.0000569889, val loss: 0.0002977327\n",
            "[INFO] epoch: 2632, train loss: 0.0000619620, val loss: 0.0000784679\n",
            "[INFO] epoch: 2633, train loss: 0.0000580955, val loss: 0.0002855874\n",
            "[INFO] epoch: 2634, train loss: 0.0000618267, val loss: 0.0000812059\n",
            "[INFO] epoch: 2635, train loss: 0.0000567269, val loss: 0.0002820847\n",
            "[INFO] epoch: 2636, train loss: 0.0000595710, val loss: 0.0000719383\n",
            "[INFO] epoch: 2637, train loss: 0.0000557313, val loss: 0.0002907242\n",
            "[INFO] epoch: 2638, train loss: 0.0000598149, val loss: 0.0000621251\n",
            "[INFO] epoch: 2639, train loss: 0.0000563275, val loss: 0.0003093949\n",
            "[INFO] epoch: 2640, train loss: 0.0000605623, val loss: 0.0000586428\n",
            "[INFO] epoch: 2641, train loss: 0.0000560699, val loss: 0.0003159049\n",
            "[INFO] epoch: 2642, train loss: 0.0000596007, val loss: 0.0000619451\n",
            "[INFO] epoch: 2643, train loss: 0.0000559126, val loss: 0.0003067621\n",
            "[INFO] epoch: 2644, train loss: 0.0000602971, val loss: 0.0000715161\n",
            "[INFO] epoch: 2645, train loss: 0.0000572304, val loss: 0.0002923555\n",
            "[INFO] epoch: 2646, train loss: 0.0000617556, val loss: 0.0000798727\n",
            "[INFO] epoch: 2647, train loss: 0.0000572833, val loss: 0.0002844637\n",
            "[INFO] epoch: 2648, train loss: 0.0000604039, val loss: 0.0000774322\n",
            "[INFO] epoch: 2649, train loss: 0.0000557877, val loss: 0.0002847090\n",
            "[INFO] epoch: 2650, train loss: 0.0000590129, val loss: 0.0000673478\n",
            "[INFO] epoch: 2651, train loss: 0.0000556064, val loss: 0.0002988419\n",
            "[INFO] epoch: 2652, train loss: 0.0000599009, val loss: 0.0000603607\n",
            "[INFO] epoch: 2653, train loss: 0.0000560348, val loss: 0.0003131179\n",
            "[INFO] epoch: 2654, train loss: 0.0000598515, val loss: 0.0000592889\n",
            "[INFO] epoch: 2655, train loss: 0.0000555340, val loss: 0.0003139371\n",
            "[INFO] epoch: 2656, train loss: 0.0000592245, val loss: 0.0000650083\n",
            "[INFO] epoch: 2657, train loss: 0.0000560246, val loss: 0.0003009985\n",
            "[INFO] epoch: 2658, train loss: 0.0000606330, val loss: 0.0000749833\n",
            "[INFO] epoch: 2659, train loss: 0.0000572071, val loss: 0.0002893097\n",
            "[INFO] epoch: 2660, train loss: 0.0000612377, val loss: 0.0000802618\n",
            "[INFO] epoch: 2661, train loss: 0.0000564522, val loss: 0.0002833372\n",
            "[INFO] epoch: 2662, train loss: 0.0000592864, val loss: 0.0000740740\n",
            "[INFO] epoch: 2663, train loss: 0.0000551760, val loss: 0.0002886651\n",
            "[INFO] epoch: 2664, train loss: 0.0000588395, val loss: 0.0000642797\n",
            "[INFO] epoch: 2665, train loss: 0.0000555686, val loss: 0.0003053246\n",
            "[INFO] epoch: 2666, train loss: 0.0000598995, val loss: 0.0000589890\n",
            "[INFO] epoch: 2667, train loss: 0.0000557101, val loss: 0.0003168914\n",
            "[INFO] epoch: 2668, train loss: 0.0000592785, val loss: 0.0000601145\n",
            "[INFO] epoch: 2669, train loss: 0.0000552207, val loss: 0.0003115739\n",
            "[INFO] epoch: 2670, train loss: 0.0000591863, val loss: 0.0000679755\n",
            "[INFO] epoch: 2671, train loss: 0.0000562852, val loss: 0.0002972462\n",
            "[INFO] epoch: 2672, train loss: 0.0000610582, val loss: 0.0000784190\n",
            "[INFO] epoch: 2673, train loss: 0.0000571440, val loss: 0.0002864639\n",
            "[INFO] epoch: 2674, train loss: 0.0000606446, val loss: 0.0000804167\n",
            "[INFO] epoch: 2675, train loss: 0.0000556943, val loss: 0.0002827039\n",
            "[INFO] epoch: 2676, train loss: 0.0000584786, val loss: 0.0000710646\n",
            "[INFO] epoch: 2677, train loss: 0.0000548538, val loss: 0.0002938096\n",
            "[INFO] epoch: 2678, train loss: 0.0000589858, val loss: 0.0000615677\n",
            "[INFO] epoch: 2679, train loss: 0.0000556387, val loss: 0.0003117540\n",
            "[INFO] epoch: 2680, train loss: 0.0000598819, val loss: 0.0000580172\n",
            "[INFO] epoch: 2681, train loss: 0.0000553381, val loss: 0.0003201539\n",
            "[INFO] epoch: 2682, train loss: 0.0000587555, val loss: 0.0000614454\n",
            "[INFO] epoch: 2683, train loss: 0.0000551580, val loss: 0.0003083925\n",
            "[INFO] epoch: 2684, train loss: 0.0000595919, val loss: 0.0000717528\n",
            "[INFO] epoch: 2685, train loss: 0.0000567679, val loss: 0.0002941851\n",
            "[INFO] epoch: 2686, train loss: 0.0000614923, val loss: 0.0000815803\n",
            "[INFO] epoch: 2687, train loss: 0.0000569263, val loss: 0.0002838017\n",
            "[INFO] epoch: 2688, train loss: 0.0000598542, val loss: 0.0000789482\n",
            "[INFO] epoch: 2689, train loss: 0.0000550649, val loss: 0.0002851741\n",
            "[INFO] epoch: 2690, train loss: 0.0000581400, val loss: 0.0000675398\n",
            "[INFO] epoch: 2691, train loss: 0.0000549242, val loss: 0.0002997866\n",
            "[INFO] epoch: 2692, train loss: 0.0000593997, val loss: 0.0000593833\n",
            "[INFO] epoch: 2693, train loss: 0.0000556008, val loss: 0.0003186569\n",
            "[INFO] epoch: 2694, train loss: 0.0000594467, val loss: 0.0000582412\n",
            "[INFO] epoch: 2695, train loss: 0.0000549120, val loss: 0.0003182658\n",
            "[INFO] epoch: 2696, train loss: 0.0000584585, val loss: 0.0000644460\n",
            "[INFO] epoch: 2697, train loss: 0.0000554178, val loss: 0.0003043019\n",
            "[INFO] epoch: 2698, train loss: 0.0000602256, val loss: 0.0000759594\n",
            "[INFO] epoch: 2699, train loss: 0.0000569589, val loss: 0.0002901621\n",
            "[INFO] epoch: 2700, train loss: 0.0000610543, val loss: 0.0000820398\n",
            "[INFO] epoch: 2701, train loss: 0.0000560794, val loss: 0.0002839270\n",
            "[INFO] epoch: 2702, train loss: 0.0000587292, val loss: 0.0000747957\n",
            "[INFO] epoch: 2703, train loss: 0.0000545836, val loss: 0.0002900052\n",
            "[INFO] epoch: 2704, train loss: 0.0000582016, val loss: 0.0000639357\n",
            "[INFO] epoch: 2705, train loss: 0.0000550468, val loss: 0.0003075787\n",
            "[INFO] epoch: 2706, train loss: 0.0000594061, val loss: 0.0000586159\n",
            "[INFO] epoch: 2707, train loss: 0.0000551546, val loss: 0.0003203519\n",
            "[INFO] epoch: 2708, train loss: 0.0000586218, val loss: 0.0000601389\n",
            "[INFO] epoch: 2709, train loss: 0.0000546410, val loss: 0.0003133537\n",
            "[INFO] epoch: 2710, train loss: 0.0000585903, val loss: 0.0000684752\n",
            "[INFO] epoch: 2711, train loss: 0.0000557390, val loss: 0.0002990726\n",
            "[INFO] epoch: 2712, train loss: 0.0000604012, val loss: 0.0000785488\n",
            "[INFO] epoch: 2713, train loss: 0.0000564785, val loss: 0.0002878777\n",
            "[INFO] epoch: 2714, train loss: 0.0000598400, val loss: 0.0000793824\n",
            "[INFO] epoch: 2715, train loss: 0.0000550777, val loss: 0.0002864464\n",
            "[INFO] epoch: 2716, train loss: 0.0000579123, val loss: 0.0000702937\n",
            "[INFO] epoch: 2717, train loss: 0.0000543680, val loss: 0.0002959935\n",
            "[INFO] epoch: 2718, train loss: 0.0000583679, val loss: 0.0000615917\n",
            "[INFO] epoch: 2719, train loss: 0.0000549316, val loss: 0.0003139995\n",
            "[INFO] epoch: 2720, train loss: 0.0000589119, val loss: 0.0000589698\n",
            "[INFO] epoch: 2721, train loss: 0.0000546200, val loss: 0.0003181409\n",
            "[INFO] epoch: 2722, train loss: 0.0000580384, val loss: 0.0000627248\n",
            "[INFO] epoch: 2723, train loss: 0.0000545982, val loss: 0.0003087226\n",
            "[INFO] epoch: 2724, train loss: 0.0000588604, val loss: 0.0000719655\n",
            "[INFO] epoch: 2725, train loss: 0.0000558541, val loss: 0.0002946176\n",
            "[INFO] epoch: 2726, train loss: 0.0000601128, val loss: 0.0000794199\n",
            "[INFO] epoch: 2727, train loss: 0.0000557617, val loss: 0.0002875200\n",
            "[INFO] epoch: 2728, train loss: 0.0000587007, val loss: 0.0000764803\n",
            "[INFO] epoch: 2729, train loss: 0.0000543265, val loss: 0.0002886359\n",
            "[INFO] epoch: 2730, train loss: 0.0000574603, val loss: 0.0000668588\n",
            "[INFO] epoch: 2731, train loss: 0.0000542252, val loss: 0.0003027941\n",
            "[INFO] epoch: 2732, train loss: 0.0000583774, val loss: 0.0000602383\n",
            "[INFO] epoch: 2733, train loss: 0.0000546411, val loss: 0.0003170866\n",
            "[INFO] epoch: 2734, train loss: 0.0000583001, val loss: 0.0000594426\n",
            "[INFO] epoch: 2735, train loss: 0.0000541487, val loss: 0.0003170259\n",
            "[INFO] epoch: 2736, train loss: 0.0000576944, val loss: 0.0000651050\n",
            "[INFO] epoch: 2737, train loss: 0.0000546303, val loss: 0.0003042029\n",
            "[INFO] epoch: 2738, train loss: 0.0000590864, val loss: 0.0000748981\n",
            "[INFO] epoch: 2739, train loss: 0.0000558151, val loss: 0.0002921239\n",
            "[INFO] epoch: 2740, train loss: 0.0000597017, val loss: 0.0000801022\n",
            "[INFO] epoch: 2741, train loss: 0.0000550684, val loss: 0.0002865650\n",
            "[INFO] epoch: 2742, train loss: 0.0000577521, val loss: 0.0000741330\n",
            "[INFO] epoch: 2743, train loss: 0.0000537695, val loss: 0.0002914791\n",
            "[INFO] epoch: 2744, train loss: 0.0000572679, val loss: 0.0000643279\n",
            "[INFO] epoch: 2745, train loss: 0.0000541774, val loss: 0.0003089369\n",
            "[INFO] epoch: 2746, train loss: 0.0000584160, val loss: 0.0000589932\n",
            "[INFO] epoch: 2747, train loss: 0.0000543958, val loss: 0.0003206400\n",
            "[INFO] epoch: 2748, train loss: 0.0000578527, val loss: 0.0000599770\n",
            "[INFO] epoch: 2749, train loss: 0.0000538704, val loss: 0.0003161346\n",
            "[INFO] epoch: 2750, train loss: 0.0000576542, val loss: 0.0000675709\n",
            "[INFO] epoch: 2751, train loss: 0.0000548969, val loss: 0.0003008825\n",
            "[INFO] epoch: 2752, train loss: 0.0000595656, val loss: 0.0000780816\n",
            "[INFO] epoch: 2753, train loss: 0.0000558939, val loss: 0.0002903268\n",
            "[INFO] epoch: 2754, train loss: 0.0000593635, val loss: 0.0000806088\n",
            "[INFO] epoch: 2755, train loss: 0.0000545255, val loss: 0.0002857020\n",
            "[INFO] epoch: 2756, train loss: 0.0000571342, val loss: 0.0000717111\n",
            "[INFO] epoch: 2757, train loss: 0.0000535549, val loss: 0.0002964897\n",
            "[INFO] epoch: 2758, train loss: 0.0000574737, val loss: 0.0000620228\n",
            "[INFO] epoch: 2759, train loss: 0.0000543140, val loss: 0.0003144067\n",
            "[INFO] epoch: 2760, train loss: 0.0000584818, val loss: 0.0000582342\n",
            "[INFO] epoch: 2761, train loss: 0.0000541195, val loss: 0.0003240565\n",
            "[INFO] epoch: 2762, train loss: 0.0000574100, val loss: 0.0000612290\n",
            "[INFO] epoch: 2763, train loss: 0.0000538053, val loss: 0.0003127091\n",
            "[INFO] epoch: 2764, train loss: 0.0000579667, val loss: 0.0000709295\n",
            "[INFO] epoch: 2765, train loss: 0.0000552901, val loss: 0.0002984005\n",
            "[INFO] epoch: 2766, train loss: 0.0000598971, val loss: 0.0000807007\n",
            "[INFO] epoch: 2767, train loss: 0.0000556419, val loss: 0.0002878428\n",
            "[INFO] epoch: 2768, train loss: 0.0000585672, val loss: 0.0000791400\n",
            "[INFO] epoch: 2769, train loss: 0.0000538825, val loss: 0.0002880700\n",
            "[INFO] epoch: 2770, train loss: 0.0000567229, val loss: 0.0000684336\n",
            "[INFO] epoch: 2771, train loss: 0.0000535200, val loss: 0.0003019380\n",
            "[INFO] epoch: 2772, train loss: 0.0000577159, val loss: 0.0000601617\n",
            "[INFO] epoch: 2773, train loss: 0.0000542123, val loss: 0.0003199466\n",
            "[INFO] epoch: 2774, train loss: 0.0000580134, val loss: 0.0000586021\n",
            "[INFO] epoch: 2775, train loss: 0.0000536420, val loss: 0.0003221865\n",
            "[INFO] epoch: 2776, train loss: 0.0000569839, val loss: 0.0000638803\n",
            "[INFO] epoch: 2777, train loss: 0.0000539043, val loss: 0.0003082999\n",
            "[INFO] epoch: 2778, train loss: 0.0000583613, val loss: 0.0000743853\n",
            "[INFO] epoch: 2779, train loss: 0.0000553401, val loss: 0.0002952642\n",
            "[INFO] epoch: 2780, train loss: 0.0000594039, val loss: 0.0000809190\n",
            "[INFO] epoch: 2781, train loss: 0.0000548270, val loss: 0.0002874964\n",
            "[INFO] epoch: 2782, train loss: 0.0000574710, val loss: 0.0000755107\n",
            "[INFO] epoch: 2783, train loss: 0.0000533343, val loss: 0.0002929659\n",
            "[INFO] epoch: 2784, train loss: 0.0000566152, val loss: 0.0000652399\n",
            "[INFO] epoch: 2785, train loss: 0.0000535720, val loss: 0.0003079001\n",
            "[INFO] epoch: 2786, train loss: 0.0000577454, val loss: 0.0000594959\n",
            "[INFO] epoch: 2787, train loss: 0.0000538533, val loss: 0.0003227560\n",
            "[INFO] epoch: 2788, train loss: 0.0000572975, val loss: 0.0000600793\n",
            "[INFO] epoch: 2789, train loss: 0.0000533404, val loss: 0.0003172290\n",
            "[INFO] epoch: 2790, train loss: 0.0000569668, val loss: 0.0000671322\n",
            "[INFO] epoch: 2791, train loss: 0.0000541219, val loss: 0.0003048022\n",
            "[INFO] epoch: 2792, train loss: 0.0000585485, val loss: 0.0000766749\n",
            "[INFO] epoch: 2793, train loss: 0.0000550575, val loss: 0.0002922077\n",
            "[INFO] epoch: 2794, train loss: 0.0000585359, val loss: 0.0000794539\n",
            "[INFO] epoch: 2795, train loss: 0.0000539749, val loss: 0.0002899124\n",
            "[INFO] epoch: 2796, train loss: 0.0000566248, val loss: 0.0000719140\n",
            "[INFO] epoch: 2797, train loss: 0.0000529943, val loss: 0.0002967846\n",
            "[INFO] epoch: 2798, train loss: 0.0000566166, val loss: 0.0000630929\n",
            "[INFO] epoch: 2799, train loss: 0.0000534648, val loss: 0.0003143181\n",
            "[INFO] epoch: 2800, train loss: 0.0000574123, val loss: 0.0000593926\n",
            "[INFO] epoch: 2801, train loss: 0.0000533666, val loss: 0.0003215910\n",
            "[INFO] epoch: 2802, train loss: 0.0000566518, val loss: 0.0000618125\n",
            "[INFO] epoch: 2803, train loss: 0.0000531002, val loss: 0.0003143609\n",
            "[INFO] epoch: 2804, train loss: 0.0000569737, val loss: 0.0000698253\n",
            "[INFO] epoch: 2805, train loss: 0.0000541705, val loss: 0.0003004280\n",
            "[INFO] epoch: 2806, train loss: 0.0000584364, val loss: 0.0000782609\n",
            "[INFO] epoch: 2807, train loss: 0.0000545777, val loss: 0.0002914529\n",
            "[INFO] epoch: 2808, train loss: 0.0000576442, val loss: 0.0000779749\n",
            "[INFO] epoch: 2809, train loss: 0.0000532267, val loss: 0.0002907838\n",
            "[INFO] epoch: 2810, train loss: 0.0000559937, val loss: 0.0000692636\n",
            "[INFO] epoch: 2811, train loss: 0.0000527464, val loss: 0.0003017136\n",
            "[INFO] epoch: 2812, train loss: 0.0000566414, val loss: 0.0000615671\n",
            "[INFO] epoch: 2813, train loss: 0.0000533273, val loss: 0.0003187246\n",
            "[INFO] epoch: 2814, train loss: 0.0000570964, val loss: 0.0000593001\n",
            "[INFO] epoch: 2815, train loss: 0.0000530112, val loss: 0.0003220334\n",
            "[INFO] epoch: 2816, train loss: 0.0000562941, val loss: 0.0000632893\n",
            "[INFO] epoch: 2817, train loss: 0.0000530456, val loss: 0.0003119951\n",
            "[INFO] epoch: 2818, train loss: 0.0000571703, val loss: 0.0000723986\n",
            "[INFO] epoch: 2819, train loss: 0.0000543547, val loss: 0.0002976164\n",
            "[INFO] epoch: 2820, train loss: 0.0000585065, val loss: 0.0000799763\n",
            "[INFO] epoch: 2821, train loss: 0.0000542602, val loss: 0.0002908930\n",
            "[INFO] epoch: 2822, train loss: 0.0000569850, val loss: 0.0000769334\n",
            "[INFO] epoch: 2823, train loss: 0.0000527402, val loss: 0.0002914417\n",
            "[INFO] epoch: 2824, train loss: 0.0000557240, val loss: 0.0000671545\n",
            "[INFO] epoch: 2825, train loss: 0.0000527114, val loss: 0.0003075443\n",
            "[INFO] epoch: 2826, train loss: 0.0000568201, val loss: 0.0000601806\n",
            "[INFO] epoch: 2827, train loss: 0.0000532783, val loss: 0.0003222856\n",
            "[INFO] epoch: 2828, train loss: 0.0000568705, val loss: 0.0000593620\n",
            "[INFO] epoch: 2829, train loss: 0.0000527192, val loss: 0.0003233056\n",
            "[INFO] epoch: 2830, train loss: 0.0000560526, val loss: 0.0000649649\n",
            "[INFO] epoch: 2831, train loss: 0.0000531456, val loss: 0.0003084398\n",
            "[INFO] epoch: 2832, train loss: 0.0000575626, val loss: 0.0000753408\n",
            "[INFO] epoch: 2833, train loss: 0.0000545491, val loss: 0.0002963769\n",
            "[INFO] epoch: 2834, train loss: 0.0000584201, val loss: 0.0000812324\n",
            "[INFO] epoch: 2835, train loss: 0.0000538224, val loss: 0.0002890562\n",
            "[INFO] epoch: 2836, train loss: 0.0000562957, val loss: 0.0000752211\n",
            "[INFO] epoch: 2837, train loss: 0.0000523467, val loss: 0.0002950683\n",
            "[INFO] epoch: 2838, train loss: 0.0000556331, val loss: 0.0000648893\n",
            "[INFO] epoch: 2839, train loss: 0.0000527244, val loss: 0.0003120653\n",
            "[INFO] epoch: 2840, train loss: 0.0000568951, val loss: 0.0000592169\n",
            "[INFO] epoch: 2841, train loss: 0.0000530315, val loss: 0.0003261283\n",
            "[INFO] epoch: 2842, train loss: 0.0000563584, val loss: 0.0000600734\n",
            "[INFO] epoch: 2843, train loss: 0.0000524066, val loss: 0.0003207968\n",
            "[INFO] epoch: 2844, train loss: 0.0000559646, val loss: 0.0000675665\n",
            "[INFO] epoch: 2845, train loss: 0.0000533388, val loss: 0.0003055964\n",
            "[INFO] epoch: 2846, train loss: 0.0000578438, val loss: 0.0000778736\n",
            "[INFO] epoch: 2847, train loss: 0.0000543915, val loss: 0.0002942873\n",
            "[INFO] epoch: 2848, train loss: 0.0000577740, val loss: 0.0000806062\n",
            "[INFO] epoch: 2849, train loss: 0.0000531522, val loss: 0.0002899824\n",
            "[INFO] epoch: 2850, train loss: 0.0000556372, val loss: 0.0000722854\n",
            "[INFO] epoch: 2851, train loss: 0.0000521100, val loss: 0.0002999427\n",
            "[INFO] epoch: 2852, train loss: 0.0000557383, val loss: 0.0000628680\n",
            "[INFO] epoch: 2853, train loss: 0.0000527398, val loss: 0.0003170397\n",
            "[INFO] epoch: 2854, train loss: 0.0000567105, val loss: 0.0000592180\n",
            "[INFO] epoch: 2855, train loss: 0.0000526287, val loss: 0.0003266498\n",
            "[INFO] epoch: 2856, train loss: 0.0000558025, val loss: 0.0000619047\n",
            "[INFO] epoch: 2857, train loss: 0.0000523187, val loss: 0.0003163046\n",
            "[INFO] epoch: 2858, train loss: 0.0000561721, val loss: 0.0000705245\n",
            "[INFO] epoch: 2859, train loss: 0.0000534985, val loss: 0.0003031259\n",
            "[INFO] epoch: 2860, train loss: 0.0000577485, val loss: 0.0000790827\n",
            "[INFO] epoch: 2861, train loss: 0.0000538919, val loss: 0.0002926270\n",
            "[INFO] epoch: 2862, train loss: 0.0000568345, val loss: 0.0000783328\n",
            "[INFO] epoch: 2863, train loss: 0.0000524764, val loss: 0.0002935536\n",
            "[INFO] epoch: 2864, train loss: 0.0000551800, val loss: 0.0000692392\n",
            "[INFO] epoch: 2865, train loss: 0.0000520126, val loss: 0.0003043020\n",
            "[INFO] epoch: 2866, train loss: 0.0000558414, val loss: 0.0000616896\n",
            "[INFO] epoch: 2867, train loss: 0.0000525703, val loss: 0.0003217052\n",
            "[INFO] epoch: 2868, train loss: 0.0000562138, val loss: 0.0000598314\n",
            "[INFO] epoch: 2869, train loss: 0.0000522038, val loss: 0.0003236816\n",
            "[INFO] epoch: 2870, train loss: 0.0000554125, val loss: 0.0000641428\n",
            "[INFO] epoch: 2871, train loss: 0.0000522938, val loss: 0.0003132892\n",
            "[INFO] epoch: 2872, train loss: 0.0000563095, val loss: 0.0000729174\n",
            "[INFO] epoch: 2873, train loss: 0.0000534527, val loss: 0.0002997146\n",
            "[INFO] epoch: 2874, train loss: 0.0000573590, val loss: 0.0000794175\n",
            "[INFO] epoch: 2875, train loss: 0.0000532606, val loss: 0.0002932667\n",
            "[INFO] epoch: 2876, train loss: 0.0000559367, val loss: 0.0000760287\n",
            "[INFO] epoch: 2877, train loss: 0.0000519054, val loss: 0.0002956009\n",
            "[INFO] epoch: 2878, train loss: 0.0000548623, val loss: 0.0000670011\n",
            "[INFO] epoch: 2879, train loss: 0.0000518957, val loss: 0.0003097026\n",
            "[INFO] epoch: 2880, train loss: 0.0000557910, val loss: 0.0000609348\n",
            "[INFO] epoch: 2881, train loss: 0.0000522918, val loss: 0.0003237661\n",
            "[INFO] epoch: 2882, train loss: 0.0000557086, val loss: 0.0000605063\n",
            "[INFO] epoch: 2883, train loss: 0.0000518544, val loss: 0.0003223731\n",
            "[INFO] epoch: 2884, train loss: 0.0000551686, val loss: 0.0000659971\n",
            "[INFO] epoch: 2885, train loss: 0.0000522892, val loss: 0.0003103152\n",
            "[INFO] epoch: 2886, train loss: 0.0000564253, val loss: 0.0000750268\n",
            "[INFO] epoch: 2887, train loss: 0.0000533974, val loss: 0.0002978188\n",
            "[INFO] epoch: 2888, train loss: 0.0000570328, val loss: 0.0000798439\n",
            "[INFO] epoch: 2889, train loss: 0.0000527356, val loss: 0.0002933869\n",
            "[INFO] epoch: 2890, train loss: 0.0000552429, val loss: 0.0000744189\n",
            "[INFO] epoch: 2891, train loss: 0.0000515135, val loss: 0.0002976030\n",
            "[INFO] epoch: 2892, train loss: 0.0000547311, val loss: 0.0000653623\n",
            "[INFO] epoch: 2893, train loss: 0.0000518396, val loss: 0.0003148320\n",
            "[INFO] epoch: 2894, train loss: 0.0000557768, val loss: 0.0000602158\n",
            "[INFO] epoch: 2895, train loss: 0.0000520976, val loss: 0.0003257357\n",
            "[INFO] epoch: 2896, train loss: 0.0000553669, val loss: 0.0000610451\n",
            "[INFO] epoch: 2897, train loss: 0.0000515963, val loss: 0.0003223495\n",
            "[INFO] epoch: 2898, train loss: 0.0000550313, val loss: 0.0000677419\n",
            "[INFO] epoch: 2899, train loss: 0.0000523783, val loss: 0.0003071715\n",
            "[INFO] epoch: 2900, train loss: 0.0000566571, val loss: 0.0000772978\n",
            "[INFO] epoch: 2901, train loss: 0.0000533674, val loss: 0.0002971829\n",
            "[INFO] epoch: 2902, train loss: 0.0000567049, val loss: 0.0000803362\n",
            "[INFO] epoch: 2903, train loss: 0.0000522612, val loss: 0.0002921050\n",
            "[INFO] epoch: 2904, train loss: 0.0000546796, val loss: 0.0000729624\n",
            "[INFO] epoch: 2905, train loss: 0.0000512161, val loss: 0.0003015143\n",
            "[INFO] epoch: 2906, train loss: 0.0000546889, val loss: 0.0000637694\n",
            "[INFO] epoch: 2907, train loss: 0.0000518151, val loss: 0.0003183271\n",
            "[INFO] epoch: 2908, train loss: 0.0000557540, val loss: 0.0000596842\n",
            "[INFO] epoch: 2909, train loss: 0.0000518430, val loss: 0.0003289538\n",
            "[INFO] epoch: 2910, train loss: 0.0000549595, val loss: 0.0000617653\n",
            "[INFO] epoch: 2911, train loss: 0.0000514008, val loss: 0.0003200765\n",
            "[INFO] epoch: 2912, train loss: 0.0000550614, val loss: 0.0000699628\n",
            "[INFO] epoch: 2913, train loss: 0.0000525498, val loss: 0.0003057028\n",
            "[INFO] epoch: 2914, train loss: 0.0000568557, val loss: 0.0000792959\n",
            "[INFO] epoch: 2915, train loss: 0.0000532151, val loss: 0.0002953571\n",
            "[INFO] epoch: 2916, train loss: 0.0000562072, val loss: 0.0000799210\n",
            "[INFO] epoch: 2917, train loss: 0.0000517888, val loss: 0.0002935454\n",
            "[INFO] epoch: 2918, train loss: 0.0000542760, val loss: 0.0000709087\n",
            "[INFO] epoch: 2919, train loss: 0.0000510923, val loss: 0.0003053346\n",
            "[INFO] epoch: 2920, train loss: 0.0000548151, val loss: 0.0000624028\n",
            "[INFO] epoch: 2921, train loss: 0.0000518089, val loss: 0.0003227118\n",
            "[INFO] epoch: 2922, train loss: 0.0000555666, val loss: 0.0000597647\n",
            "[INFO] epoch: 2923, train loss: 0.0000515197, val loss: 0.0003291023\n",
            "[INFO] epoch: 2924, train loss: 0.0000545796, val loss: 0.0000634069\n",
            "[INFO] epoch: 2925, train loss: 0.0000513873, val loss: 0.0003169188\n",
            "[INFO] epoch: 2926, train loss: 0.0000552869, val loss: 0.0000724322\n",
            "[INFO] epoch: 2927, train loss: 0.0000526635, val loss: 0.0003038871\n",
            "[INFO] epoch: 2928, train loss: 0.0000567226, val loss: 0.0000802230\n",
            "[INFO] epoch: 2929, train loss: 0.0000527786, val loss: 0.0002943904\n",
            "[INFO] epoch: 2930, train loss: 0.0000554599, val loss: 0.0000779922\n",
            "[INFO] epoch: 2931, train loss: 0.0000512919, val loss: 0.0002969703\n",
            "[INFO] epoch: 2932, train loss: 0.0000540052, val loss: 0.0000685551\n",
            "[INFO] epoch: 2933, train loss: 0.0000510661, val loss: 0.0003092710\n",
            "[INFO] epoch: 2934, train loss: 0.0000549003, val loss: 0.0000616549\n",
            "[INFO] epoch: 2935, train loss: 0.0000516078, val loss: 0.0003263171\n",
            "[INFO] epoch: 2936, train loss: 0.0000550591, val loss: 0.0000605521\n",
            "[INFO] epoch: 2937, train loss: 0.0000511572, val loss: 0.0003258380\n",
            "[INFO] epoch: 2938, train loss: 0.0000543035, val loss: 0.0000655853\n",
            "[INFO] epoch: 2939, train loss: 0.0000513888, val loss: 0.0003144397\n",
            "[INFO] epoch: 2940, train loss: 0.0000553727, val loss: 0.0000744671\n",
            "[INFO] epoch: 2941, train loss: 0.0000525236, val loss: 0.0003009619\n",
            "[INFO] epoch: 2942, train loss: 0.0000562095, val loss: 0.0000800509\n",
            "[INFO] epoch: 2943, train loss: 0.0000521129, val loss: 0.0002957909\n",
            "[INFO] epoch: 2944, train loss: 0.0000546150, val loss: 0.0000756107\n",
            "[INFO] epoch: 2945, train loss: 0.0000508153, val loss: 0.0002991746\n",
            "[INFO] epoch: 2946, train loss: 0.0000537951, val loss: 0.0000667201\n",
            "[INFO] epoch: 2947, train loss: 0.0000509542, val loss: 0.0003145125\n",
            "[INFO] epoch: 2948, train loss: 0.0000547470, val loss: 0.0000613042\n",
            "[INFO] epoch: 2949, train loss: 0.0000512625, val loss: 0.0003268561\n",
            "[INFO] epoch: 2950, train loss: 0.0000545100, val loss: 0.0000615865\n",
            "[INFO] epoch: 2951, train loss: 0.0000508299, val loss: 0.0003240942\n",
            "[INFO] epoch: 2952, train loss: 0.0000540991, val loss: 0.0000674545\n",
            "[INFO] epoch: 2953, train loss: 0.0000513656, val loss: 0.0003112978\n",
            "[INFO] epoch: 2954, train loss: 0.0000553848, val loss: 0.0000761235\n",
            "[INFO] epoch: 2955, train loss: 0.0000523340, val loss: 0.0003000012\n",
            "[INFO] epoch: 2956, train loss: 0.0000557177, val loss: 0.0000797742\n",
            "[INFO] epoch: 2957, train loss: 0.0000515526, val loss: 0.0002962813\n",
            "[INFO] epoch: 2958, train loss: 0.0000539934, val loss: 0.0000739189\n",
            "[INFO] epoch: 2959, train loss: 0.0000505074, val loss: 0.0003020003\n",
            "[INFO] epoch: 2960, train loss: 0.0000537173, val loss: 0.0000654455\n",
            "[INFO] epoch: 2961, train loss: 0.0000508799, val loss: 0.0003186747\n",
            "[INFO] epoch: 2962, train loss: 0.0000546424, val loss: 0.0000610480\n",
            "[INFO] epoch: 2963, train loss: 0.0000510417, val loss: 0.0003279774\n",
            "[INFO] epoch: 2964, train loss: 0.0000541765, val loss: 0.0000623719\n",
            "[INFO] epoch: 2965, train loss: 0.0000506221, val loss: 0.0003233443\n",
            "[INFO] epoch: 2966, train loss: 0.0000540142, val loss: 0.0000691146\n",
            "[INFO] epoch: 2967, train loss: 0.0000514337, val loss: 0.0003087460\n",
            "[INFO] epoch: 2968, train loss: 0.0000555083, val loss: 0.0000778236\n",
            "[INFO] epoch: 2969, train loss: 0.0000522174, val loss: 0.0002997009\n",
            "[INFO] epoch: 2970, train loss: 0.0000553447, val loss: 0.0000799223\n",
            "[INFO] epoch: 2971, train loss: 0.0000511313, val loss: 0.0002955720\n",
            "[INFO] epoch: 2972, train loss: 0.0000535399, val loss: 0.0000727348\n",
            "[INFO] epoch: 2973, train loss: 0.0000502573, val loss: 0.0003056675\n",
            "[INFO] epoch: 2974, train loss: 0.0000536551, val loss: 0.0000643047\n",
            "[INFO] epoch: 2975, train loss: 0.0000508251, val loss: 0.0003213025\n",
            "[INFO] epoch: 2976, train loss: 0.0000545756, val loss: 0.0000607550\n",
            "[INFO] epoch: 2977, train loss: 0.0000507945, val loss: 0.0003307704\n",
            "[INFO] epoch: 2978, train loss: 0.0000538106, val loss: 0.0000630155\n",
            "[INFO] epoch: 2979, train loss: 0.0000504301, val loss: 0.0003212256\n",
            "[INFO] epoch: 2980, train loss: 0.0000539879, val loss: 0.0000708954\n",
            "[INFO] epoch: 2981, train loss: 0.0000515000, val loss: 0.0003080029\n",
            "[INFO] epoch: 2982, train loss: 0.0000555916, val loss: 0.0000795092\n",
            "[INFO] epoch: 2983, train loss: 0.0000520536, val loss: 0.0002977162\n",
            "[INFO] epoch: 2984, train loss: 0.0000549234, val loss: 0.0000799112\n",
            "[INFO] epoch: 2985, train loss: 0.0000506943, val loss: 0.0002969133\n",
            "[INFO] epoch: 2986, train loss: 0.0000531157, val loss: 0.0000713503\n",
            "[INFO] epoch: 2987, train loss: 0.0000500512, val loss: 0.0003080256\n",
            "[INFO] epoch: 2988, train loss: 0.0000536519, val loss: 0.0000632546\n",
            "[INFO] epoch: 2989, train loss: 0.0000507496, val loss: 0.0003254636\n",
            "[INFO] epoch: 2990, train loss: 0.0000543898, val loss: 0.0000606621\n",
            "[INFO] epoch: 2991, train loss: 0.0000504881, val loss: 0.0003311177\n",
            "[INFO] epoch: 2992, train loss: 0.0000534506, val loss: 0.0000641805\n",
            "[INFO] epoch: 2993, train loss: 0.0000503297, val loss: 0.0003196839\n",
            "[INFO] epoch: 2994, train loss: 0.0000540768, val loss: 0.0000728465\n",
            "[INFO] epoch: 2995, train loss: 0.0000515592, val loss: 0.0003062891\n",
            "[INFO] epoch: 2996, train loss: 0.0000555317, val loss: 0.0000806436\n",
            "[INFO] epoch: 2997, train loss: 0.0000517551, val loss: 0.0002972282\n",
            "[INFO] epoch: 2998, train loss: 0.0000543777, val loss: 0.0000788294\n",
            "[INFO] epoch: 2999, train loss: 0.0000502929, val loss: 0.0002992174\n",
            "[INFO] epoch: 3000, train loss: 0.0000528787, val loss: 0.0000696329\n",
            "[INFO] epoch: 3001, train loss: 0.0000500377, val loss: 0.0003115143\n",
            "[INFO] epoch: 3002, train loss: 0.0000537766, val loss: 0.0000625785\n",
            "[INFO] epoch: 3003, train loss: 0.0000506484, val loss: 0.0003289224\n",
            "[INFO] epoch: 3004, train loss: 0.0000540675, val loss: 0.0000612380\n",
            "[INFO] epoch: 3005, train loss: 0.0000502409, val loss: 0.0003291882\n",
            "[INFO] epoch: 3006, train loss: 0.0000532638, val loss: 0.0000659485\n",
            "[INFO] epoch: 3007, train loss: 0.0000503703, val loss: 0.0003179495\n",
            "[INFO] epoch: 3008, train loss: 0.0000542254, val loss: 0.0000747217\n",
            "[INFO] epoch: 3009, train loss: 0.0000515445, val loss: 0.0003039563\n",
            "[INFO] epoch: 3010, train loss: 0.0000552277, val loss: 0.0000807734\n",
            "[INFO] epoch: 3011, train loss: 0.0000512659, val loss: 0.0002986095\n",
            "[INFO] epoch: 3012, train loss: 0.0000537076, val loss: 0.0000769103\n",
            "[INFO] epoch: 3013, train loss: 0.0000499256, val loss: 0.0003010258\n",
            "[INFO] epoch: 3014, train loss: 0.0000527406, val loss: 0.0000680538\n",
            "[INFO] epoch: 3015, train loss: 0.0000499751, val loss: 0.0003164507\n",
            "[INFO] epoch: 3016, train loss: 0.0000536693, val loss: 0.0000623037\n",
            "[INFO] epoch: 3017, train loss: 0.0000503505, val loss: 0.0003290836\n",
            "[INFO] epoch: 3018, train loss: 0.0000535559, val loss: 0.0000622573\n",
            "[INFO] epoch: 3019, train loss: 0.0000499010, val loss: 0.0003277126\n",
            "[INFO] epoch: 3020, train loss: 0.0000530025, val loss: 0.0000676811\n",
            "[INFO] epoch: 3021, train loss: 0.0000502886, val loss: 0.0003145028\n",
            "[INFO] epoch: 3022, train loss: 0.0000541559, val loss: 0.0000762088\n",
            "[INFO] epoch: 3023, train loss: 0.0000512818, val loss: 0.0003034517\n",
            "[INFO] epoch: 3024, train loss: 0.0000546529, val loss: 0.0000803422\n",
            "[INFO] epoch: 3025, train loss: 0.0000506508, val loss: 0.0002987602\n",
            "[INFO] epoch: 3026, train loss: 0.0000530274, val loss: 0.0000752507\n",
            "[INFO] epoch: 3027, train loss: 0.0000495526, val loss: 0.0003040652\n",
            "[INFO] epoch: 3028, train loss: 0.0000525588, val loss: 0.0000668865\n",
            "[INFO] epoch: 3029, train loss: 0.0000498144, val loss: 0.0003196595\n",
            "[INFO] epoch: 3030, train loss: 0.0000534623, val loss: 0.0000622458\n",
            "[INFO] epoch: 3031, train loss: 0.0000500633, val loss: 0.0003300144\n",
            "[INFO] epoch: 3032, train loss: 0.0000531555, val loss: 0.0000630795\n",
            "[INFO] epoch: 3033, train loss: 0.0000496462, val loss: 0.0003263940\n",
            "[INFO] epoch: 3034, train loss: 0.0000528491, val loss: 0.0000691911\n",
            "[INFO] epoch: 3035, train loss: 0.0000502825, val loss: 0.0003125552\n",
            "[INFO] epoch: 3036, train loss: 0.0000541807, val loss: 0.0000774993\n",
            "[INFO] epoch: 3037, train loss: 0.0000511256, val loss: 0.0003032541\n",
            "[INFO] epoch: 3038, train loss: 0.0000542859, val loss: 0.0000803403\n",
            "[INFO] epoch: 3039, train loss: 0.0000502738, val loss: 0.0002986704\n",
            "[INFO] epoch: 3040, train loss: 0.0000526303, val loss: 0.0000742341\n",
            "[INFO] epoch: 3041, train loss: 0.0000493287, val loss: 0.0003073196\n",
            "[INFO] epoch: 3042, train loss: 0.0000524842, val loss: 0.0000660525\n",
            "[INFO] epoch: 3043, train loss: 0.0000497714, val loss: 0.0003218637\n",
            "[INFO] epoch: 3044, train loss: 0.0000534261, val loss: 0.0000621141\n",
            "[INFO] epoch: 3045, train loss: 0.0000498832, val loss: 0.0003325098\n",
            "[INFO] epoch: 3046, train loss: 0.0000528675, val loss: 0.0000636535\n",
            "[INFO] epoch: 3047, train loss: 0.0000494766, val loss: 0.0003246682\n",
            "[INFO] epoch: 3048, train loss: 0.0000527867, val loss: 0.0000705925\n",
            "[INFO] epoch: 3049, train loss: 0.0000503023, val loss: 0.0003122162\n",
            "[INFO] epoch: 3050, train loss: 0.0000542292, val loss: 0.0000789153\n",
            "[INFO] epoch: 3051, train loss: 0.0000510075, val loss: 0.0003014057\n",
            "[INFO] epoch: 3052, train loss: 0.0000539772, val loss: 0.0000806748\n",
            "[INFO] epoch: 3053, train loss: 0.0000498988, val loss: 0.0002997578\n",
            "[INFO] epoch: 3054, train loss: 0.0000521992, val loss: 0.0000733808\n",
            "[INFO] epoch: 3055, train loss: 0.0000490605, val loss: 0.0003085558\n",
            "[INFO] epoch: 3056, train loss: 0.0000523749, val loss: 0.0000652405\n",
            "[INFO] epoch: 3057, train loss: 0.0000496669, val loss: 0.0003256096\n",
            "[INFO] epoch: 3058, train loss: 0.0000532828, val loss: 0.0000618692\n",
            "[INFO] epoch: 3059, train loss: 0.0000496064, val loss: 0.0003332788\n",
            "[INFO] epoch: 3060, train loss: 0.0000525050, val loss: 0.0000643538\n",
            "[INFO] epoch: 3061, train loss: 0.0000492691, val loss: 0.0003242424\n",
            "[INFO] epoch: 3062, train loss: 0.0000527021, val loss: 0.0000720001\n",
            "[INFO] epoch: 3063, train loss: 0.0000502807, val loss: 0.0003106470\n",
            "[INFO] epoch: 3064, train loss: 0.0000542126, val loss: 0.0000802763\n",
            "[INFO] epoch: 3065, train loss: 0.0000508249, val loss: 0.0003009228\n",
            "[INFO] epoch: 3066, train loss: 0.0000535909, val loss: 0.0000805068\n",
            "[INFO] epoch: 3067, train loss: 0.0000495137, val loss: 0.0003007599\n",
            "[INFO] epoch: 3068, train loss: 0.0000518565, val loss: 0.0000722786\n",
            "[INFO] epoch: 3069, train loss: 0.0000489375, val loss: 0.0003110734\n",
            "[INFO] epoch: 3070, train loss: 0.0000524205, val loss: 0.0000645147\n",
            "[INFO] epoch: 3071, train loss: 0.0000496011, val loss: 0.0003290326\n",
            "[INFO] epoch: 3072, train loss: 0.0000531019, val loss: 0.0000620447\n",
            "[INFO] epoch: 3073, train loss: 0.0000493951, val loss: 0.0003330375\n",
            "[INFO] epoch: 3074, train loss: 0.0000522801, val loss: 0.0000654809\n",
            "[INFO] epoch: 3075, train loss: 0.0000492189, val loss: 0.0003234038\n",
            "[INFO] epoch: 3076, train loss: 0.0000527819, val loss: 0.0000736245\n",
            "[INFO] epoch: 3077, train loss: 0.0000503422, val loss: 0.0003087540\n",
            "[INFO] epoch: 3078, train loss: 0.0000541504, val loss: 0.0000810198\n",
            "[INFO] epoch: 3079, train loss: 0.0000505426, val loss: 0.0003018635\n",
            "[INFO] epoch: 3080, train loss: 0.0000530965, val loss: 0.0000794435\n",
            "[INFO] epoch: 3081, train loss: 0.0000491899, val loss: 0.0003017460\n",
            "[INFO] epoch: 3082, train loss: 0.0000516834, val loss: 0.0000709406\n",
            "[INFO] epoch: 3083, train loss: 0.0000488913, val loss: 0.0003154094\n",
            "[INFO] epoch: 3084, train loss: 0.0000524129, val loss: 0.0000640904\n",
            "[INFO] epoch: 3085, train loss: 0.0000494340, val loss: 0.0003299055\n",
            "[INFO] epoch: 3086, train loss: 0.0000527494, val loss: 0.0000627343\n",
            "[INFO] epoch: 3087, train loss: 0.0000490884, val loss: 0.0003326051\n",
            "[INFO] epoch: 3088, train loss: 0.0000519751, val loss: 0.0000668797\n",
            "[INFO] epoch: 3089, train loss: 0.0000491142, val loss: 0.0003202376\n",
            "[INFO] epoch: 3090, train loss: 0.0000527409, val loss: 0.0000751312\n",
            "[INFO] epoch: 3091, train loss: 0.0000501778, val loss: 0.0003083751\n",
            "[INFO] epoch: 3092, train loss: 0.0000537306, val loss: 0.0000809987\n",
            "[INFO] epoch: 3093, train loss: 0.0000500112, val loss: 0.0003015476\n",
            "[INFO] epoch: 3094, train loss: 0.0000524284, val loss: 0.0000780688\n",
            "[INFO] epoch: 3095, train loss: 0.0000487760, val loss: 0.0003044411\n",
            "[INFO] epoch: 3096, train loss: 0.0000514174, val loss: 0.0000697477\n",
            "[INFO] epoch: 3097, train loss: 0.0000487061, val loss: 0.0003180764\n",
            "[INFO] epoch: 3098, train loss: 0.0000522239, val loss: 0.0000639830\n",
            "[INFO] epoch: 3099, train loss: 0.0000491568, val loss: 0.0003312491\n",
            "[INFO] epoch: 3100, train loss: 0.0000523251, val loss: 0.0000634224\n",
            "[INFO] epoch: 3101, train loss: 0.0000487677, val loss: 0.0003311277\n",
            "[INFO] epoch: 3102, train loss: 0.0000517062, val loss: 0.0000682435\n",
            "[INFO] epoch: 3103, train loss: 0.0000490260, val loss: 0.0003185603\n",
            "[INFO] epoch: 3104, train loss: 0.0000526989, val loss: 0.0000762612\n",
            "[INFO] epoch: 3105, train loss: 0.0000500127, val loss: 0.0003079603\n",
            "[INFO] epoch: 3106, train loss: 0.0000533822, val loss: 0.0000810678\n",
            "[INFO] epoch: 3107, train loss: 0.0000496353, val loss: 0.0003017376\n",
            "[INFO] epoch: 3108, train loss: 0.0000519930, val loss: 0.0000771210\n",
            "[INFO] epoch: 3109, train loss: 0.0000484994, val loss: 0.0003070929\n",
            "[INFO] epoch: 3110, train loss: 0.0000512599, val loss: 0.0000690260\n",
            "[INFO] epoch: 3111, train loss: 0.0000486294, val loss: 0.0003200926\n",
            "[INFO] epoch: 3112, train loss: 0.0000521807, val loss: 0.0000638923\n",
            "[INFO] epoch: 3113, train loss: 0.0000490019, val loss: 0.0003334851\n",
            "[INFO] epoch: 3114, train loss: 0.0000520596, val loss: 0.0000639445\n",
            "[INFO] epoch: 3115, train loss: 0.0000485793, val loss: 0.0003298112\n",
            "[INFO] epoch: 3116, train loss: 0.0000515718, val loss: 0.0000693342\n",
            "[INFO] epoch: 3117, train loss: 0.0000489819, val loss: 0.0003183678\n",
            "[INFO] epoch: 3118, train loss: 0.0000526905, val loss: 0.0000774087\n",
            "[INFO] epoch: 3119, train loss: 0.0000499181, val loss: 0.0003064391\n",
            "[INFO] epoch: 3120, train loss: 0.0000531616, val loss: 0.0000815469\n",
            "[INFO] epoch: 3121, train loss: 0.0000493248, val loss: 0.0003027024\n",
            "[INFO] epoch: 3122, train loss: 0.0000515792, val loss: 0.0000766540\n",
            "[INFO] epoch: 3123, train loss: 0.0000481977, val loss: 0.0003076244\n",
            "[INFO] epoch: 3124, train loss: 0.0000510652, val loss: 0.0000684303\n",
            "[INFO] epoch: 3125, train loss: 0.0000485047, val loss: 0.0003232717\n",
            "[INFO] epoch: 3126, train loss: 0.0000520744, val loss: 0.0000636338\n",
            "[INFO] epoch: 3127, train loss: 0.0000487888, val loss: 0.0003344628\n",
            "[INFO] epoch: 3128, train loss: 0.0000517599, val loss: 0.0000643264\n",
            "[INFO] epoch: 3129, train loss: 0.0000483305, val loss: 0.0003303071\n",
            "[INFO] epoch: 3130, train loss: 0.0000513587, val loss: 0.0000702263\n",
            "[INFO] epoch: 3131, train loss: 0.0000488694, val loss: 0.0003170454\n",
            "[INFO] epoch: 3132, train loss: 0.0000526461, val loss: 0.0000786734\n",
            "[INFO] epoch: 3133, train loss: 0.0000498147, val loss: 0.0003061473\n",
            "[INFO] epoch: 3134, train loss: 0.0000529367, val loss: 0.0000820517\n",
            "[INFO] epoch: 3135, train loss: 0.0000490010, val loss: 0.0003027081\n",
            "[INFO] epoch: 3136, train loss: 0.0000511849, val loss: 0.0000762126\n",
            "[INFO] epoch: 3137, train loss: 0.0000479562, val loss: 0.0003091161\n",
            "[INFO] epoch: 3138, train loss: 0.0000509459, val loss: 0.0000677966\n",
            "[INFO] epoch: 3139, train loss: 0.0000483998, val loss: 0.0003259899\n",
            "[INFO] epoch: 3140, train loss: 0.0000519709, val loss: 0.0000634996\n",
            "[INFO] epoch: 3141, train loss: 0.0000486124, val loss: 0.0003357297\n",
            "[INFO] epoch: 3142, train loss: 0.0000515173, val loss: 0.0000648025\n",
            "[INFO] epoch: 3143, train loss: 0.0000481508, val loss: 0.0003303463\n",
            "[INFO] epoch: 3144, train loss: 0.0000512491, val loss: 0.0000713557\n",
            "[INFO] epoch: 3145, train loss: 0.0000488686, val loss: 0.0003158121\n",
            "[INFO] epoch: 3146, train loss: 0.0000526861, val loss: 0.0000797699\n",
            "[INFO] epoch: 3147, train loss: 0.0000497058, val loss: 0.0003064829\n",
            "[INFO] epoch: 3148, train loss: 0.0000526571, val loss: 0.0000820598\n",
            "[INFO] epoch: 3149, train loss: 0.0000487178, val loss: 0.0003028555\n",
            "[INFO] epoch: 3150, train loss: 0.0000509133, val loss: 0.0000753618\n",
            "[INFO] epoch: 3151, train loss: 0.0000478242, val loss: 0.0003121146\n",
            "[INFO] epoch: 3152, train loss: 0.0000509109, val loss: 0.0000672486\n",
            "[INFO] epoch: 3153, train loss: 0.0000483282, val loss: 0.0003276259\n",
            "[INFO] epoch: 3154, train loss: 0.0000518326, val loss: 0.0000636934\n",
            "[INFO] epoch: 3155, train loss: 0.0000483949, val loss: 0.0003367752\n",
            "[INFO] epoch: 3156, train loss: 0.0000512118, val loss: 0.0000656572\n",
            "[INFO] epoch: 3157, train loss: 0.0000479937, val loss: 0.0003283543\n",
            "[INFO] epoch: 3158, train loss: 0.0000511809, val loss: 0.0000726769\n",
            "[INFO] epoch: 3159, train loss: 0.0000488164, val loss: 0.0003154037\n",
            "[INFO] epoch: 3160, train loss: 0.0000525309, val loss: 0.0000804041\n",
            "[INFO] epoch: 3161, train loss: 0.0000493919, val loss: 0.0003058640\n",
            "[INFO] epoch: 3162, train loss: 0.0000521553, val loss: 0.0000813895\n",
            "[INFO] epoch: 3163, train loss: 0.0000483270, val loss: 0.0003046848\n",
            "[INFO] epoch: 3164, train loss: 0.0000505716, val loss: 0.0000742794\n",
            "[INFO] epoch: 3165, train loss: 0.0000476281, val loss: 0.0003142421\n",
            "[INFO] epoch: 3166, train loss: 0.0000507961, val loss: 0.0000668902\n",
            "[INFO] epoch: 3167, train loss: 0.0000481614, val loss: 0.0003297423\n",
            "[INFO] epoch: 3168, train loss: 0.0000515336, val loss: 0.0000640919\n",
            "[INFO] epoch: 3169, train loss: 0.0000480806, val loss: 0.0003360107\n",
            "[INFO] epoch: 3170, train loss: 0.0000508603, val loss: 0.0000667329\n",
            "[INFO] epoch: 3171, train loss: 0.0000478354, val loss: 0.0003269406\n",
            "[INFO] epoch: 3172, train loss: 0.0000510966, val loss: 0.0000737867\n",
            "[INFO] epoch: 3173, train loss: 0.0000487013, val loss: 0.0003146488\n",
            "[INFO] epoch: 3174, train loss: 0.0000522972, val loss: 0.0000807940\n",
            "[INFO] epoch: 3175, train loss: 0.0000490782, val loss: 0.0003060293\n",
            "[INFO] epoch: 3176, train loss: 0.0000517090, val loss: 0.0000806579\n",
            "[INFO] epoch: 3177, train loss: 0.0000479820, val loss: 0.0003065200\n",
            "[INFO] epoch: 3178, train loss: 0.0000502910, val loss: 0.0000734976\n",
            "[INFO] epoch: 3179, train loss: 0.0000474890, val loss: 0.0003162126\n",
            "[INFO] epoch: 3180, train loss: 0.0000507252, val loss: 0.0000666902\n",
            "[INFO] epoch: 3181, train loss: 0.0000480127, val loss: 0.0003317762\n",
            "[INFO] epoch: 3182, train loss: 0.0000512698, val loss: 0.0000645167\n",
            "[INFO] epoch: 3183, train loss: 0.0000478418, val loss: 0.0003353160\n",
            "[INFO] epoch: 3184, train loss: 0.0000506144, val loss: 0.0000676004\n",
            "[INFO] epoch: 3185, train loss: 0.0000476960, val loss: 0.0003264215\n",
            "[INFO] epoch: 3186, train loss: 0.0000509875, val loss: 0.0000747398\n",
            "[INFO] epoch: 3187, train loss: 0.0000485819, val loss: 0.0003135690\n",
            "[INFO] epoch: 3188, train loss: 0.0000521078, val loss: 0.0000812985\n",
            "[INFO] epoch: 3189, train loss: 0.0000488011, val loss: 0.0003066864\n",
            "[INFO] epoch: 3190, train loss: 0.0000513012, val loss: 0.0000804142\n",
            "[INFO] epoch: 3191, train loss: 0.0000476420, val loss: 0.0003069055\n",
            "[INFO] epoch: 3192, train loss: 0.0000500000, val loss: 0.0000730128\n",
            "[INFO] epoch: 3193, train loss: 0.0000473090, val loss: 0.0003186002\n",
            "[INFO] epoch: 3194, train loss: 0.0000505885, val loss: 0.0000665124\n",
            "[INFO] epoch: 3195, train loss: 0.0000478353, val loss: 0.0003329409\n",
            "[INFO] epoch: 3196, train loss: 0.0000510468, val loss: 0.0000647343\n",
            "[INFO] epoch: 3197, train loss: 0.0000476147, val loss: 0.0003361712\n",
            "[INFO] epoch: 3198, train loss: 0.0000503562, val loss: 0.0000681579\n",
            "[INFO] epoch: 3199, train loss: 0.0000475149, val loss: 0.0003257741\n",
            "[INFO] epoch: 3200, train loss: 0.0000508667, val loss: 0.0000756444\n",
            "[INFO] epoch: 3201, train loss: 0.0000484974, val loss: 0.0003134116\n",
            "[INFO] epoch: 3202, train loss: 0.0000519939, val loss: 0.0000820346\n",
            "[INFO] epoch: 3203, train loss: 0.0000485900, val loss: 0.0003065723\n",
            "[INFO] epoch: 3204, train loss: 0.0000509959, val loss: 0.0000804985\n",
            "[INFO] epoch: 3205, train loss: 0.0000473908, val loss: 0.0003076079\n",
            "[INFO] epoch: 3206, train loss: 0.0000497739, val loss: 0.0000727144\n",
            "[INFO] epoch: 3207, train loss: 0.0000471454, val loss: 0.0003203779\n",
            "[INFO] epoch: 3208, train loss: 0.0000504882, val loss: 0.0000662893\n",
            "[INFO] epoch: 3209, train loss: 0.0000477272, val loss: 0.0003346864\n",
            "[INFO] epoch: 3210, train loss: 0.0000509098, val loss: 0.0000648392\n",
            "[INFO] epoch: 3211, train loss: 0.0000474264, val loss: 0.0003371999\n",
            "[INFO] epoch: 3212, train loss: 0.0000501425, val loss: 0.0000687027\n",
            "[INFO] epoch: 3213, train loss: 0.0000474056, val loss: 0.0003253233\n",
            "[INFO] epoch: 3214, train loss: 0.0000508388, val loss: 0.0000765714\n",
            "[INFO] epoch: 3215, train loss: 0.0000484603, val loss: 0.0003135579\n",
            "[INFO] epoch: 3216, train loss: 0.0000519162, val loss: 0.0000827294\n",
            "[INFO] epoch: 3217, train loss: 0.0000484336, val loss: 0.0003062342\n",
            "[INFO] epoch: 3218, train loss: 0.0000507599, val loss: 0.0000804508\n",
            "[INFO] epoch: 3219, train loss: 0.0000471798, val loss: 0.0003090854\n",
            "[INFO] epoch: 3220, train loss: 0.0000495880, val loss: 0.0000723231\n",
            "[INFO] epoch: 3221, train loss: 0.0000470341, val loss: 0.0003216729\n",
            "[INFO] epoch: 3222, train loss: 0.0000504325, val loss: 0.0000661431\n",
            "[INFO] epoch: 3223, train loss: 0.0000476046, val loss: 0.0003366914\n",
            "[INFO] epoch: 3224, train loss: 0.0000506949, val loss: 0.0000651225\n",
            "[INFO] epoch: 3225, train loss: 0.0000472181, val loss: 0.0003369222\n",
            "[INFO] epoch: 3226, train loss: 0.0000499444, val loss: 0.0000695071\n",
            "[INFO] epoch: 3227, train loss: 0.0000472990, val loss: 0.0003250206\n",
            "[INFO] epoch: 3228, train loss: 0.0000507490, val loss: 0.0000774308\n",
            "[INFO] epoch: 3229, train loss: 0.0000483124, val loss: 0.0003129903\n",
            "[INFO] epoch: 3230, train loss: 0.0000516643, val loss: 0.0000829723\n",
            "[INFO] epoch: 3231, train loss: 0.0000481393, val loss: 0.0003069346\n",
            "[INFO] epoch: 3232, train loss: 0.0000503795, val loss: 0.0000799182\n",
            "[INFO] epoch: 3233, train loss: 0.0000469039, val loss: 0.0003103467\n",
            "[INFO] epoch: 3234, train loss: 0.0000493949, val loss: 0.0000717897\n",
            "[INFO] epoch: 3235, train loss: 0.0000469134, val loss: 0.0003237644\n",
            "[INFO] epoch: 3236, train loss: 0.0000502949, val loss: 0.0000661950\n",
            "[INFO] epoch: 3237, train loss: 0.0000473806, val loss: 0.0003374847\n",
            "[INFO] epoch: 3238, train loss: 0.0000503750, val loss: 0.0000657073\n",
            "[INFO] epoch: 3239, train loss: 0.0000470002, val loss: 0.0003360844\n",
            "[INFO] epoch: 3240, train loss: 0.0000497595, val loss: 0.0000704354\n",
            "[INFO] epoch: 3241, train loss: 0.0000471887, val loss: 0.0003245144\n",
            "[INFO] epoch: 3242, train loss: 0.0000506387, val loss: 0.0000781312\n",
            "[INFO] epoch: 3243, train loss: 0.0000481492, val loss: 0.0003126772\n",
            "[INFO] epoch: 3244, train loss: 0.0000513656, val loss: 0.0000828594\n",
            "[INFO] epoch: 3245, train loss: 0.0000478160, val loss: 0.0003082641\n",
            "[INFO] epoch: 3246, train loss: 0.0000500181, val loss: 0.0000791678\n",
            "[INFO] epoch: 3247, train loss: 0.0000467071, val loss: 0.0003117159\n",
            "[INFO] epoch: 3248, train loss: 0.0000492913, val loss: 0.0000714240\n",
            "[INFO] epoch: 3249, train loss: 0.0000468042, val loss: 0.0003261279\n",
            "[INFO] epoch: 3250, train loss: 0.0000501276, val loss: 0.0000663720\n",
            "[INFO] epoch: 3251, train loss: 0.0000471772, val loss: 0.0003374470\n",
            "[INFO] epoch: 3252, train loss: 0.0000500996, val loss: 0.0000663835\n",
            "[INFO] epoch: 3253, train loss: 0.0000467986, val loss: 0.0003357996\n",
            "[INFO] epoch: 3254, train loss: 0.0000495689, val loss: 0.0000712396\n",
            "[INFO] epoch: 3255, train loss: 0.0000470681, val loss: 0.0003233618\n",
            "[INFO] epoch: 3256, train loss: 0.0000505101, val loss: 0.0000787711\n",
            "[INFO] epoch: 3257, train loss: 0.0000479495, val loss: 0.0003132224\n",
            "[INFO] epoch: 3258, train loss: 0.0000510324, val loss: 0.0000827890\n",
            "[INFO] epoch: 3259, train loss: 0.0000474896, val loss: 0.0003084444\n",
            "[INFO] epoch: 3260, train loss: 0.0000496730, val loss: 0.0000787335\n",
            "[INFO] epoch: 3261, train loss: 0.0000464742, val loss: 0.0003136237\n",
            "[INFO] epoch: 3262, train loss: 0.0000490981, val loss: 0.0000712313\n",
            "[INFO] epoch: 3263, train loss: 0.0000466317, val loss: 0.0003272097\n",
            "[INFO] epoch: 3264, train loss: 0.0000499469, val loss: 0.0000665332\n",
            "[INFO] epoch: 3265, train loss: 0.0000469807, val loss: 0.0003384447\n",
            "[INFO] epoch: 3266, train loss: 0.0000498385, val loss: 0.0000668024\n",
            "[INFO] epoch: 3267, train loss: 0.0000465749, val loss: 0.0003355376\n",
            "[INFO] epoch: 3268, train loss: 0.0000493594, val loss: 0.0000718743\n",
            "[INFO] epoch: 3269, train loss: 0.0000469277, val loss: 0.0003231431\n",
            "[INFO] epoch: 3270, train loss: 0.0000503671, val loss: 0.0000793832\n",
            "[INFO] epoch: 3271, train loss: 0.0000477657, val loss: 0.0003133461\n",
            "[INFO] epoch: 3272, train loss: 0.0000507859, val loss: 0.0000830512\n",
            "[INFO] epoch: 3273, train loss: 0.0000472531, val loss: 0.0003086884\n",
            "[INFO] epoch: 3274, train loss: 0.0000493951, val loss: 0.0000787591\n",
            "[INFO] epoch: 3275, train loss: 0.0000462313, val loss: 0.0003148069\n",
            "[INFO] epoch: 3276, train loss: 0.0000488842, val loss: 0.0000710860\n",
            "[INFO] epoch: 3277, train loss: 0.0000464890, val loss: 0.0003282869\n",
            "[INFO] epoch: 3278, train loss: 0.0000498299, val loss: 0.0000666050\n",
            "[INFO] epoch: 3279, train loss: 0.0000468130, val loss: 0.0003400873\n",
            "[INFO] epoch: 3280, train loss: 0.0000496249, val loss: 0.0000669781\n",
            "[INFO] epoch: 3281, train loss: 0.0000463976, val loss: 0.0003355182\n",
            "[INFO] epoch: 3282, train loss: 0.0000491997, val loss: 0.0000724218\n",
            "[INFO] epoch: 3283, train loss: 0.0000468096, val loss: 0.0003238728\n",
            "[INFO] epoch: 3284, train loss: 0.0000502782, val loss: 0.0000799734\n",
            "[INFO] epoch: 3285, train loss: 0.0000476866, val loss: 0.0003127880\n",
            "[INFO] epoch: 3286, train loss: 0.0000506853, val loss: 0.0000836411\n",
            "[INFO] epoch: 3287, train loss: 0.0000470888, val loss: 0.0003097069\n",
            "[INFO] epoch: 3288, train loss: 0.0000491694, val loss: 0.0000787830\n",
            "[INFO] epoch: 3289, train loss: 0.0000460561, val loss: 0.0003149475\n",
            "[INFO] epoch: 3290, train loss: 0.0000487558, val loss: 0.0000710522\n",
            "[INFO] epoch: 3291, train loss: 0.0000463843, val loss: 0.0003303459\n",
            "[INFO] epoch: 3292, train loss: 0.0000497242, val loss: 0.0000665531\n",
            "[INFO] epoch: 3293, train loss: 0.0000466748, val loss: 0.0003407770\n",
            "[INFO] epoch: 3294, train loss: 0.0000494586, val loss: 0.0000673173\n",
            "[INFO] epoch: 3295, train loss: 0.0000462397, val loss: 0.0003363301\n",
            "[INFO] epoch: 3296, train loss: 0.0000490319, val loss: 0.0000728789\n",
            "[INFO] epoch: 3297, train loss: 0.0000466835, val loss: 0.0003235143\n",
            "[INFO] epoch: 3298, train loss: 0.0000501731, val loss: 0.0000806617\n",
            "[INFO] epoch: 3299, train loss: 0.0000475545, val loss: 0.0003131639\n",
            "[INFO] epoch: 3300, train loss: 0.0000504729, val loss: 0.0000838634\n",
            "[INFO] epoch: 3301, train loss: 0.0000468477, val loss: 0.0003099865\n",
            "[INFO] epoch: 3302, train loss: 0.0000489025, val loss: 0.0000786281\n",
            "[INFO] epoch: 3303, train loss: 0.0000458762, val loss: 0.0003161588\n",
            "[INFO] epoch: 3304, train loss: 0.0000486073, val loss: 0.0000709191\n",
            "[INFO] epoch: 3305, train loss: 0.0000462260, val loss: 0.0003316942\n",
            "[INFO] epoch: 3306, train loss: 0.0000495376, val loss: 0.0000667309\n",
            "[INFO] epoch: 3307, train loss: 0.0000464853, val loss: 0.0003410965\n",
            "[INFO] epoch: 3308, train loss: 0.0000492219, val loss: 0.0000677853\n",
            "[INFO] epoch: 3309, train loss: 0.0000460472, val loss: 0.0003366199\n",
            "[INFO] epoch: 3310, train loss: 0.0000488599, val loss: 0.0000735233\n",
            "[INFO] epoch: 3311, train loss: 0.0000465675, val loss: 0.0003227940\n",
            "[INFO] epoch: 3312, train loss: 0.0000500259, val loss: 0.0000811015\n",
            "[INFO] epoch: 3313, train loss: 0.0000473390, val loss: 0.0003142744\n",
            "[INFO] epoch: 3314, train loss: 0.0000501650, val loss: 0.0000836362\n",
            "[INFO] epoch: 3315, train loss: 0.0000466101, val loss: 0.0003102010\n",
            "[INFO] epoch: 3316, train loss: 0.0000486884, val loss: 0.0000782973\n",
            "[INFO] epoch: 3317, train loss: 0.0000457227, val loss: 0.0003185624\n",
            "[INFO] epoch: 3318, train loss: 0.0000484746, val loss: 0.0000708249\n",
            "[INFO] epoch: 3319, train loss: 0.0000461021, val loss: 0.0003319282\n",
            "[INFO] epoch: 3320, train loss: 0.0000493687, val loss: 0.0000671480\n",
            "[INFO] epoch: 3321, train loss: 0.0000462905, val loss: 0.0003422436\n",
            "[INFO] epoch: 3322, train loss: 0.0000489751, val loss: 0.0000683292\n",
            "[INFO] epoch: 3323, train loss: 0.0000459045, val loss: 0.0003349634\n",
            "[INFO] epoch: 3324, train loss: 0.0000487497, val loss: 0.0000742535\n",
            "[INFO] epoch: 3325, train loss: 0.0000464537, val loss: 0.0003239769\n",
            "[INFO] epoch: 3326, train loss: 0.0000498450, val loss: 0.0000812695\n",
            "[INFO] epoch: 3327, train loss: 0.0000471192, val loss: 0.0003135670\n",
            "[INFO] epoch: 3328, train loss: 0.0000498684, val loss: 0.0000834634\n",
            "[INFO] epoch: 3329, train loss: 0.0000463664, val loss: 0.0003124276\n",
            "[INFO] epoch: 3330, train loss: 0.0000484473, val loss: 0.0000779505\n",
            "[INFO] epoch: 3331, train loss: 0.0000455566, val loss: 0.0003188158\n",
            "[INFO] epoch: 3332, train loss: 0.0000483432, val loss: 0.0000709587\n",
            "[INFO] epoch: 3333, train loss: 0.0000459621, val loss: 0.0003339514\n",
            "[INFO] epoch: 3334, train loss: 0.0000491567, val loss: 0.0000674106\n",
            "[INFO] epoch: 3335, train loss: 0.0000460753, val loss: 0.0003413333\n",
            "[INFO] epoch: 3336, train loss: 0.0000487294, val loss: 0.0000689534\n",
            "[INFO] epoch: 3337, train loss: 0.0000457215, val loss: 0.0003356197\n",
            "[INFO] epoch: 3338, train loss: 0.0000485554, val loss: 0.0000746333\n",
            "[INFO] epoch: 3339, train loss: 0.0000462583, val loss: 0.0003232334\n",
            "[INFO] epoch: 3340, train loss: 0.0000496024, val loss: 0.0000816440\n",
            "[INFO] epoch: 3341, train loss: 0.0000468790, val loss: 0.0003145023\n",
            "[INFO] epoch: 3342, train loss: 0.0000495611, val loss: 0.0000834371\n",
            "[INFO] epoch: 3343, train loss: 0.0000460724, val loss: 0.0003126979\n",
            "[INFO] epoch: 3344, train loss: 0.0000481357, val loss: 0.0000780032\n",
            "[INFO] epoch: 3345, train loss: 0.0000453209, val loss: 0.0003198210\n",
            "[INFO] epoch: 3346, train loss: 0.0000481096, val loss: 0.0000710272\n",
            "[INFO] epoch: 3347, train loss: 0.0000457261, val loss: 0.0003347711\n",
            "[INFO] epoch: 3348, train loss: 0.0000488928, val loss: 0.0000676538\n",
            "[INFO] epoch: 3349, train loss: 0.0000458475, val loss: 0.0003418723\n",
            "[INFO] epoch: 3350, train loss: 0.0000484861, val loss: 0.0000691795\n",
            "[INFO] epoch: 3351, train loss: 0.0000454924, val loss: 0.0003362785\n",
            "[INFO] epoch: 3352, train loss: 0.0000483166, val loss: 0.0000749912\n",
            "[INFO] epoch: 3353, train loss: 0.0000460848, val loss: 0.0003232975\n",
            "[INFO] epoch: 3354, train loss: 0.0000494519, val loss: 0.0000820463\n",
            "[INFO] epoch: 3355, train loss: 0.0000467271, val loss: 0.0003154284\n",
            "[INFO] epoch: 3356, train loss: 0.0000493902, val loss: 0.0000838356\n",
            "[INFO] epoch: 3357, train loss: 0.0000459139, val loss: 0.0003125776\n",
            "[INFO] epoch: 3358, train loss: 0.0000479667, val loss: 0.0000782874\n",
            "[INFO] epoch: 3359, train loss: 0.0000451623, val loss: 0.0003212081\n",
            "[INFO] epoch: 3360, train loss: 0.0000479485, val loss: 0.0000711450\n",
            "[INFO] epoch: 3361, train loss: 0.0000456081, val loss: 0.0003351290\n",
            "[INFO] epoch: 3362, train loss: 0.0000488119, val loss: 0.0000678090\n",
            "[INFO] epoch: 3363, train loss: 0.0000457475, val loss: 0.0003437769\n",
            "[INFO] epoch: 3364, train loss: 0.0000483548, val loss: 0.0000692727\n",
            "[INFO] epoch: 3365, train loss: 0.0000453563, val loss: 0.0003364614\n",
            "[INFO] epoch: 3366, train loss: 0.0000481844, val loss: 0.0000753707\n",
            "[INFO] epoch: 3367, train loss: 0.0000459788, val loss: 0.0003244673\n",
            "[INFO] epoch: 3368, train loss: 0.0000493517, val loss: 0.0000824407\n",
            "[INFO] epoch: 3369, train loss: 0.0000466226, val loss: 0.0003152041\n",
            "[INFO] epoch: 3370, train loss: 0.0000492744, val loss: 0.0000843294\n",
            "[INFO] epoch: 3371, train loss: 0.0000457844, val loss: 0.0003134838\n",
            "[INFO] epoch: 3372, train loss: 0.0000477969, val loss: 0.0000784712\n",
            "[INFO] epoch: 3373, train loss: 0.0000449984, val loss: 0.0003214389\n",
            "[INFO] epoch: 3374, train loss: 0.0000477974, val loss: 0.0000713143\n",
            "[INFO] epoch: 3375, train loss: 0.0000454930, val loss: 0.0003363780\n",
            "[INFO] epoch: 3376, train loss: 0.0000486931, val loss: 0.0000679111\n",
            "[INFO] epoch: 3377, train loss: 0.0000456020, val loss: 0.0003444136\n",
            "[INFO] epoch: 3378, train loss: 0.0000481859, val loss: 0.0000695975\n",
            "[INFO] epoch: 3379, train loss: 0.0000452177, val loss: 0.0003371107\n",
            "[INFO] epoch: 3380, train loss: 0.0000480417, val loss: 0.0000756716\n",
            "[INFO] epoch: 3381, train loss: 0.0000458366, val loss: 0.0003248739\n",
            "[INFO] epoch: 3382, train loss: 0.0000491893, val loss: 0.0000827730\n",
            "[INFO] epoch: 3383, train loss: 0.0000464739, val loss: 0.0003156311\n",
            "[INFO] epoch: 3384, train loss: 0.0000490946, val loss: 0.0000843885\n",
            "[INFO] epoch: 3385, train loss: 0.0000456051, val loss: 0.0003145116\n",
            "[INFO] epoch: 3386, train loss: 0.0000476058, val loss: 0.0000785060\n",
            "[INFO] epoch: 3387, train loss: 0.0000448592, val loss: 0.0003219941\n",
            "[INFO] epoch: 3388, train loss: 0.0000476637, val loss: 0.0000714613\n",
            "[INFO] epoch: 3389, train loss: 0.0000453343, val loss: 0.0003375204\n",
            "[INFO] epoch: 3390, train loss: 0.0000484705, val loss: 0.0000682514\n",
            "[INFO] epoch: 3391, train loss: 0.0000454115, val loss: 0.0003441138\n",
            "[INFO] epoch: 3392, train loss: 0.0000479854, val loss: 0.0000700697\n",
            "[INFO] epoch: 3393, train loss: 0.0000450539, val loss: 0.0003376441\n",
            "[INFO] epoch: 3394, train loss: 0.0000478539, val loss: 0.0000760273\n",
            "[INFO] epoch: 3395, train loss: 0.0000456502, val loss: 0.0003246963\n",
            "[INFO] epoch: 3396, train loss: 0.0000489478, val loss: 0.0000828817\n",
            "[INFO] epoch: 3397, train loss: 0.0000462223, val loss: 0.0003169030\n",
            "[INFO] epoch: 3398, train loss: 0.0000487832, val loss: 0.0000841871\n",
            "[INFO] epoch: 3399, train loss: 0.0000453632, val loss: 0.0003149631\n",
            "[INFO] epoch: 3400, train loss: 0.0000473908, val loss: 0.0000784537\n",
            "[INFO] epoch: 3401, train loss: 0.0000446944, val loss: 0.0003235886\n",
            "[INFO] epoch: 3402, train loss: 0.0000474693, val loss: 0.0000717015\n",
            "[INFO] epoch: 3403, train loss: 0.0000451229, val loss: 0.0003375167\n",
            "[INFO] epoch: 3404, train loss: 0.0000482118, val loss: 0.0000687215\n",
            "[INFO] epoch: 3405, train loss: 0.0000452054, val loss: 0.0003444868\n",
            "[INFO] epoch: 3406, train loss: 0.0000477625, val loss: 0.0000704881\n",
            "[INFO] epoch: 3407, train loss: 0.0000448705, val loss: 0.0003374899\n",
            "[INFO] epoch: 3408, train loss: 0.0000476600, val loss: 0.0000763679\n",
            "[INFO] epoch: 3409, train loss: 0.0000454719, val loss: 0.0003254191\n",
            "[INFO] epoch: 3410, train loss: 0.0000487172, val loss: 0.0000829159\n",
            "[INFO] epoch: 3411, train loss: 0.0000459915, val loss: 0.0003177018\n",
            "[INFO] epoch: 3412, train loss: 0.0000485292, val loss: 0.0000841265\n",
            "[INFO] epoch: 3413, train loss: 0.0000451906, val loss: 0.0003158591\n",
            "[INFO] epoch: 3414, train loss: 0.0000472312, val loss: 0.0000786368\n",
            "[INFO] epoch: 3415, train loss: 0.0000445320, val loss: 0.0003246292\n",
            "[INFO] epoch: 3416, train loss: 0.0000472799, val loss: 0.0000719895\n",
            "[INFO] epoch: 3417, train loss: 0.0000449669, val loss: 0.0003377632\n",
            "[INFO] epoch: 3418, train loss: 0.0000480421, val loss: 0.0000691015\n",
            "[INFO] epoch: 3419, train loss: 0.0000450393, val loss: 0.0003452924\n",
            "[INFO] epoch: 3420, train loss: 0.0000475719, val loss: 0.0000707238\n",
            "[INFO] epoch: 3421, train loss: 0.0000447150, val loss: 0.0003376155\n",
            "[INFO] epoch: 3422, train loss: 0.0000474947, val loss: 0.0000766160\n",
            "[INFO] epoch: 3423, train loss: 0.0000453011, val loss: 0.0003266837\n",
            "[INFO] epoch: 3424, train loss: 0.0000485214, val loss: 0.0000830175\n",
            "[INFO] epoch: 3425, train loss: 0.0000458390, val loss: 0.0003177539\n",
            "[INFO] epoch: 3426, train loss: 0.0000483848, val loss: 0.0000845350\n",
            "[INFO] epoch: 3427, train loss: 0.0000450353, val loss: 0.0003170800\n",
            "[INFO] epoch: 3428, train loss: 0.0000470348, val loss: 0.0000789644\n",
            "[INFO] epoch: 3429, train loss: 0.0000443468, val loss: 0.0003243177\n",
            "[INFO] epoch: 3430, train loss: 0.0000470845, val loss: 0.0000724029\n",
            "[INFO] epoch: 3431, train loss: 0.0000447967, val loss: 0.0003389636\n",
            "[INFO] epoch: 3432, train loss: 0.0000478613, val loss: 0.0000692120\n",
            "[INFO] epoch: 3433, train loss: 0.0000448761, val loss: 0.0003456561\n",
            "[INFO] epoch: 3434, train loss: 0.0000474048, val loss: 0.0000709245\n",
            "[INFO] epoch: 3435, train loss: 0.0000445295, val loss: 0.0003391089\n",
            "[INFO] epoch: 3436, train loss: 0.0000472621, val loss: 0.0000766075\n",
            "[INFO] epoch: 3437, train loss: 0.0000450859, val loss: 0.0003270799\n",
            "[INFO] epoch: 3438, train loss: 0.0000483175, val loss: 0.0000833763\n",
            "[INFO] epoch: 3439, train loss: 0.0000456879, val loss: 0.0003186379\n",
            "[INFO] epoch: 3440, train loss: 0.0000482421, val loss: 0.0000850085\n",
            "[INFO] epoch: 3441, train loss: 0.0000448725, val loss: 0.0003171594\n",
            "[INFO] epoch: 3442, train loss: 0.0000468367, val loss: 0.0000795191\n",
            "[INFO] epoch: 3443, train loss: 0.0000441647, val loss: 0.0003246733\n",
            "[INFO] epoch: 3444, train loss: 0.0000468807, val loss: 0.0000727140\n",
            "[INFO] epoch: 3445, train loss: 0.0000446193, val loss: 0.0003395484\n",
            "[INFO] epoch: 3446, train loss: 0.0000477066, val loss: 0.0000694082\n",
            "[INFO] epoch: 3447, train loss: 0.0000447605, val loss: 0.0003466079\n",
            "[INFO] epoch: 3448, train loss: 0.0000472922, val loss: 0.0000709801\n",
            "[INFO] epoch: 3449, train loss: 0.0000443775, val loss: 0.0003407247\n",
            "[INFO] epoch: 3450, train loss: 0.0000470758, val loss: 0.0000766797\n",
            "[INFO] epoch: 3451, train loss: 0.0000449366, val loss: 0.0003275704\n",
            "[INFO] epoch: 3452, train loss: 0.0000481837, val loss: 0.0000836164\n",
            "[INFO] epoch: 3453, train loss: 0.0000455733, val loss: 0.0003200554\n",
            "[INFO] epoch: 3454, train loss: 0.0000481388, val loss: 0.0000853400\n",
            "[INFO] epoch: 3455, train loss: 0.0000447906, val loss: 0.0003170218\n",
            "[INFO] epoch: 3456, train loss: 0.0000467485, val loss: 0.0000800045\n",
            "[INFO] epoch: 3457, train loss: 0.0000440399, val loss: 0.0003259624\n",
            "[INFO] epoch: 3458, train loss: 0.0000467085, val loss: 0.0000730086\n",
            "[INFO] epoch: 3459, train loss: 0.0000444848, val loss: 0.0003390909\n",
            "[INFO] epoch: 3460, train loss: 0.0000475853, val loss: 0.0000697838\n",
            "[INFO] epoch: 3461, train loss: 0.0000446451, val loss: 0.0003483212\n",
            "[INFO] epoch: 3462, train loss: 0.0000471546, val loss: 0.0000710962\n",
            "[INFO] epoch: 3463, train loss: 0.0000442516, val loss: 0.0003404900\n",
            "[INFO] epoch: 3464, train loss: 0.0000469331, val loss: 0.0000769231\n",
            "[INFO] epoch: 3465, train loss: 0.0000447775, val loss: 0.0003296396\n",
            "[INFO] epoch: 3466, train loss: 0.0000479792, val loss: 0.0000835244\n",
            "[INFO] epoch: 3467, train loss: 0.0000454009, val loss: 0.0003198220\n",
            "[INFO] epoch: 3468, train loss: 0.0000479722, val loss: 0.0000855224\n",
            "[INFO] epoch: 3469, train loss: 0.0000446624, val loss: 0.0003190648\n",
            "[INFO] epoch: 3470, train loss: 0.0000466101, val loss: 0.0000801595\n",
            "[INFO] epoch: 3471, train loss: 0.0000438988, val loss: 0.0003254351\n",
            "[INFO] epoch: 3472, train loss: 0.0000465390, val loss: 0.0000735275\n",
            "[INFO] epoch: 3473, train loss: 0.0000443219, val loss: 0.0003404502\n",
            "[INFO] epoch: 3474, train loss: 0.0000473725, val loss: 0.0000700860\n",
            "[INFO] epoch: 3475, train loss: 0.0000444704, val loss: 0.0003474025\n",
            "[INFO] epoch: 3476, train loss: 0.0000469828, val loss: 0.0000715742\n",
            "[INFO] epoch: 3477, train loss: 0.0000441108, val loss: 0.0003420567\n",
            "[INFO] epoch: 3478, train loss: 0.0000467630, val loss: 0.0000769158\n",
            "[INFO] epoch: 3479, train loss: 0.0000445868, val loss: 0.0003296820\n",
            "[INFO] epoch: 3480, train loss: 0.0000477338, val loss: 0.0000835436\n",
            "[INFO] epoch: 3481, train loss: 0.0000452051, val loss: 0.0003215754\n",
            "[INFO] epoch: 3482, train loss: 0.0000477721, val loss: 0.0000853939\n",
            "[INFO] epoch: 3483, train loss: 0.0000444974, val loss: 0.0003195918\n",
            "[INFO] epoch: 3484, train loss: 0.0000464419, val loss: 0.0000805168\n",
            "[INFO] epoch: 3485, train loss: 0.0000437493, val loss: 0.0003262759\n",
            "[INFO] epoch: 3486, train loss: 0.0000463370, val loss: 0.0000739489\n",
            "[INFO] epoch: 3487, train loss: 0.0000440936, val loss: 0.0003403620\n",
            "[INFO] epoch: 3488, train loss: 0.0000470808, val loss: 0.0000705910\n",
            "[INFO] epoch: 3489, train loss: 0.0000442473, val loss: 0.0003475281\n",
            "[INFO] epoch: 3490, train loss: 0.0000467562, val loss: 0.0000718190\n",
            "[INFO] epoch: 3491, train loss: 0.0000438819, val loss: 0.0003427486\n",
            "[INFO] epoch: 3492, train loss: 0.0000464690, val loss: 0.0000769793\n",
            "[INFO] epoch: 3493, train loss: 0.0000442998, val loss: 0.0003303982\n",
            "[INFO] epoch: 3494, train loss: 0.0000474011, val loss: 0.0000834756\n",
            "[INFO] epoch: 3495, train loss: 0.0000449161, val loss: 0.0003229249\n",
            "[INFO] epoch: 3496, train loss: 0.0000474870, val loss: 0.0000855529\n",
            "[INFO] epoch: 3497, train loss: 0.0000442857, val loss: 0.0003197425\n",
            "[INFO] epoch: 3498, train loss: 0.0000462363, val loss: 0.0000810740\n",
            "[INFO] epoch: 3499, train loss: 0.0000435320, val loss: 0.0003272894\n",
            "[INFO] epoch: 3500, train loss: 0.0000460570, val loss: 0.0000744609\n",
            "[INFO] epoch: 3501, train loss: 0.0000438621, val loss: 0.0003397291\n",
            "[INFO] epoch: 3502, train loss: 0.0000468692, val loss: 0.0000710351\n",
            "[INFO] epoch: 3503, train loss: 0.0000440982, val loss: 0.0003490843\n",
            "[INFO] epoch: 3504, train loss: 0.0000466194, val loss: 0.0000718289\n",
            "[INFO] epoch: 3505, train loss: 0.0000437383, val loss: 0.0003434421\n",
            "[INFO] epoch: 3506, train loss: 0.0000462994, val loss: 0.0000769877\n",
            "[INFO] epoch: 3507, train loss: 0.0000441320, val loss: 0.0003326328\n",
            "[INFO] epoch: 3508, train loss: 0.0000472311, val loss: 0.0000833961\n",
            "[INFO] epoch: 3509, train loss: 0.0000448149, val loss: 0.0003234044\n",
            "[INFO] epoch: 3510, train loss: 0.0000474485, val loss: 0.0000861301\n",
            "[INFO] epoch: 3511, train loss: 0.0000442656, val loss: 0.0003210001\n",
            "[INFO] epoch: 3512, train loss: 0.0000462013, val loss: 0.0000817554\n",
            "[INFO] epoch: 3513, train loss: 0.0000434333, val loss: 0.0003269231\n",
            "[INFO] epoch: 3514, train loss: 0.0000459035, val loss: 0.0000751448\n",
            "[INFO] epoch: 3515, train loss: 0.0000437496, val loss: 0.0003403285\n",
            "[INFO] epoch: 3516, train loss: 0.0000467825, val loss: 0.0000712226\n",
            "[INFO] epoch: 3517, train loss: 0.0000440337, val loss: 0.0003499105\n",
            "[INFO] epoch: 3518, train loss: 0.0000465658, val loss: 0.0000718947\n",
            "[INFO] epoch: 3519, train loss: 0.0000436416, val loss: 0.0003452885\n",
            "[INFO] epoch: 3520, train loss: 0.0000461543, val loss: 0.0000767685\n",
            "[INFO] epoch: 3521, train loss: 0.0000439540, val loss: 0.0003340416\n",
            "[INFO] epoch: 3522, train loss: 0.0000470339, val loss: 0.0000834859\n",
            "[INFO] epoch: 3523, train loss: 0.0000446964, val loss: 0.0003242118\n",
            "[INFO] epoch: 3524, train loss: 0.0000473892, val loss: 0.0000866027\n",
            "[INFO] epoch: 3525, train loss: 0.0000441911, val loss: 0.0003217440\n",
            "[INFO] epoch: 3526, train loss: 0.0000460899, val loss: 0.0000825461\n",
            "[INFO] epoch: 3527, train loss: 0.0000432852, val loss: 0.0003263651\n",
            "[INFO] epoch: 3528, train loss: 0.0000457001, val loss: 0.0000757412\n",
            "[INFO] epoch: 3529, train loss: 0.0000435598, val loss: 0.0003408898\n",
            "[INFO] epoch: 3530, train loss: 0.0000465804, val loss: 0.0000715307\n",
            "[INFO] epoch: 3531, train loss: 0.0000439001, val loss: 0.0003500427\n",
            "[INFO] epoch: 3532, train loss: 0.0000464672, val loss: 0.0000719946\n",
            "[INFO] epoch: 3533, train loss: 0.0000435059, val loss: 0.0003474746\n",
            "[INFO] epoch: 3534, train loss: 0.0000459582, val loss: 0.0000766228\n",
            "[INFO] epoch: 3535, train loss: 0.0000437432, val loss: 0.0003347166\n",
            "[INFO] epoch: 3536, train loss: 0.0000467951, val loss: 0.0000834354\n",
            "[INFO] epoch: 3537, train loss: 0.0000445166, val loss: 0.0003261804\n",
            "[INFO] epoch: 3538, train loss: 0.0000472422, val loss: 0.0000867736\n",
            "[INFO] epoch: 3539, train loss: 0.0000440921, val loss: 0.0003217411\n",
            "[INFO] epoch: 3540, train loss: 0.0000460021, val loss: 0.0000831602\n",
            "[INFO] epoch: 3541, train loss: 0.0000431695, val loss: 0.0003274859\n",
            "[INFO] epoch: 3542, train loss: 0.0000455073, val loss: 0.0000763015\n",
            "[INFO] epoch: 3543, train loss: 0.0000433540, val loss: 0.0003399444\n",
            "[INFO] epoch: 3544, train loss: 0.0000463419, val loss: 0.0000720824\n",
            "[INFO] epoch: 3545, train loss: 0.0000437359, val loss: 0.0003511123\n",
            "[INFO] epoch: 3546, train loss: 0.0000463204, val loss: 0.0000721481\n",
            "[INFO] epoch: 3547, train loss: 0.0000433674, val loss: 0.0003477924\n",
            "[INFO] epoch: 3548, train loss: 0.0000457848, val loss: 0.0000766718\n",
            "[INFO] epoch: 3549, train loss: 0.0000435503, val loss: 0.0003368119\n",
            "[INFO] epoch: 3550, train loss: 0.0000465491, val loss: 0.0000831460\n",
            "[INFO] epoch: 3551, train loss: 0.0000443065, val loss: 0.0003270697\n",
            "[INFO] epoch: 3552, train loss: 0.0000470477, val loss: 0.0000868048\n",
            "[INFO] epoch: 3553, train loss: 0.0000439915, val loss: 0.0003232201\n",
            "[INFO] epoch: 3554, train loss: 0.0000459364, val loss: 0.0000835492\n",
            "[INFO] epoch: 3555, train loss: 0.0000430676, val loss: 0.0003278990\n",
            "[INFO] epoch: 3556, train loss: 0.0000453339, val loss: 0.0000769960\n",
            "[INFO] epoch: 3557, train loss: 0.0000431809, val loss: 0.0003398638\n",
            "[INFO] epoch: 3558, train loss: 0.0000461237, val loss: 0.0000726256\n",
            "[INFO] epoch: 3559, train loss: 0.0000435633, val loss: 0.0003511536\n",
            "[INFO] epoch: 3560, train loss: 0.0000461445, val loss: 0.0000724773\n",
            "[INFO] epoch: 3561, train loss: 0.0000432271, val loss: 0.0003486174\n",
            "[INFO] epoch: 3562, train loss: 0.0000456187, val loss: 0.0000766182\n",
            "[INFO] epoch: 3563, train loss: 0.0000433335, val loss: 0.0003383815\n",
            "[INFO] epoch: 3564, train loss: 0.0000462510, val loss: 0.0000829038\n",
            "[INFO] epoch: 3565, train loss: 0.0000440619, val loss: 0.0003281849\n",
            "[INFO] epoch: 3566, train loss: 0.0000468205, val loss: 0.0000868062\n",
            "[INFO] epoch: 3567, train loss: 0.0000438320, val loss: 0.0003246666\n",
            "[INFO] epoch: 3568, train loss: 0.0000457921, val loss: 0.0000840715\n",
            "[INFO] epoch: 3569, train loss: 0.0000429124, val loss: 0.0003277285\n",
            "[INFO] epoch: 3570, train loss: 0.0000451139, val loss: 0.0000777850\n",
            "[INFO] epoch: 3571, train loss: 0.0000429477, val loss: 0.0003401479\n",
            "[INFO] epoch: 3572, train loss: 0.0000458303, val loss: 0.0000731546\n",
            "[INFO] epoch: 3573, train loss: 0.0000433540, val loss: 0.0003507475\n",
            "[INFO] epoch: 3574, train loss: 0.0000459681, val loss: 0.0000727530\n",
            "[INFO] epoch: 3575, train loss: 0.0000430672, val loss: 0.0003503217\n",
            "[INFO] epoch: 3576, train loss: 0.0000454216, val loss: 0.0000764387\n",
            "[INFO] epoch: 3577, train loss: 0.0000430999, val loss: 0.0003396779\n",
            "[INFO] epoch: 3578, train loss: 0.0000459577, val loss: 0.0000826904\n",
            "[INFO] epoch: 3579, train loss: 0.0000438371, val loss: 0.0003300280\n",
            "[INFO] epoch: 3580, train loss: 0.0000466492, val loss: 0.0000869547\n",
            "[INFO] epoch: 3581, train loss: 0.0000437361, val loss: 0.0003253441\n",
            "[INFO] epoch: 3582, train loss: 0.0000457279, val loss: 0.0000849010\n",
            "[INFO] epoch: 3583, train loss: 0.0000428174, val loss: 0.0003281103\n",
            "[INFO] epoch: 3584, train loss: 0.0000449487, val loss: 0.0000786359\n",
            "[INFO] epoch: 3585, train loss: 0.0000427604, val loss: 0.0003396971\n",
            "[INFO] epoch: 3586, train loss: 0.0000456130, val loss: 0.0000737361\n",
            "[INFO] epoch: 3587, train loss: 0.0000432357, val loss: 0.0003513080\n",
            "[INFO] epoch: 3588, train loss: 0.0000459153, val loss: 0.0000728255\n",
            "[INFO] epoch: 3589, train loss: 0.0000429933, val loss: 0.0003521160\n",
            "[INFO] epoch: 3590, train loss: 0.0000453022, val loss: 0.0000762062\n",
            "[INFO] epoch: 3591, train loss: 0.0000429242, val loss: 0.0003416427\n",
            "[INFO] epoch: 3592, train loss: 0.0000457368, val loss: 0.0000824073\n",
            "[INFO] epoch: 3593, train loss: 0.0000436668, val loss: 0.0003318010\n",
            "[INFO] epoch: 3594, train loss: 0.0000465375, val loss: 0.0000872268\n",
            "[INFO] epoch: 3595, train loss: 0.0000437071, val loss: 0.0003258813\n",
            "[INFO] epoch: 3596, train loss: 0.0000457505, val loss: 0.0000858214\n",
            "[INFO] epoch: 3597, train loss: 0.0000427610, val loss: 0.0003286216\n",
            "[INFO] epoch: 3598, train loss: 0.0000447937, val loss: 0.0000795769\n",
            "[INFO] epoch: 3599, train loss: 0.0000425879, val loss: 0.0003386456\n",
            "[INFO] epoch: 3600, train loss: 0.0000454210, val loss: 0.0000743234\n",
            "[INFO] epoch: 3601, train loss: 0.0000431224, val loss: 0.0003523026\n",
            "[INFO] epoch: 3602, train loss: 0.0000458494, val loss: 0.0000728888\n",
            "[INFO] epoch: 3603, train loss: 0.0000429279, val loss: 0.0003533047\n",
            "[INFO] epoch: 3604, train loss: 0.0000452159, val loss: 0.0000759770\n",
            "[INFO] epoch: 3605, train loss: 0.0000427566, val loss: 0.0003443191\n",
            "[INFO] epoch: 3606, train loss: 0.0000454974, val loss: 0.0000820625\n",
            "[INFO] epoch: 3607, train loss: 0.0000434744, val loss: 0.0003328869\n",
            "[INFO] epoch: 3608, train loss: 0.0000463905, val loss: 0.0000873451\n",
            "[INFO] epoch: 3609, train loss: 0.0000436475, val loss: 0.0003274233\n",
            "[INFO] epoch: 3610, train loss: 0.0000457422, val loss: 0.0000865691\n",
            "[INFO] epoch: 3611, train loss: 0.0000426949, val loss: 0.0003282351\n",
            "[INFO] epoch: 3612, train loss: 0.0000446405, val loss: 0.0000805182\n",
            "[INFO] epoch: 3613, train loss: 0.0000423906, val loss: 0.0003386175\n",
            "[INFO] epoch: 3614, train loss: 0.0000451431, val loss: 0.0000749420\n",
            "[INFO] epoch: 3615, train loss: 0.0000429078, val loss: 0.0003516740\n",
            "[INFO] epoch: 3616, train loss: 0.0000456611, val loss: 0.0000731756\n",
            "[INFO] epoch: 3617, train loss: 0.0000427885, val loss: 0.0003548999\n",
            "[INFO] epoch: 3618, train loss: 0.0000450730, val loss: 0.0000758402\n",
            "[INFO] epoch: 3619, train loss: 0.0000425519, val loss: 0.0003459074\n",
            "[INFO] epoch: 3620, train loss: 0.0000452036, val loss: 0.0000816979\n",
            "[INFO] epoch: 3621, train loss: 0.0000432109, val loss: 0.0003348440\n",
            "[INFO] epoch: 3622, train loss: 0.0000461431, val loss: 0.0000872216\n",
            "[INFO] epoch: 3623, train loss: 0.0000435062, val loss: 0.0003286900\n",
            "[INFO] epoch: 3624, train loss: 0.0000456688, val loss: 0.0000870380\n",
            "[INFO] epoch: 3625, train loss: 0.0000426403, val loss: 0.0003287733\n",
            "[INFO] epoch: 3626, train loss: 0.0000445478, val loss: 0.0000814307\n",
            "[INFO] epoch: 3627, train loss: 0.0000422264, val loss: 0.0003385140\n",
            "[INFO] epoch: 3628, train loss: 0.0000448772, val loss: 0.0000756728\n",
            "[INFO] epoch: 3629, train loss: 0.0000427144, val loss: 0.0003511404\n",
            "[INFO] epoch: 3630, train loss: 0.0000454983, val loss: 0.0000736312\n",
            "[INFO] epoch: 3631, train loss: 0.0000426826, val loss: 0.0003559367\n",
            "[INFO] epoch: 3632, train loss: 0.0000449680, val loss: 0.0000757797\n",
            "[INFO] epoch: 3633, train loss: 0.0000424033, val loss: 0.0003476184\n",
            "[INFO] epoch: 3634, train loss: 0.0000449753, val loss: 0.0000813586\n",
            "[INFO] epoch: 3635, train loss: 0.0000429707, val loss: 0.0003369478\n",
            "[INFO] epoch: 3636, train loss: 0.0000458853, val loss: 0.0000869078\n",
            "[INFO] epoch: 3637, train loss: 0.0000433647, val loss: 0.0003299928\n",
            "[INFO] epoch: 3638, train loss: 0.0000455981, val loss: 0.0000874548\n",
            "[INFO] epoch: 3639, train loss: 0.0000425840, val loss: 0.0003298342\n",
            "[INFO] epoch: 3640, train loss: 0.0000444521, val loss: 0.0000822906\n",
            "[INFO] epoch: 3641, train loss: 0.0000420627, val loss: 0.0003378805\n",
            "[INFO] epoch: 3642, train loss: 0.0000446069, val loss: 0.0000765645\n",
            "[INFO] epoch: 3643, train loss: 0.0000424898, val loss: 0.0003508796\n",
            "[INFO] epoch: 3644, train loss: 0.0000452727, val loss: 0.0000740557\n",
            "[INFO] epoch: 3645, train loss: 0.0000425358, val loss: 0.0003564340\n",
            "[INFO] epoch: 3646, train loss: 0.0000448265, val loss: 0.0000757804\n",
            "[INFO] epoch: 3647, train loss: 0.0000422109, val loss: 0.0003495242\n",
            "[INFO] epoch: 3648, train loss: 0.0000446977, val loss: 0.0000809726\n",
            "[INFO] epoch: 3649, train loss: 0.0000426783, val loss: 0.0003388502\n",
            "[INFO] epoch: 3650, train loss: 0.0000455557, val loss: 0.0000866433\n",
            "[INFO] epoch: 3651, train loss: 0.0000431579, val loss: 0.0003313431\n",
            "[INFO] epoch: 3652, train loss: 0.0000454824, val loss: 0.0000879683\n",
            "[INFO] epoch: 3653, train loss: 0.0000424944, val loss: 0.0003306024\n",
            "[INFO] epoch: 3654, train loss: 0.0000443278, val loss: 0.0000833447\n",
            "[INFO] epoch: 3655, train loss: 0.0000418868, val loss: 0.0003372475\n",
            "[INFO] epoch: 3656, train loss: 0.0000443473, val loss: 0.0000774922\n",
            "[INFO] epoch: 3657, train loss: 0.0000422842, val loss: 0.0003507249\n",
            "[INFO] epoch: 3658, train loss: 0.0000450900, val loss: 0.0000744818\n",
            "[INFO] epoch: 3659, train loss: 0.0000424522, val loss: 0.0003571889\n",
            "[INFO] epoch: 3660, train loss: 0.0000447982, val loss: 0.0000756908\n",
            "[INFO] epoch: 3661, train loss: 0.0000421106, val loss: 0.0003522667\n",
            "[INFO] epoch: 3662, train loss: 0.0000445096, val loss: 0.0000804956\n",
            "[INFO] epoch: 3663, train loss: 0.0000424675, val loss: 0.0003407194\n",
            "[INFO] epoch: 3664, train loss: 0.0000453430, val loss: 0.0000864167\n",
            "[INFO] epoch: 3665, train loss: 0.0000430609, val loss: 0.0003334509\n",
            "[INFO] epoch: 3666, train loss: 0.0000454906, val loss: 0.0000885531\n",
            "[INFO] epoch: 3667, train loss: 0.0000425228, val loss: 0.0003306373\n",
            "[INFO] epoch: 3668, train loss: 0.0000443413, val loss: 0.0000845456\n",
            "[INFO] epoch: 3669, train loss: 0.0000417982, val loss: 0.0003375119\n",
            "[INFO] epoch: 3670, train loss: 0.0000441361, val loss: 0.0000784261\n",
            "[INFO] epoch: 3671, train loss: 0.0000421065, val loss: 0.0003493471\n",
            "[INFO] epoch: 3672, train loss: 0.0000449429, val loss: 0.0000749665\n",
            "[INFO] epoch: 3673, train loss: 0.0000423860, val loss: 0.0003590078\n",
            "[INFO] epoch: 3674, train loss: 0.0000447721, val loss: 0.0000755034\n",
            "[INFO] epoch: 3675, train loss: 0.0000420111, val loss: 0.0003539118\n",
            "[INFO] epoch: 3676, train loss: 0.0000443243, val loss: 0.0000800194\n",
            "[INFO] epoch: 3677, train loss: 0.0000422341, val loss: 0.0003437651\n",
            "[INFO] epoch: 3678, train loss: 0.0000450855, val loss: 0.0000860789\n",
            "[INFO] epoch: 3679, train loss: 0.0000429212, val loss: 0.0003343694\n",
            "[INFO] epoch: 3680, train loss: 0.0000454535, val loss: 0.0000890783\n",
            "[INFO] epoch: 3681, train loss: 0.0000425281, val loss: 0.0003319195\n",
            "[INFO] epoch: 3682, train loss: 0.0000443435, val loss: 0.0000856788\n",
            "[INFO] epoch: 3683, train loss: 0.0000416817, val loss: 0.0003363622\n",
            "[INFO] epoch: 3684, train loss: 0.0000438804, val loss: 0.0000795253\n",
            "[INFO] epoch: 3685, train loss: 0.0000418707, val loss: 0.0003490708\n",
            "[INFO] epoch: 3686, train loss: 0.0000446976, val loss: 0.0000754496\n",
            "[INFO] epoch: 3687, train loss: 0.0000422452, val loss: 0.0003589074\n",
            "[INFO] epoch: 3688, train loss: 0.0000446808, val loss: 0.0000755392\n",
            "[INFO] epoch: 3689, train loss: 0.0000418842, val loss: 0.0003567714\n",
            "[INFO] epoch: 3690, train loss: 0.0000441362, val loss: 0.0000795235\n",
            "[INFO] epoch: 3691, train loss: 0.0000419799, val loss: 0.0003455175\n",
            "[INFO] epoch: 3692, train loss: 0.0000447593, val loss: 0.0000856760\n",
            "[INFO] epoch: 3693, train loss: 0.0000426955, val loss: 0.0003366857\n",
            "[INFO] epoch: 3694, train loss: 0.0000453156, val loss: 0.0000892624\n",
            "[INFO] epoch: 3695, train loss: 0.0000424749, val loss: 0.0003325914\n",
            "[INFO] epoch: 3696, train loss: 0.0000443209, val loss: 0.0000867523\n",
            "[INFO] epoch: 3697, train loss: 0.0000415794, val loss: 0.0003362724\n",
            "[INFO] epoch: 3698, train loss: 0.0000436522, val loss: 0.0000805940\n",
            "[INFO] epoch: 3699, train loss: 0.0000416136, val loss: 0.0003481004\n",
            "[INFO] epoch: 3700, train loss: 0.0000443774, val loss: 0.0000761962\n",
            "[INFO] epoch: 3701, train loss: 0.0000420477, val loss: 0.0003590194\n",
            "[INFO] epoch: 3702, train loss: 0.0000445474, val loss: 0.0000756467\n",
            "[INFO] epoch: 3703, train loss: 0.0000417587, val loss: 0.0003584380\n",
            "[INFO] epoch: 3704, train loss: 0.0000439684, val loss: 0.0000792509\n",
            "[INFO] epoch: 3705, train loss: 0.0000417354, val loss: 0.0003482393\n",
            "[INFO] epoch: 3706, train loss: 0.0000444156, val loss: 0.0000850392\n",
            "[INFO] epoch: 3707, train loss: 0.0000424295, val loss: 0.0003385890\n",
            "[INFO] epoch: 3708, train loss: 0.0000451302, val loss: 0.0000893407\n",
            "[INFO] epoch: 3709, train loss: 0.0000424195, val loss: 0.0003341020\n",
            "[INFO] epoch: 3710, train loss: 0.0000443317, val loss: 0.0000875447\n",
            "[INFO] epoch: 3711, train loss: 0.0000415404, val loss: 0.0003365177\n",
            "[INFO] epoch: 3712, train loss: 0.0000435233, val loss: 0.0000817984\n",
            "[INFO] epoch: 3713, train loss: 0.0000414324, val loss: 0.0003472302\n",
            "[INFO] epoch: 3714, train loss: 0.0000441137, val loss: 0.0000770002\n",
            "[INFO] epoch: 3715, train loss: 0.0000418960, val loss: 0.0003589076\n",
            "[INFO] epoch: 3716, train loss: 0.0000444681, val loss: 0.0000759589\n",
            "[INFO] epoch: 3717, train loss: 0.0000417120, val loss: 0.0003600355\n",
            "[INFO] epoch: 3718, train loss: 0.0000438889, val loss: 0.0000788945\n",
            "[INFO] epoch: 3719, train loss: 0.0000415589, val loss: 0.0003508081\n",
            "[INFO] epoch: 3720, train loss: 0.0000441516, val loss: 0.0000845306\n",
            "[INFO] epoch: 3721, train loss: 0.0000422052, val loss: 0.0003408345\n",
            "[INFO] epoch: 3722, train loss: 0.0000449339, val loss: 0.0000891743\n",
            "[INFO] epoch: 3723, train loss: 0.0000423465, val loss: 0.0003354709\n",
            "[INFO] epoch: 3724, train loss: 0.0000443551, val loss: 0.0000883981\n",
            "[INFO] epoch: 3725, train loss: 0.0000415249, val loss: 0.0003370231\n",
            "[INFO] epoch: 3726, train loss: 0.0000434124, val loss: 0.0000830108\n",
            "[INFO] epoch: 3727, train loss: 0.0000412509, val loss: 0.0003461069\n",
            "[INFO] epoch: 3728, train loss: 0.0000438488, val loss: 0.0000779326\n",
            "[INFO] epoch: 3729, train loss: 0.0000417322, val loss: 0.0003587558\n",
            "[INFO] epoch: 3730, train loss: 0.0000443513, val loss: 0.0000762649\n",
            "[INFO] epoch: 3731, train loss: 0.0000416382, val loss: 0.0003613998\n",
            "[INFO] epoch: 3732, train loss: 0.0000438138, val loss: 0.0000785939\n",
            "[INFO] epoch: 3733, train loss: 0.0000413874, val loss: 0.0003535037\n",
            "[INFO] epoch: 3734, train loss: 0.0000438639, val loss: 0.0000839737\n",
            "[INFO] epoch: 3735, train loss: 0.0000419243, val loss: 0.0003430117\n",
            "[INFO] epoch: 3736, train loss: 0.0000446691, val loss: 0.0000889704\n",
            "[INFO] epoch: 3737, train loss: 0.0000422321, val loss: 0.0003370456\n",
            "[INFO] epoch: 3738, train loss: 0.0000443447, val loss: 0.0000893172\n",
            "[INFO] epoch: 3739, train loss: 0.0000414773, val loss: 0.0003369800\n",
            "[INFO] epoch: 3740, train loss: 0.0000432746, val loss: 0.0000843140\n",
            "[INFO] epoch: 3741, train loss: 0.0000410357, val loss: 0.0003454517\n",
            "[INFO] epoch: 3742, train loss: 0.0000435166, val loss: 0.0000789559\n",
            "[INFO] epoch: 3743, train loss: 0.0000414853, val loss: 0.0003577138\n",
            "[INFO] epoch: 3744, train loss: 0.0000441569, val loss: 0.0000766123\n",
            "[INFO] epoch: 3745, train loss: 0.0000415327, val loss: 0.0003632861\n",
            "[INFO] epoch: 3746, train loss: 0.0000437425, val loss: 0.0000783394\n",
            "[INFO] epoch: 3747, train loss: 0.0000412191, val loss: 0.0003560544\n",
            "[INFO] epoch: 3748, train loss: 0.0000435842, val loss: 0.0000832790\n",
            "[INFO] epoch: 3749, train loss: 0.0000416550, val loss: 0.0003459077\n",
            "[INFO] epoch: 3750, train loss: 0.0000444421, val loss: 0.0000887879\n",
            "[INFO] epoch: 3751, train loss: 0.0000421662, val loss: 0.0003385861\n",
            "[INFO] epoch: 3752, train loss: 0.0000444084, val loss: 0.0000901069\n",
            "[INFO] epoch: 3753, train loss: 0.0000415393, val loss: 0.0003376861\n",
            "[INFO] epoch: 3754, train loss: 0.0000433014, val loss: 0.0000857314\n",
            "[INFO] epoch: 3755, train loss: 0.0000409397, val loss: 0.0003445110\n",
            "[INFO] epoch: 3756, train loss: 0.0000432804, val loss: 0.0000800580\n",
            "[INFO] epoch: 3757, train loss: 0.0000413123, val loss: 0.0003571696\n",
            "[INFO] epoch: 3758, train loss: 0.0000440418, val loss: 0.0000770358\n",
            "[INFO] epoch: 3759, train loss: 0.0000415091, val loss: 0.0003643052\n",
            "[INFO] epoch: 3760, train loss: 0.0000437537, val loss: 0.0000781179\n",
            "[INFO] epoch: 3761, train loss: 0.0000411226, val loss: 0.0003594157\n",
            "[INFO] epoch: 3762, train loss: 0.0000433883, val loss: 0.0000825817\n",
            "[INFO] epoch: 3763, train loss: 0.0000414161, val loss: 0.0003480851\n",
            "[INFO] epoch: 3764, train loss: 0.0000441701, val loss: 0.0000884598\n",
            "[INFO] epoch: 3765, train loss: 0.0000420192, val loss: 0.0003408049\n",
            "[INFO] epoch: 3766, train loss: 0.0000443812, val loss: 0.0000906201\n",
            "[INFO] epoch: 3767, train loss: 0.0000415488, val loss: 0.0003380029\n",
            "[INFO] epoch: 3768, train loss: 0.0000432817, val loss: 0.0000871763\n",
            "[INFO] epoch: 3769, train loss: 0.0000407772, val loss: 0.0003437490\n",
            "[INFO] epoch: 3770, train loss: 0.0000429487, val loss: 0.0000812252\n",
            "[INFO] epoch: 3771, train loss: 0.0000410162, val loss: 0.0003558239\n",
            "[INFO] epoch: 3772, train loss: 0.0000437304, val loss: 0.0000777251\n",
            "[INFO] epoch: 3773, train loss: 0.0000413217, val loss: 0.0003650262\n",
            "[INFO] epoch: 3774, train loss: 0.0000436250, val loss: 0.0000779883\n",
            "[INFO] epoch: 3775, train loss: 0.0000409562, val loss: 0.0003616663\n",
            "[INFO] epoch: 3776, train loss: 0.0000431371, val loss: 0.0000821277\n",
            "[INFO] epoch: 3777, train loss: 0.0000410844, val loss: 0.0003512274\n",
            "[INFO] epoch: 3778, train loss: 0.0000437537, val loss: 0.0000877971\n",
            "[INFO] epoch: 3779, train loss: 0.0000417502, val loss: 0.0003425923\n",
            "[INFO] epoch: 3780, train loss: 0.0000442475, val loss: 0.0000910431\n",
            "[INFO] epoch: 3781, train loss: 0.0000415203, val loss: 0.0003394258\n",
            "[INFO] epoch: 3782, train loss: 0.0000432929, val loss: 0.0000883627\n",
            "[INFO] epoch: 3783, train loss: 0.0000407068, val loss: 0.0003432485\n",
            "[INFO] epoch: 3784, train loss: 0.0000427605, val loss: 0.0000825995\n",
            "[INFO] epoch: 3785, train loss: 0.0000408122, val loss: 0.0003549324\n",
            "[INFO] epoch: 3786, train loss: 0.0000434859, val loss: 0.0000785066\n",
            "[INFO] epoch: 3787, train loss: 0.0000412309, val loss: 0.0003652571\n",
            "[INFO] epoch: 3788, train loss: 0.0000436513, val loss: 0.0000781065\n",
            "[INFO] epoch: 3789, train loss: 0.0000409684, val loss: 0.0003642230\n",
            "[INFO] epoch: 3790, train loss: 0.0000430941, val loss: 0.0000816039\n",
            "[INFO] epoch: 3791, train loss: 0.0000409328, val loss: 0.0003544255\n",
            "[INFO] epoch: 3792, train loss: 0.0000435239, val loss: 0.0000870962\n",
            "[INFO] epoch: 3793, train loss: 0.0000415995, val loss: 0.0003451003\n",
            "[INFO] epoch: 3794, train loss: 0.0000441814, val loss: 0.0000911159\n",
            "[INFO] epoch: 3795, train loss: 0.0000415674, val loss: 0.0003408870\n",
            "[INFO] epoch: 3796, train loss: 0.0000434268, val loss: 0.0000893870\n",
            "[INFO] epoch: 3797, train loss: 0.0000407492, val loss: 0.0003434839\n",
            "[INFO] epoch: 3798, train loss: 0.0000426593, val loss: 0.0000839872\n",
            "[INFO] epoch: 3799, train loss: 0.0000406271, val loss: 0.0003534257\n",
            "[INFO] epoch: 3800, train loss: 0.0000432145, val loss: 0.0000794211\n",
            "[INFO] epoch: 3801, train loss: 0.0000410809, val loss: 0.0003651636\n",
            "[INFO] epoch: 3802, train loss: 0.0000435545, val loss: 0.0000783690\n",
            "[INFO] epoch: 3803, train loss: 0.0000408818, val loss: 0.0003658611\n",
            "[INFO] epoch: 3804, train loss: 0.0000429705, val loss: 0.0000811355\n",
            "[INFO] epoch: 3805, train loss: 0.0000406994, val loss: 0.0003574173\n",
            "[INFO] epoch: 3806, train loss: 0.0000431602, val loss: 0.0000864883\n",
            "[INFO] epoch: 3807, train loss: 0.0000412722, val loss: 0.0003473032\n",
            "[INFO] epoch: 3808, train loss: 0.0000438970, val loss: 0.0000909619\n",
            "[INFO] epoch: 3809, train loss: 0.0000414527, val loss: 0.0003425072\n",
            "[INFO] epoch: 3810, train loss: 0.0000434131, val loss: 0.0000904670\n",
            "[INFO] epoch: 3811, train loss: 0.0000406731, val loss: 0.0003432774\n",
            "[INFO] epoch: 3812, train loss: 0.0000424655, val loss: 0.0000854694\n",
            "[INFO] epoch: 3813, train loss: 0.0000403706, val loss: 0.0003524097\n",
            "[INFO] epoch: 3814, train loss: 0.0000428478, val loss: 0.0000804977\n",
            "[INFO] epoch: 3815, train loss: 0.0000408304, val loss: 0.0003641378\n",
            "[INFO] epoch: 3816, train loss: 0.0000433739, val loss: 0.0000787601\n",
            "[INFO] epoch: 3817, train loss: 0.0000407876, val loss: 0.0003681760\n",
            "[INFO] epoch: 3818, train loss: 0.0000428988, val loss: 0.0000807296\n",
            "[INFO] epoch: 3819, train loss: 0.0000405147, val loss: 0.0003600861\n",
            "[INFO] epoch: 3820, train loss: 0.0000428548, val loss: 0.0000858185\n",
            "[INFO] epoch: 3821, train loss: 0.0000409931, val loss: 0.0003507023\n",
            "[INFO] epoch: 3822, train loss: 0.0000436619, val loss: 0.0000906622\n",
            "[INFO] epoch: 3823, train loss: 0.0000413868, val loss: 0.0003439297\n",
            "[INFO] epoch: 3824, train loss: 0.0000434859, val loss: 0.0000915363\n",
            "[INFO] epoch: 3825, train loss: 0.0000407244, val loss: 0.0003439436\n",
            "[INFO] epoch: 3826, train loss: 0.0000424484, val loss: 0.0000869526\n",
            "[INFO] epoch: 3827, train loss: 0.0000402297, val loss: 0.0003512116\n",
            "[INFO] epoch: 3828, train loss: 0.0000425570, val loss: 0.0000817658\n",
            "[INFO] epoch: 3829, train loss: 0.0000406248, val loss: 0.0003634027\n",
            "[INFO] epoch: 3830, train loss: 0.0000432386, val loss: 0.0000791433\n",
            "[INFO] epoch: 3831, train loss: 0.0000407507, val loss: 0.0003695665\n",
            "[INFO] epoch: 3832, train loss: 0.0000428962, val loss: 0.0000804859\n",
            "[INFO] epoch: 3833, train loss: 0.0000403860, val loss: 0.0003637598\n",
            "[INFO] epoch: 3834, train loss: 0.0000426017, val loss: 0.0000849665\n",
            "[INFO] epoch: 3835, train loss: 0.0000407071, val loss: 0.0003532765\n",
            "[INFO] epoch: 3836, train loss: 0.0000433659, val loss: 0.0000904095\n",
            "[INFO] epoch: 3837, train loss: 0.0000412498, val loss: 0.0003461948\n",
            "[INFO] epoch: 3838, train loss: 0.0000434957, val loss: 0.0000922414\n",
            "[INFO] epoch: 3839, train loss: 0.0000407665, val loss: 0.0003443503\n",
            "[INFO] epoch: 3840, train loss: 0.0000424581, val loss: 0.0000886036\n",
            "[INFO] epoch: 3841, train loss: 0.0000400860, val loss: 0.0003501907\n",
            "[INFO] epoch: 3842, train loss: 0.0000422450, val loss: 0.0000831143\n",
            "[INFO] epoch: 3843, train loss: 0.0000403802, val loss: 0.0003622315\n",
            "[INFO] epoch: 3844, train loss: 0.0000430264, val loss: 0.0000797555\n",
            "[INFO] epoch: 3845, train loss: 0.0000406661, val loss: 0.0003705998\n",
            "[INFO] epoch: 3846, train loss: 0.0000428875, val loss: 0.0000803244\n",
            "[INFO] epoch: 3847, train loss: 0.0000403065, val loss: 0.0003670025\n",
            "[INFO] epoch: 3848, train loss: 0.0000424317, val loss: 0.0000842396\n",
            "[INFO] epoch: 3849, train loss: 0.0000404455, val loss: 0.0003565689\n",
            "[INFO] epoch: 3850, train loss: 0.0000430412, val loss: 0.0000898649\n",
            "[INFO] epoch: 3851, train loss: 0.0000410818, val loss: 0.0003484557\n",
            "[INFO] epoch: 3852, train loss: 0.0000434851, val loss: 0.0000926747\n",
            "[INFO] epoch: 3853, train loss: 0.0000408287, val loss: 0.0003455702\n",
            "[INFO] epoch: 3854, train loss: 0.0000425425, val loss: 0.0000901489\n",
            "[INFO] epoch: 3855, train loss: 0.0000400451, val loss: 0.0003493155\n",
            "[INFO] epoch: 3856, train loss: 0.0000420524, val loss: 0.0000845148\n",
            "[INFO] epoch: 3857, train loss: 0.0000401661, val loss: 0.0003611702\n",
            "[INFO] epoch: 3858, train loss: 0.0000427647, val loss: 0.0000806478\n",
            "[INFO] epoch: 3859, train loss: 0.0000405634, val loss: 0.0003706692\n",
            "[INFO] epoch: 3860, train loss: 0.0000428975, val loss: 0.0000802980\n",
            "[INFO] epoch: 3861, train loss: 0.0000402732, val loss: 0.0003700038\n",
            "[INFO] epoch: 3862, train loss: 0.0000423137, val loss: 0.0000837037\n",
            "[INFO] epoch: 3863, train loss: 0.0000402061, val loss: 0.0003597322\n",
            "[INFO] epoch: 3864, train loss: 0.0000426985, val loss: 0.0000890592\n",
            "[INFO] epoch: 3865, train loss: 0.0000408338, val loss: 0.0003512155\n",
            "[INFO] epoch: 3866, train loss: 0.0000433278, val loss: 0.0000928717\n",
            "[INFO] epoch: 3867, train loss: 0.0000408084, val loss: 0.0003466418\n",
            "[INFO] epoch: 3868, train loss: 0.0000426106, val loss: 0.0000913703\n",
            "[INFO] epoch: 3869, train loss: 0.0000400208, val loss: 0.0003496394\n",
            "[INFO] epoch: 3870, train loss: 0.0000418630, val loss: 0.0000860510\n",
            "[INFO] epoch: 3871, train loss: 0.0000398963, val loss: 0.0003589647\n",
            "[INFO] epoch: 3872, train loss: 0.0000424045, val loss: 0.0000817619\n",
            "[INFO] epoch: 3873, train loss: 0.0000403621, val loss: 0.0003709447\n",
            "[INFO] epoch: 3874, train loss: 0.0000427801, val loss: 0.0000805014\n",
            "[INFO] epoch: 3875, train loss: 0.0000401909, val loss: 0.0003716067\n",
            "[INFO] epoch: 3876, train loss: 0.0000422153, val loss: 0.0000832932\n",
            "[INFO] epoch: 3877, train loss: 0.0000399959, val loss: 0.0003636955\n",
            "[INFO] epoch: 3878, train loss: 0.0000423618, val loss: 0.0000882325\n",
            "[INFO] epoch: 3879, train loss: 0.0000405391, val loss: 0.0003535929\n",
            "[INFO] epoch: 3880, train loss: 0.0000431022, val loss: 0.0000928033\n",
            "[INFO] epoch: 3881, train loss: 0.0000407726, val loss: 0.0003489304\n",
            "[INFO] epoch: 3882, train loss: 0.0000427065, val loss: 0.0000923673\n",
            "[INFO] epoch: 3883, train loss: 0.0000400456, val loss: 0.0003495173\n",
            "[INFO] epoch: 3884, train loss: 0.0000417769, val loss: 0.0000877850\n",
            "[INFO] epoch: 3885, train loss: 0.0000397230, val loss: 0.0003580267\n",
            "[INFO] epoch: 3886, train loss: 0.0000421070, val loss: 0.0000828429\n",
            "[INFO] epoch: 3887, train loss: 0.0000401671, val loss: 0.0003698323\n",
            "[INFO] epoch: 3888, train loss: 0.0000426557, val loss: 0.0000810576\n",
            "[INFO] epoch: 3889, train loss: 0.0000401639, val loss: 0.0003739134\n",
            "[INFO] epoch: 3890, train loss: 0.0000422176, val loss: 0.0000827747\n",
            "[INFO] epoch: 3891, train loss: 0.0000398550, val loss: 0.0003667190\n",
            "[INFO] epoch: 3892, train loss: 0.0000420845, val loss: 0.0000875954\n",
            "[INFO] epoch: 3893, train loss: 0.0000402628, val loss: 0.0003570714\n",
            "[INFO] epoch: 3894, train loss: 0.0000428416, val loss: 0.0000923480\n",
            "[INFO] epoch: 3895, train loss: 0.0000406776, val loss: 0.0003506722\n",
            "[INFO] epoch: 3896, train loss: 0.0000427547, val loss: 0.0000934271\n",
            "[INFO] epoch: 3897, train loss: 0.0000400918, val loss: 0.0003500562\n",
            "[INFO] epoch: 3898, train loss: 0.0000417596, val loss: 0.0000893898\n",
            "[INFO] epoch: 3899, train loss: 0.0000395646, val loss: 0.0003569224\n",
            "[INFO] epoch: 3900, train loss: 0.0000417754, val loss: 0.0000842326\n",
            "[INFO] epoch: 3901, train loss: 0.0000399171, val loss: 0.0003684971\n",
            "[INFO] epoch: 3902, train loss: 0.0000424619, val loss: 0.0000816438\n",
            "[INFO] epoch: 3903, train loss: 0.0000400892, val loss: 0.0003754290\n",
            "[INFO] epoch: 3904, train loss: 0.0000421980, val loss: 0.0000825164\n",
            "[INFO] epoch: 3905, train loss: 0.0000397288, val loss: 0.0003702155\n",
            "[INFO] epoch: 3906, train loss: 0.0000418366, val loss: 0.0000868293\n",
            "[INFO] epoch: 3907, train loss: 0.0000399593, val loss: 0.0003603254\n",
            "[INFO] epoch: 3908, train loss: 0.0000425072, val loss: 0.0000918575\n",
            "[INFO] epoch: 3909, train loss: 0.0000405158, val loss: 0.0003530104\n",
            "[INFO] epoch: 3910, train loss: 0.0000427610, val loss: 0.0000942226\n",
            "[INFO] epoch: 3911, train loss: 0.0000401599, val loss: 0.0003508486\n",
            "[INFO] epoch: 3912, train loss: 0.0000418248, val loss: 0.0000910073\n",
            "[INFO] epoch: 3913, train loss: 0.0000394699, val loss: 0.0003560074\n",
            "[INFO] epoch: 3914, train loss: 0.0000415031, val loss: 0.0000858785\n",
            "[INFO] epoch: 3915, train loss: 0.0000396913, val loss: 0.0003670696\n",
            "[INFO] epoch: 3916, train loss: 0.0000422526, val loss: 0.0000822986\n",
            "[INFO] epoch: 3917, train loss: 0.0000400212, val loss: 0.0003764195\n",
            "[INFO] epoch: 3918, train loss: 0.0000422251, val loss: 0.0000825131\n",
            "[INFO] epoch: 3919, train loss: 0.0000396980, val loss: 0.0003734917\n",
            "[INFO] epoch: 3920, train loss: 0.0000417149, val loss: 0.0000859477\n",
            "[INFO] epoch: 3921, train loss: 0.0000397008, val loss: 0.0003641898\n",
            "[INFO] epoch: 3922, train loss: 0.0000421553, val loss: 0.0000913089\n",
            "[INFO] epoch: 3923, train loss: 0.0000403127, val loss: 0.0003552445\n",
            "[INFO] epoch: 3924, train loss: 0.0000427044, val loss: 0.0000945483\n",
            "[INFO] epoch: 3925, train loss: 0.0000401968, val loss: 0.0003525239\n",
            "[INFO] epoch: 3926, train loss: 0.0000419028, val loss: 0.0000926362\n",
            "[INFO] epoch: 3927, train loss: 0.0000394200, val loss: 0.0003546950\n",
            "[INFO] epoch: 3928, train loss: 0.0000412786, val loss: 0.0000875379\n",
            "[INFO] epoch: 3929, train loss: 0.0000394057, val loss: 0.0003661110\n",
            "[INFO] epoch: 3930, train loss: 0.0000418816, val loss: 0.0000832903\n",
            "[INFO] epoch: 3931, train loss: 0.0000398394, val loss: 0.0003755872\n",
            "[INFO] epoch: 3932, train loss: 0.0000421707, val loss: 0.0000826826\n",
            "[INFO] epoch: 3933, train loss: 0.0000396272, val loss: 0.0003772736\n",
            "[INFO] epoch: 3934, train loss: 0.0000415873, val loss: 0.0000852856\n",
            "[INFO] epoch: 3935, train loss: 0.0000394565, val loss: 0.0003670818\n",
            "[INFO] epoch: 3936, train loss: 0.0000418036, val loss: 0.0000906174\n",
            "[INFO] epoch: 3937, train loss: 0.0000400383, val loss: 0.0003591116\n",
            "[INFO] epoch: 3938, train loss: 0.0000425262, val loss: 0.0000944624\n",
            "[INFO] epoch: 3939, train loss: 0.0000401995, val loss: 0.0003533916\n",
            "[INFO] epoch: 3940, train loss: 0.0000420453, val loss: 0.0000941523\n",
            "[INFO] epoch: 3941, train loss: 0.0000394749, val loss: 0.0003556613\n",
            "[INFO] epoch: 3942, train loss: 0.0000411884, val loss: 0.0000890500\n",
            "[INFO] epoch: 3943, train loss: 0.0000392056, val loss: 0.0003636454\n",
            "[INFO] epoch: 3944, train loss: 0.0000415689, val loss: 0.0000847654\n",
            "[INFO] epoch: 3945, train loss: 0.0000396728, val loss: 0.0003759934\n",
            "[INFO] epoch: 3946, train loss: 0.0000421003, val loss: 0.0000827850\n",
            "[INFO] epoch: 3947, train loss: 0.0000396196, val loss: 0.0003786429\n",
            "[INFO] epoch: 3948, train loss: 0.0000416018, val loss: 0.0000851064\n",
            "[INFO] epoch: 3949, train loss: 0.0000393372, val loss: 0.0003718398\n",
            "[INFO] epoch: 3950, train loss: 0.0000415474, val loss: 0.0000895091\n",
            "[INFO] epoch: 3951, train loss: 0.0000397706, val loss: 0.0003616310\n",
            "[INFO] epoch: 3952, train loss: 0.0000422946, val loss: 0.0000944293\n",
            "[INFO] epoch: 3953, train loss: 0.0000401724, val loss: 0.0003561910\n",
            "[INFO] epoch: 3954, train loss: 0.0000421845, val loss: 0.0000949074\n",
            "[INFO] epoch: 3955, train loss: 0.0000395700, val loss: 0.0003558204\n",
            "[INFO] epoch: 3956, train loss: 0.0000412011, val loss: 0.0000909792\n",
            "[INFO] epoch: 3957, train loss: 0.0000390989, val loss: 0.0003626188\n",
            "[INFO] epoch: 3958, train loss: 0.0000413117, val loss: 0.0000859778\n",
            "[INFO] epoch: 3959, train loss: 0.0000394792, val loss: 0.0003747382\n",
            "[INFO] epoch: 3960, train loss: 0.0000419479, val loss: 0.0000835573\n",
            "[INFO] epoch: 3961, train loss: 0.0000396077, val loss: 0.0003802629\n",
            "[INFO] epoch: 3962, train loss: 0.0000416568, val loss: 0.0000846878\n",
            "[INFO] epoch: 3963, train loss: 0.0000392441, val loss: 0.0003751946\n",
            "[INFO] epoch: 3964, train loss: 0.0000413003, val loss: 0.0000888545\n",
            "[INFO] epoch: 3965, train loss: 0.0000394679, val loss: 0.0003651063\n",
            "[INFO] epoch: 3966, train loss: 0.0000419495, val loss: 0.0000938192\n",
            "[INFO] epoch: 3967, train loss: 0.0000399781, val loss: 0.0003584238\n",
            "[INFO] epoch: 3968, train loss: 0.0000421245, val loss: 0.0000956621\n",
            "[INFO] epoch: 3969, train loss: 0.0000395748, val loss: 0.0003564345\n",
            "[INFO] epoch: 3970, train loss: 0.0000411927, val loss: 0.0000927167\n",
            "[INFO] epoch: 3971, train loss: 0.0000389333, val loss: 0.0003618701\n",
            "[INFO] epoch: 3972, train loss: 0.0000409364, val loss: 0.0000875245\n",
            "[INFO] epoch: 3973, train loss: 0.0000391415, val loss: 0.0003726383\n",
            "[INFO] epoch: 3974, train loss: 0.0000416115, val loss: 0.0000845429\n",
            "[INFO] epoch: 3975, train loss: 0.0000394361, val loss: 0.0003812950\n",
            "[INFO] epoch: 3976, train loss: 0.0000415682, val loss: 0.0000845830\n",
            "[INFO] epoch: 3977, train loss: 0.0000391013, val loss: 0.0003783780\n",
            "[INFO] epoch: 3978, train loss: 0.0000410598, val loss: 0.0000882336\n",
            "[INFO] epoch: 3979, train loss: 0.0000391288, val loss: 0.0003688554\n",
            "[INFO] epoch: 3980, train loss: 0.0000415209, val loss: 0.0000931277\n",
            "[INFO] epoch: 3981, train loss: 0.0000397011, val loss: 0.0003612102\n",
            "[INFO] epoch: 3982, train loss: 0.0000420027, val loss: 0.0000962160\n",
            "[INFO] epoch: 3983, train loss: 0.0000395964, val loss: 0.0003577643\n",
            "[INFO] epoch: 3984, train loss: 0.0000412863, val loss: 0.0000943265\n",
            "[INFO] epoch: 3985, train loss: 0.0000388861, val loss: 0.0003614474\n",
            "[INFO] epoch: 3986, train loss: 0.0000407089, val loss: 0.0000893958\n",
            "[INFO] epoch: 3987, train loss: 0.0000389033, val loss: 0.0003709283\n",
            "[INFO] epoch: 3988, train loss: 0.0000413426, val loss: 0.0000855461\n",
            "[INFO] epoch: 3989, train loss: 0.0000393341, val loss: 0.0003819126\n",
            "[INFO] epoch: 3990, train loss: 0.0000415922, val loss: 0.0000847010\n",
            "[INFO] epoch: 3991, train loss: 0.0000391340, val loss: 0.0003814075\n",
            "[INFO] epoch: 3992, train loss: 0.0000410665, val loss: 0.0000875676\n",
            "[INFO] epoch: 3993, train loss: 0.0000389681, val loss: 0.0003735736\n",
            "[INFO] epoch: 3994, train loss: 0.0000412360, val loss: 0.0000922515\n",
            "[INFO] epoch: 3995, train loss: 0.0000395087, val loss: 0.0003638705\n",
            "[INFO] epoch: 3996, train loss: 0.0000419412, val loss: 0.0000964594\n",
            "[INFO] epoch: 3997, train loss: 0.0000396796, val loss: 0.0003601653\n",
            "[INFO] epoch: 3998, train loss: 0.0000414810, val loss: 0.0000956537\n",
            "[INFO] epoch: 3999, train loss: 0.0000389717, val loss: 0.0003606975\n",
            "[INFO] Finished training! Smallest loss: 0.0000577260\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial test set loss: 5.772596227870963e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:582: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:775.)\n",
            "  self.dropout, self.training, self.bidirectional, self.batch_first)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PFxnUx7KdY5i",
        "outputId": "25abe326-3a7d-4caa-eeb3-ef157a0bcd61"
      },
      "source": [
        "criterion = nn.MSELoss()\n",
        "opt = optim.Adam(best_trained_model.parameters(), lr=1e-5)\n",
        "\n",
        "train_losses, val_losses, best_trained_model_2 = best_trained_model.train(opt, criterion, train_loader, test_loader, epochs=1000)\n",
        "\n",
        "plt.plot(train_losses, label='train')\n",
        "plt.plot(val_losses, label='val')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "test_l = test_loss(best_trained_model_2)\n",
        "print(\"Best trial test set loss: {}\".format(test_l))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:582: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:775.)\n",
            "  self.dropout, self.training, self.bidirectional, self.batch_first)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO] epoch: 0, train loss: 0.0000300045, val loss: 0.0000566547\n",
            "[INFO] epoch: 1, train loss: 0.0000073561, val loss: 0.0000631752\n",
            "[INFO] epoch: 2, train loss: 0.0000027403, val loss: 0.0001082601\n",
            "[INFO] epoch: 3, train loss: 0.0000028043, val loss: 0.0001306378\n",
            "[INFO] epoch: 4, train loss: 0.0000024420, val loss: 0.0001263088\n",
            "[INFO] epoch: 5, train loss: 0.0000023867, val loss: 0.0001190410\n",
            "[INFO] epoch: 6, train loss: 0.0000023181, val loss: 0.0001184424\n",
            "[INFO] epoch: 7, train loss: 0.0000022695, val loss: 0.0001223845\n",
            "[INFO] epoch: 8, train loss: 0.0000022355, val loss: 0.0001269435\n",
            "[INFO] epoch: 9, train loss: 0.0000022179, val loss: 0.0001304761\n",
            "[INFO] epoch: 10, train loss: 0.0000021946, val loss: 0.0001334444\n",
            "[INFO] epoch: 11, train loss: 0.0000021692, val loss: 0.0001367132\n",
            "[INFO] epoch: 12, train loss: 0.0000021460, val loss: 0.0001404139\n",
            "[INFO] epoch: 13, train loss: 0.0000021244, val loss: 0.0001442101\n",
            "[INFO] epoch: 14, train loss: 0.0000021030, val loss: 0.0001479781\n",
            "[INFO] epoch: 15, train loss: 0.0000020823, val loss: 0.0001518120\n",
            "[INFO] epoch: 16, train loss: 0.0000020624, val loss: 0.0001557383\n",
            "[INFO] epoch: 17, train loss: 0.0000020432, val loss: 0.0001597157\n",
            "[INFO] epoch: 18, train loss: 0.0000020247, val loss: 0.0001637276\n",
            "[INFO] epoch: 19, train loss: 0.0000020068, val loss: 0.0001677720\n",
            "[INFO] epoch: 20, train loss: 0.0000019897, val loss: 0.0001718396\n",
            "[INFO] epoch: 21, train loss: 0.0000019732, val loss: 0.0001759253\n",
            "[INFO] epoch: 22, train loss: 0.0000019573, val loss: 0.0001800221\n",
            "[INFO] epoch: 23, train loss: 0.0000019420, val loss: 0.0001841225\n",
            "[INFO] epoch: 24, train loss: 0.0000019273, val loss: 0.0001882227\n",
            "[INFO] epoch: 25, train loss: 0.0000019131, val loss: 0.0001923170\n",
            "[INFO] epoch: 26, train loss: 0.0000018995, val loss: 0.0001964020\n",
            "[INFO] epoch: 27, train loss: 0.0000018863, val loss: 0.0002004713\n",
            "[INFO] epoch: 28, train loss: 0.0000018736, val loss: 0.0002045239\n",
            "[INFO] epoch: 29, train loss: 0.0000018613, val loss: 0.0002085542\n",
            "[INFO] epoch: 30, train loss: 0.0000018495, val loss: 0.0002125610\n",
            "[INFO] epoch: 31, train loss: 0.0000018380, val loss: 0.0002165400\n",
            "[INFO] epoch: 32, train loss: 0.0000018269, val loss: 0.0002204920\n",
            "[INFO] epoch: 33, train loss: 0.0000018162, val loss: 0.0002244091\n",
            "[INFO] epoch: 34, train loss: 0.0000018058, val loss: 0.0002282965\n",
            "[INFO] epoch: 35, train loss: 0.0000017957, val loss: 0.0002321465\n",
            "[INFO] epoch: 36, train loss: 0.0000017859, val loss: 0.0002359630\n",
            "[INFO] epoch: 37, train loss: 0.0000017763, val loss: 0.0002397396\n",
            "[INFO] epoch: 38, train loss: 0.0000017671, val loss: 0.0002434812\n",
            "[INFO] epoch: 39, train loss: 0.0000017581, val loss: 0.0002471803\n",
            "[INFO] epoch: 40, train loss: 0.0000017493, val loss: 0.0002508420\n",
            "[INFO] epoch: 41, train loss: 0.0000017407, val loss: 0.0002544613\n",
            "[INFO] epoch: 42, train loss: 0.0000017324, val loss: 0.0002580420\n",
            "[INFO] epoch: 43, train loss: 0.0000017243, val loss: 0.0002615810\n",
            "[INFO] epoch: 44, train loss: 0.0000017163, val loss: 0.0002650792\n",
            "[INFO] epoch: 45, train loss: 0.0000017086, val loss: 0.0002685360\n",
            "[INFO] epoch: 46, train loss: 0.0000017010, val loss: 0.0002719522\n",
            "[INFO] epoch: 47, train loss: 0.0000016937, val loss: 0.0002753256\n",
            "[INFO] epoch: 48, train loss: 0.0000016864, val loss: 0.0002786612\n",
            "[INFO] epoch: 49, train loss: 0.0000016794, val loss: 0.0002819513\n",
            "[INFO] epoch: 50, train loss: 0.0000016725, val loss: 0.0002852059\n",
            "[INFO] epoch: 51, train loss: 0.0000016657, val loss: 0.0002884151\n",
            "[INFO] epoch: 52, train loss: 0.0000016591, val loss: 0.0002915889\n",
            "[INFO] epoch: 53, train loss: 0.0000016526, val loss: 0.0002947181\n",
            "[INFO] epoch: 54, train loss: 0.0000016462, val loss: 0.0002978113\n",
            "[INFO] epoch: 55, train loss: 0.0000016400, val loss: 0.0003008614\n",
            "[INFO] epoch: 56, train loss: 0.0000016339, val loss: 0.0003038799\n",
            "[INFO] epoch: 57, train loss: 0.0000016279, val loss: 0.0003068490\n",
            "[INFO] epoch: 58, train loss: 0.0000016220, val loss: 0.0003097906\n",
            "[INFO] epoch: 59, train loss: 0.0000016162, val loss: 0.0003126836\n",
            "[INFO] epoch: 60, train loss: 0.0000016106, val loss: 0.0003155529\n",
            "[INFO] epoch: 61, train loss: 0.0000016050, val loss: 0.0003183673\n",
            "[INFO] epoch: 62, train loss: 0.0000015995, val loss: 0.0003211665\n",
            "[INFO] epoch: 63, train loss: 0.0000015942, val loss: 0.0003239041\n",
            "[INFO] epoch: 64, train loss: 0.0000015889, val loss: 0.0003266346\n",
            "[INFO] epoch: 65, train loss: 0.0000015837, val loss: 0.0003292941\n",
            "[INFO] epoch: 66, train loss: 0.0000015787, val loss: 0.0003319634\n",
            "[INFO] epoch: 67, train loss: 0.0000015736, val loss: 0.0003345393\n",
            "[INFO] epoch: 68, train loss: 0.0000015688, val loss: 0.0003371580\n",
            "[INFO] epoch: 69, train loss: 0.0000015638, val loss: 0.0003396356\n",
            "[INFO] epoch: 70, train loss: 0.0000015592, val loss: 0.0003422299\n",
            "[INFO] epoch: 71, train loss: 0.0000015543, val loss: 0.0003445805\n",
            "[INFO] epoch: 72, train loss: 0.0000015501, val loss: 0.0003471940\n",
            "[INFO] epoch: 73, train loss: 0.0000015450, val loss: 0.0003493455\n",
            "[INFO] epoch: 74, train loss: 0.0000015414, val loss: 0.0003520964\n",
            "[INFO] epoch: 75, train loss: 0.0000015359, val loss: 0.0003538730\n",
            "[INFO] epoch: 76, train loss: 0.0000015334, val loss: 0.0003570391\n",
            "[INFO] epoch: 77, train loss: 0.0000015269, val loss: 0.0003580010\n",
            "[INFO] epoch: 78, train loss: 0.0000015278, val loss: 0.0003622942\n",
            "[INFO] epoch: 79, train loss: 0.0000015199, val loss: 0.0003612879\n",
            "[INFO] epoch: 80, train loss: 0.0000015326, val loss: 0.0003686036\n",
            "[INFO] epoch: 81, train loss: 0.0000015308, val loss: 0.0003624878\n",
            "[INFO] epoch: 82, train loss: 0.0000016020, val loss: 0.0003780995\n",
            "[INFO] epoch: 83, train loss: 0.0000016935, val loss: 0.0003579183\n",
            "[INFO] epoch: 84, train loss: 0.0000021303, val loss: 0.0003973475\n",
            "[INFO] epoch: 85, train loss: 0.0000029981, val loss: 0.0003359436\n",
            "[INFO] epoch: 86, train loss: 0.0000053177, val loss: 0.0004475963\n",
            "[INFO] epoch: 87, train loss: 0.0000086209, val loss: 0.0002754412\n",
            "[INFO] epoch: 88, train loss: 0.0000108426, val loss: 0.0005222316\n",
            "[INFO] epoch: 89, train loss: 0.0000088021, val loss: 0.0002535541\n",
            "[INFO] epoch: 90, train loss: 0.0000051727, val loss: 0.0004676366\n",
            "[INFO] epoch: 91, train loss: 0.0000024850, val loss: 0.0003204248\n",
            "[INFO] epoch: 92, train loss: 0.0000018835, val loss: 0.0003995148\n",
            "[INFO] epoch: 93, train loss: 0.0000015055, val loss: 0.0003616458\n",
            "[INFO] epoch: 94, train loss: 0.0000014860, val loss: 0.0003803794\n",
            "[INFO] epoch: 95, train loss: 0.0000014628, val loss: 0.0003770934\n",
            "[INFO] epoch: 96, train loss: 0.0000014584, val loss: 0.0003788845\n",
            "[INFO] epoch: 97, train loss: 0.0000014594, val loss: 0.0003825875\n",
            "[INFO] epoch: 98, train loss: 0.0000014530, val loss: 0.0003827131\n",
            "[INFO] epoch: 99, train loss: 0.0000014531, val loss: 0.0003857452\n",
            "[INFO] epoch: 100, train loss: 0.0000014485, val loss: 0.0003867648\n",
            "[INFO] epoch: 101, train loss: 0.0000014472, val loss: 0.0003889912\n",
            "[INFO] epoch: 102, train loss: 0.0000014439, val loss: 0.0003903917\n",
            "[INFO] epoch: 103, train loss: 0.0000014419, val loss: 0.0003922653\n",
            "[INFO] epoch: 104, train loss: 0.0000014392, val loss: 0.0003937802\n",
            "[INFO] epoch: 105, train loss: 0.0000014369, val loss: 0.0003954732\n",
            "[INFO] epoch: 106, train loss: 0.0000014343, val loss: 0.0003969958\n",
            "[INFO] epoch: 107, train loss: 0.0000014320, val loss: 0.0003985830\n",
            "[INFO] epoch: 108, train loss: 0.0000014296, val loss: 0.0004000742\n",
            "[INFO] epoch: 109, train loss: 0.0000014273, val loss: 0.0004015895\n",
            "[INFO] epoch: 110, train loss: 0.0000014249, val loss: 0.0004030360\n",
            "[INFO] epoch: 111, train loss: 0.0000014226, val loss: 0.0004044901\n",
            "[INFO] epoch: 112, train loss: 0.0000014202, val loss: 0.0004058937\n",
            "[INFO] epoch: 113, train loss: 0.0000014179, val loss: 0.0004072941\n",
            "[INFO] epoch: 114, train loss: 0.0000014157, val loss: 0.0004086515\n",
            "[INFO] epoch: 115, train loss: 0.0000014134, val loss: 0.0004100057\n",
            "[INFO] epoch: 116, train loss: 0.0000014111, val loss: 0.0004113210\n",
            "[INFO] epoch: 117, train loss: 0.0000014089, val loss: 0.0004126291\n",
            "[INFO] epoch: 118, train loss: 0.0000014067, val loss: 0.0004139017\n",
            "[INFO] epoch: 119, train loss: 0.0000014045, val loss: 0.0004151725\n",
            "[INFO] epoch: 120, train loss: 0.0000014023, val loss: 0.0004164035\n",
            "[INFO] epoch: 121, train loss: 0.0000014001, val loss: 0.0004176346\n",
            "[INFO] epoch: 122, train loss: 0.0000013979, val loss: 0.0004188306\n",
            "[INFO] epoch: 123, train loss: 0.0000013958, val loss: 0.0004200253\n",
            "[INFO] epoch: 124, train loss: 0.0000013936, val loss: 0.0004211822\n",
            "[INFO] epoch: 125, train loss: 0.0000013915, val loss: 0.0004223455\n",
            "[INFO] epoch: 126, train loss: 0.0000013894, val loss: 0.0004234686\n",
            "[INFO] epoch: 127, train loss: 0.0000013873, val loss: 0.0004245984\n",
            "[INFO] epoch: 128, train loss: 0.0000013852, val loss: 0.0004256878\n",
            "[INFO] epoch: 129, train loss: 0.0000013831, val loss: 0.0004267899\n",
            "[INFO] epoch: 130, train loss: 0.0000013810, val loss: 0.0004278428\n",
            "[INFO] epoch: 131, train loss: 0.0000013790, val loss: 0.0004289222\n",
            "[INFO] epoch: 132, train loss: 0.0000013769, val loss: 0.0004299374\n",
            "[INFO] epoch: 133, train loss: 0.0000013749, val loss: 0.0004309980\n",
            "[INFO] epoch: 134, train loss: 0.0000013729, val loss: 0.0004319737\n",
            "[INFO] epoch: 135, train loss: 0.0000013709, val loss: 0.0004330208\n",
            "[INFO] epoch: 136, train loss: 0.0000013688, val loss: 0.0004339487\n",
            "[INFO] epoch: 137, train loss: 0.0000013670, val loss: 0.0004349980\n",
            "[INFO] epoch: 138, train loss: 0.0000013648, val loss: 0.0004358597\n",
            "[INFO] epoch: 139, train loss: 0.0000013631, val loss: 0.0004369369\n",
            "[INFO] epoch: 140, train loss: 0.0000013609, val loss: 0.0004377026\n",
            "[INFO] epoch: 141, train loss: 0.0000013593, val loss: 0.0004388523\n",
            "[INFO] epoch: 142, train loss: 0.0000013569, val loss: 0.0004394575\n",
            "[INFO] epoch: 143, train loss: 0.0000013556, val loss: 0.0004407732\n",
            "[INFO] epoch: 144, train loss: 0.0000013529, val loss: 0.0004410850\n",
            "[INFO] epoch: 145, train loss: 0.0000013522, val loss: 0.0004427638\n",
            "[INFO] epoch: 146, train loss: 0.0000013489, val loss: 0.0004424953\n",
            "[INFO] epoch: 147, train loss: 0.0000013495, val loss: 0.0004449622\n",
            "[INFO] epoch: 148, train loss: 0.0000013452, val loss: 0.0004434825\n",
            "[INFO] epoch: 149, train loss: 0.0000013490, val loss: 0.0004476848\n",
            "[INFO] epoch: 150, train loss: 0.0000013437, val loss: 0.0004435637\n",
            "[INFO] epoch: 151, train loss: 0.0000013580, val loss: 0.0004516881\n",
            "[INFO] epoch: 152, train loss: 0.0000013578, val loss: 0.0004415596\n",
            "[INFO] epoch: 153, train loss: 0.0000014130, val loss: 0.0004588341\n",
            "[INFO] epoch: 154, train loss: 0.0000014665, val loss: 0.0004345233\n",
            "[INFO] epoch: 155, train loss: 0.0000017186, val loss: 0.0004738697\n",
            "[INFO] epoch: 156, train loss: 0.0000021421, val loss: 0.0004148489\n",
            "[INFO] epoch: 157, train loss: 0.0000033706, val loss: 0.0005094102\n",
            "[INFO] epoch: 158, train loss: 0.0000055779, val loss: 0.0003648557\n",
            "[INFO] epoch: 159, train loss: 0.0000092223, val loss: 0.0005858293\n",
            "[INFO] epoch: 160, train loss: 0.0000118467, val loss: 0.0002976355\n",
            "[INFO] epoch: 161, train loss: 0.0000103513, val loss: 0.0006084509\n",
            "[INFO] epoch: 162, train loss: 0.0000062141, val loss: 0.0003275026\n",
            "[INFO] epoch: 163, train loss: 0.0000034665, val loss: 0.0005151780\n",
            "[INFO] epoch: 164, train loss: 0.0000018469, val loss: 0.0003956474\n",
            "[INFO] epoch: 165, train loss: 0.0000015594, val loss: 0.0004606038\n",
            "[INFO] epoch: 166, train loss: 0.0000013312, val loss: 0.0004281833\n",
            "[INFO] epoch: 167, train loss: 0.0000013254, val loss: 0.0004445412\n",
            "[INFO] epoch: 168, train loss: 0.0000013078, val loss: 0.0004404471\n",
            "[INFO] epoch: 169, train loss: 0.0000013080, val loss: 0.0004425370\n",
            "[INFO] epoch: 170, train loss: 0.0000013072, val loss: 0.0004444099\n",
            "[INFO] epoch: 171, train loss: 0.0000013043, val loss: 0.0004447084\n",
            "[INFO] epoch: 172, train loss: 0.0000013043, val loss: 0.0004465137\n",
            "[INFO] epoch: 173, train loss: 0.0000013019, val loss: 0.0004471216\n",
            "[INFO] epoch: 174, train loss: 0.0000013013, val loss: 0.0004485041\n",
            "[INFO] epoch: 175, train loss: 0.0000012995, val loss: 0.0004492941\n",
            "[INFO] epoch: 176, train loss: 0.0000012985, val loss: 0.0004504297\n",
            "[INFO] epoch: 177, train loss: 0.0000012970, val loss: 0.0004512801\n",
            "[INFO] epoch: 178, train loss: 0.0000012958, val loss: 0.0004522734\n",
            "[INFO] epoch: 179, train loss: 0.0000012944, val loss: 0.0004531209\n",
            "[INFO] epoch: 180, train loss: 0.0000012932, val loss: 0.0004540244\n",
            "[INFO] epoch: 181, train loss: 0.0000012919, val loss: 0.0004548438\n",
            "[INFO] epoch: 182, train loss: 0.0000012907, val loss: 0.0004556838\n",
            "[INFO] epoch: 183, train loss: 0.0000012894, val loss: 0.0004564674\n",
            "[INFO] epoch: 184, train loss: 0.0000012881, val loss: 0.0004572573\n",
            "[INFO] epoch: 185, train loss: 0.0000012869, val loss: 0.0004580031\n",
            "[INFO] epoch: 186, train loss: 0.0000012856, val loss: 0.0004587499\n",
            "[INFO] epoch: 187, train loss: 0.0000012844, val loss: 0.0004594606\n",
            "[INFO] epoch: 188, train loss: 0.0000012832, val loss: 0.0004601704\n",
            "[INFO] epoch: 189, train loss: 0.0000012819, val loss: 0.0004608478\n",
            "[INFO] epoch: 190, train loss: 0.0000012807, val loss: 0.0004615230\n",
            "[INFO] epoch: 191, train loss: 0.0000012795, val loss: 0.0004621708\n",
            "[INFO] epoch: 192, train loss: 0.0000012783, val loss: 0.0004628193\n",
            "[INFO] epoch: 193, train loss: 0.0000012770, val loss: 0.0004634369\n",
            "[INFO] epoch: 194, train loss: 0.0000012758, val loss: 0.0004640582\n",
            "[INFO] epoch: 195, train loss: 0.0000012746, val loss: 0.0004646513\n",
            "[INFO] epoch: 196, train loss: 0.0000012734, val loss: 0.0004652494\n",
            "[INFO] epoch: 197, train loss: 0.0000012722, val loss: 0.0004658172\n",
            "[INFO] epoch: 198, train loss: 0.0000012710, val loss: 0.0004663936\n",
            "[INFO] epoch: 199, train loss: 0.0000012698, val loss: 0.0004669387\n",
            "[INFO] epoch: 200, train loss: 0.0000012687, val loss: 0.0004674950\n",
            "[INFO] epoch: 201, train loss: 0.0000012675, val loss: 0.0004680181\n",
            "[INFO] epoch: 202, train loss: 0.0000012663, val loss: 0.0004685578\n",
            "[INFO] epoch: 203, train loss: 0.0000012651, val loss: 0.0004690604\n",
            "[INFO] epoch: 204, train loss: 0.0000012639, val loss: 0.0004695848\n",
            "[INFO] epoch: 205, train loss: 0.0000012628, val loss: 0.0004700670\n",
            "[INFO] epoch: 206, train loss: 0.0000012616, val loss: 0.0004705786\n",
            "[INFO] epoch: 207, train loss: 0.0000012604, val loss: 0.0004710388\n",
            "[INFO] epoch: 208, train loss: 0.0000012593, val loss: 0.0004715440\n",
            "[INFO] epoch: 209, train loss: 0.0000012581, val loss: 0.0004719769\n",
            "[INFO] epoch: 210, train loss: 0.0000012570, val loss: 0.0004724800\n",
            "[INFO] epoch: 211, train loss: 0.0000012558, val loss: 0.0004728832\n",
            "[INFO] epoch: 212, train loss: 0.0000012547, val loss: 0.0004733929\n",
            "[INFO] epoch: 213, train loss: 0.0000012535, val loss: 0.0004737582\n",
            "[INFO] epoch: 214, train loss: 0.0000012525, val loss: 0.0004742854\n",
            "[INFO] epoch: 215, train loss: 0.0000012512, val loss: 0.0004745953\n",
            "[INFO] epoch: 216, train loss: 0.0000012502, val loss: 0.0004751656\n",
            "[INFO] epoch: 217, train loss: 0.0000012489, val loss: 0.0004753910\n",
            "[INFO] epoch: 218, train loss: 0.0000012481, val loss: 0.0004760478\n",
            "[INFO] epoch: 219, train loss: 0.0000012465, val loss: 0.0004761245\n",
            "[INFO] epoch: 220, train loss: 0.0000012460, val loss: 0.0004769553\n",
            "[INFO] epoch: 221, train loss: 0.0000012442, val loss: 0.0004767682\n",
            "[INFO] epoch: 222, train loss: 0.0000012441, val loss: 0.0004779407\n",
            "[INFO] epoch: 223, train loss: 0.0000012418, val loss: 0.0004772485\n",
            "[INFO] epoch: 224, train loss: 0.0000012426, val loss: 0.0004791045\n",
            "[INFO] epoch: 225, train loss: 0.0000012395, val loss: 0.0004774171\n",
            "[INFO] epoch: 226, train loss: 0.0000012425, val loss: 0.0004806722\n",
            "[INFO] epoch: 227, train loss: 0.0000012384, val loss: 0.0004769419\n",
            "[INFO] epoch: 228, train loss: 0.0000012473, val loss: 0.0004831416\n",
            "[INFO] epoch: 229, train loss: 0.0000012441, val loss: 0.0004750723\n",
            "[INFO] epoch: 230, train loss: 0.0000012727, val loss: 0.0004876616\n",
            "[INFO] epoch: 231, train loss: 0.0000012875, val loss: 0.0004700480\n",
            "[INFO] epoch: 232, train loss: 0.0000013952, val loss: 0.0004969439\n",
            "[INFO] epoch: 233, train loss: 0.0000015331, val loss: 0.0004576792\n",
            "[INFO] epoch: 234, train loss: 0.0000020010, val loss: 0.0005175731\n",
            "[INFO] epoch: 235, train loss: 0.0000028161, val loss: 0.0004279089\n",
            "[INFO] epoch: 236, train loss: 0.0000047392, val loss: 0.0005649694\n",
            "[INFO] epoch: 237, train loss: 0.0000076834, val loss: 0.0003652130\n",
            "[INFO] epoch: 238, train loss: 0.0000108393, val loss: 0.0006405885\n",
            "[INFO] epoch: 239, train loss: 0.0000113610, val loss: 0.0003174189\n",
            "[INFO] epoch: 240, train loss: 0.0000083330, val loss: 0.0006190718\n",
            "[INFO] epoch: 241, train loss: 0.0000045192, val loss: 0.0003686447\n",
            "[INFO] epoch: 242, train loss: 0.0000027030, val loss: 0.0005289335\n",
            "[INFO] epoch: 243, train loss: 0.0000015931, val loss: 0.0004272970\n",
            "[INFO] epoch: 244, train loss: 0.0000014087, val loss: 0.0004853570\n",
            "[INFO] epoch: 245, train loss: 0.0000012321, val loss: 0.0004545765\n",
            "[INFO] epoch: 246, train loss: 0.0000012318, val loss: 0.0004713368\n",
            "[INFO] epoch: 247, train loss: 0.0000012113, val loss: 0.0004654203\n",
            "[INFO] epoch: 248, train loss: 0.0000012148, val loss: 0.0004689583\n",
            "[INFO] epoch: 249, train loss: 0.0000012117, val loss: 0.0004691938\n",
            "[INFO] epoch: 250, train loss: 0.0000012113, val loss: 0.0004701968\n",
            "[INFO] epoch: 251, train loss: 0.0000012106, val loss: 0.0004711549\n",
            "[INFO] epoch: 252, train loss: 0.0000012094, val loss: 0.0004718700\n",
            "[INFO] epoch: 253, train loss: 0.0000012088, val loss: 0.0004727871\n",
            "[INFO] epoch: 254, train loss: 0.0000012078, val loss: 0.0004734545\n",
            "[INFO] epoch: 255, train loss: 0.0000012071, val loss: 0.0004742640\n",
            "[INFO] epoch: 256, train loss: 0.0000012062, val loss: 0.0004749130\n",
            "[INFO] epoch: 257, train loss: 0.0000012055, val loss: 0.0004756273\n",
            "[INFO] epoch: 258, train loss: 0.0000012046, val loss: 0.0004762483\n",
            "[INFO] epoch: 259, train loss: 0.0000012038, val loss: 0.0004768929\n",
            "[INFO] epoch: 260, train loss: 0.0000012030, val loss: 0.0004774743\n",
            "[INFO] epoch: 261, train loss: 0.0000012023, val loss: 0.0004780656\n",
            "[INFO] epoch: 262, train loss: 0.0000012014, val loss: 0.0004786104\n",
            "[INFO] epoch: 263, train loss: 0.0000012007, val loss: 0.0004791541\n",
            "[INFO] epoch: 264, train loss: 0.0000011999, val loss: 0.0004796650\n",
            "[INFO] epoch: 265, train loss: 0.0000011991, val loss: 0.0004801690\n",
            "[INFO] epoch: 266, train loss: 0.0000011983, val loss: 0.0004806447\n",
            "[INFO] epoch: 267, train loss: 0.0000011975, val loss: 0.0004811165\n",
            "[INFO] epoch: 268, train loss: 0.0000011968, val loss: 0.0004815632\n",
            "[INFO] epoch: 269, train loss: 0.0000011960, val loss: 0.0004820037\n",
            "[INFO] epoch: 270, train loss: 0.0000011952, val loss: 0.0004824214\n",
            "[INFO] epoch: 271, train loss: 0.0000011944, val loss: 0.0004828348\n",
            "[INFO] epoch: 272, train loss: 0.0000011937, val loss: 0.0004832298\n",
            "[INFO] epoch: 273, train loss: 0.0000011929, val loss: 0.0004836172\n",
            "[INFO] epoch: 274, train loss: 0.0000011921, val loss: 0.0004839929\n",
            "[INFO] epoch: 275, train loss: 0.0000011914, val loss: 0.0004843562\n",
            "[INFO] epoch: 276, train loss: 0.0000011906, val loss: 0.0004847119\n",
            "[INFO] epoch: 277, train loss: 0.0000011898, val loss: 0.0004850518\n",
            "[INFO] epoch: 278, train loss: 0.0000011891, val loss: 0.0004853938\n",
            "[INFO] epoch: 279, train loss: 0.0000011883, val loss: 0.0004857163\n",
            "[INFO] epoch: 280, train loss: 0.0000011875, val loss: 0.0004860381\n",
            "[INFO] epoch: 281, train loss: 0.0000011868, val loss: 0.0004863430\n",
            "[INFO] epoch: 282, train loss: 0.0000011860, val loss: 0.0004866542\n",
            "[INFO] epoch: 283, train loss: 0.0000011853, val loss: 0.0004869405\n",
            "[INFO] epoch: 284, train loss: 0.0000011845, val loss: 0.0004872393\n",
            "[INFO] epoch: 285, train loss: 0.0000011837, val loss: 0.0004875093\n",
            "[INFO] epoch: 286, train loss: 0.0000011830, val loss: 0.0004877983\n",
            "[INFO] epoch: 287, train loss: 0.0000011822, val loss: 0.0004880532\n",
            "[INFO] epoch: 288, train loss: 0.0000011815, val loss: 0.0004883327\n",
            "[INFO] epoch: 289, train loss: 0.0000011807, val loss: 0.0004885690\n",
            "[INFO] epoch: 290, train loss: 0.0000011800, val loss: 0.0004888429\n",
            "[INFO] epoch: 291, train loss: 0.0000011792, val loss: 0.0004890632\n",
            "[INFO] epoch: 292, train loss: 0.0000011785, val loss: 0.0004893374\n",
            "[INFO] epoch: 293, train loss: 0.0000011777, val loss: 0.0004895333\n",
            "[INFO] epoch: 294, train loss: 0.0000011770, val loss: 0.0004898132\n",
            "[INFO] epoch: 295, train loss: 0.0000011762, val loss: 0.0004899742\n",
            "[INFO] epoch: 296, train loss: 0.0000011756, val loss: 0.0004902783\n",
            "[INFO] epoch: 297, train loss: 0.0000011747, val loss: 0.0004903896\n",
            "[INFO] epoch: 298, train loss: 0.0000011741, val loss: 0.0004907391\n",
            "[INFO] epoch: 299, train loss: 0.0000011732, val loss: 0.0004907657\n",
            "[INFO] epoch: 300, train loss: 0.0000011727, val loss: 0.0004912106\n",
            "[INFO] epoch: 301, train loss: 0.0000011716, val loss: 0.0004910861\n",
            "[INFO] epoch: 302, train loss: 0.0000011714, val loss: 0.0004917163\n",
            "[INFO] epoch: 303, train loss: 0.0000011700, val loss: 0.0004913197\n",
            "[INFO] epoch: 304, train loss: 0.0000011702, val loss: 0.0004923121\n",
            "[INFO] epoch: 305, train loss: 0.0000011684, val loss: 0.0004913848\n",
            "[INFO] epoch: 306, train loss: 0.0000011695, val loss: 0.0004931138\n",
            "[INFO] epoch: 307, train loss: 0.0000011669, val loss: 0.0004911119\n",
            "[INFO] epoch: 308, train loss: 0.0000011704, val loss: 0.0004943709\n",
            "[INFO] epoch: 309, train loss: 0.0000011669, val loss: 0.0004901272\n",
            "[INFO] epoch: 310, train loss: 0.0000011769, val loss: 0.0004966589\n",
            "[INFO] epoch: 311, train loss: 0.0000011756, val loss: 0.0004875625\n",
            "[INFO] epoch: 312, train loss: 0.0000012086, val loss: 0.0005012982\n",
            "[INFO] epoch: 313, train loss: 0.0000012322, val loss: 0.0004813851\n",
            "[INFO] epoch: 314, train loss: 0.0000013626, val loss: 0.0005114477\n",
            "[INFO] epoch: 315, train loss: 0.0000015507, val loss: 0.0004666765\n",
            "[INFO] epoch: 316, train loss: 0.0000021384, val loss: 0.0005348670\n",
            "[INFO] epoch: 317, train loss: 0.0000032101, val loss: 0.0004316216\n",
            "[INFO] epoch: 318, train loss: 0.0000055762, val loss: 0.0005893670\n",
            "[INFO] epoch: 319, train loss: 0.0000090259, val loss: 0.0003613647\n",
            "[INFO] epoch: 320, train loss: 0.0000118851, val loss: 0.0006652371\n",
            "[INFO] epoch: 321, train loss: 0.0000113511, val loss: 0.0003232513\n",
            "[INFO] epoch: 322, train loss: 0.0000075840, val loss: 0.0006216186\n",
            "[INFO] epoch: 323, train loss: 0.0000038401, val loss: 0.0003856442\n",
            "[INFO] epoch: 324, train loss: 0.0000023406, val loss: 0.0005310013\n",
            "[INFO] epoch: 325, train loss: 0.0000014357, val loss: 0.0004409499\n",
            "[INFO] epoch: 326, train loss: 0.0000012999, val loss: 0.0004922857\n",
            "[INFO] epoch: 327, train loss: 0.0000011597, val loss: 0.0004654412\n",
            "[INFO] epoch: 328, train loss: 0.0000011613, val loss: 0.0004799344\n",
            "[INFO] epoch: 329, train loss: 0.0000011454, val loss: 0.0004750662\n",
            "[INFO] epoch: 330, train loss: 0.0000011484, val loss: 0.0004780051\n",
            "[INFO] epoch: 331, train loss: 0.0000011461, val loss: 0.0004783350\n",
            "[INFO] epoch: 332, train loss: 0.0000011457, val loss: 0.0004791902\n",
            "[INFO] epoch: 333, train loss: 0.0000011453, val loss: 0.0004800590\n",
            "[INFO] epoch: 334, train loss: 0.0000011444, val loss: 0.0004806826\n",
            "[INFO] epoch: 335, train loss: 0.0000011441, val loss: 0.0004814909\n",
            "[INFO] epoch: 336, train loss: 0.0000011433, val loss: 0.0004820701\n",
            "[INFO] epoch: 337, train loss: 0.0000011429, val loss: 0.0004827719\n",
            "[INFO] epoch: 338, train loss: 0.0000011422, val loss: 0.0004833264\n",
            "[INFO] epoch: 339, train loss: 0.0000011417, val loss: 0.0004839377\n",
            "[INFO] epoch: 340, train loss: 0.0000011411, val loss: 0.0004844575\n",
            "[INFO] epoch: 341, train loss: 0.0000011406, val loss: 0.0004850012\n",
            "[INFO] epoch: 342, train loss: 0.0000011400, val loss: 0.0004854816\n",
            "[INFO] epoch: 343, train loss: 0.0000011395, val loss: 0.0004859696\n",
            "[INFO] epoch: 344, train loss: 0.0000011389, val loss: 0.0004864115\n",
            "[INFO] epoch: 345, train loss: 0.0000011384, val loss: 0.0004868552\n",
            "[INFO] epoch: 346, train loss: 0.0000011378, val loss: 0.0004872602\n",
            "[INFO] epoch: 347, train loss: 0.0000011373, val loss: 0.0004876654\n",
            "[INFO] epoch: 348, train loss: 0.0000011367, val loss: 0.0004880362\n",
            "[INFO] epoch: 349, train loss: 0.0000011362, val loss: 0.0004884067\n",
            "[INFO] epoch: 350, train loss: 0.0000011357, val loss: 0.0004887466\n",
            "[INFO] epoch: 351, train loss: 0.0000011351, val loss: 0.0004890879\n",
            "[INFO] epoch: 352, train loss: 0.0000011346, val loss: 0.0004893998\n",
            "[INFO] epoch: 353, train loss: 0.0000011341, val loss: 0.0004897150\n",
            "[INFO] epoch: 354, train loss: 0.0000011335, val loss: 0.0004900003\n",
            "[INFO] epoch: 355, train loss: 0.0000011330, val loss: 0.0004902941\n",
            "[INFO] epoch: 356, train loss: 0.0000011324, val loss: 0.0004905555\n",
            "[INFO] epoch: 357, train loss: 0.0000011319, val loss: 0.0004908289\n",
            "[INFO] epoch: 358, train loss: 0.0000011314, val loss: 0.0004910681\n",
            "[INFO] epoch: 359, train loss: 0.0000011308, val loss: 0.0004913254\n",
            "[INFO] epoch: 360, train loss: 0.0000011303, val loss: 0.0004915446\n",
            "[INFO] epoch: 361, train loss: 0.0000011298, val loss: 0.0004917850\n",
            "[INFO] epoch: 362, train loss: 0.0000011292, val loss: 0.0004919841\n",
            "[INFO] epoch: 363, train loss: 0.0000011287, val loss: 0.0004922146\n",
            "[INFO] epoch: 364, train loss: 0.0000011281, val loss: 0.0004923940\n",
            "[INFO] epoch: 365, train loss: 0.0000011276, val loss: 0.0004926131\n",
            "[INFO] epoch: 366, train loss: 0.0000011270, val loss: 0.0004927742\n",
            "[INFO] epoch: 367, train loss: 0.0000011265, val loss: 0.0004929840\n",
            "[INFO] epoch: 368, train loss: 0.0000011260, val loss: 0.0004931293\n",
            "[INFO] epoch: 369, train loss: 0.0000011255, val loss: 0.0004933344\n",
            "[INFO] epoch: 370, train loss: 0.0000011249, val loss: 0.0004934568\n",
            "[INFO] epoch: 371, train loss: 0.0000011244, val loss: 0.0004936615\n",
            "[INFO] epoch: 372, train loss: 0.0000011238, val loss: 0.0004937593\n",
            "[INFO] epoch: 373, train loss: 0.0000011233, val loss: 0.0004939734\n",
            "[INFO] epoch: 374, train loss: 0.0000011227, val loss: 0.0004940359\n",
            "[INFO] epoch: 375, train loss: 0.0000011223, val loss: 0.0004942723\n",
            "[INFO] epoch: 376, train loss: 0.0000011216, val loss: 0.0004942847\n",
            "[INFO] epoch: 377, train loss: 0.0000011213, val loss: 0.0004945633\n",
            "[INFO] epoch: 378, train loss: 0.0000011205, val loss: 0.0004945009\n",
            "[INFO] epoch: 379, train loss: 0.0000011203, val loss: 0.0004948563\n",
            "[INFO] epoch: 380, train loss: 0.0000011194, val loss: 0.0004946712\n",
            "[INFO] epoch: 381, train loss: 0.0000011193, val loss: 0.0004951710\n",
            "[INFO] epoch: 382, train loss: 0.0000011182, val loss: 0.0004947757\n",
            "[INFO] epoch: 383, train loss: 0.0000011184, val loss: 0.0004955426\n",
            "[INFO] epoch: 384, train loss: 0.0000011170, val loss: 0.0004947650\n",
            "[INFO] epoch: 385, train loss: 0.0000011178, val loss: 0.0004960362\n",
            "[INFO] epoch: 386, train loss: 0.0000011158, val loss: 0.0004945460\n",
            "[INFO] epoch: 387, train loss: 0.0000011179, val loss: 0.0004967929\n",
            "[INFO] epoch: 388, train loss: 0.0000011150, val loss: 0.0004939147\n",
            "[INFO] epoch: 389, train loss: 0.0000011202, val loss: 0.0004981057\n",
            "[INFO] epoch: 390, train loss: 0.0000011167, val loss: 0.0004924498\n",
            "[INFO] epoch: 391, train loss: 0.0000011303, val loss: 0.0005006082\n",
            "[INFO] epoch: 392, train loss: 0.0000011308, val loss: 0.0004891962\n",
            "[INFO] epoch: 393, train loss: 0.0000011731, val loss: 0.0005057267\n",
            "[INFO] epoch: 394, train loss: 0.0000012063, val loss: 0.0004820186\n",
            "[INFO] epoch: 395, train loss: 0.0000013623, val loss: 0.0005167074\n",
            "[INFO] epoch: 396, train loss: 0.0000015836, val loss: 0.0004659809\n",
            "[INFO] epoch: 397, train loss: 0.0000022254, val loss: 0.0005411065\n",
            "[INFO] epoch: 398, train loss: 0.0000033333, val loss: 0.0004301431\n",
            "[INFO] epoch: 399, train loss: 0.0000056074, val loss: 0.0005941182\n",
            "[INFO] epoch: 400, train loss: 0.0000086652, val loss: 0.0003646013\n",
            "[INFO] epoch: 401, train loss: 0.0000109926, val loss: 0.0006596646\n",
            "[INFO] epoch: 402, train loss: 0.0000103813, val loss: 0.0003325473\n",
            "[INFO] epoch: 403, train loss: 0.0000071824, val loss: 0.0006190774\n",
            "[INFO] epoch: 404, train loss: 0.0000038419, val loss: 0.0003881154\n",
            "[INFO] epoch: 405, train loss: 0.0000024043, val loss: 0.0005367343\n",
            "[INFO] epoch: 406, train loss: 0.0000014654, val loss: 0.0004398995\n",
            "[INFO] epoch: 407, train loss: 0.0000013022, val loss: 0.0004983787\n",
            "[INFO] epoch: 408, train loss: 0.0000011261, val loss: 0.0004646777\n",
            "[INFO] epoch: 409, train loss: 0.0000011251, val loss: 0.0004846203\n",
            "[INFO] epoch: 410, train loss: 0.0000010965, val loss: 0.0004754103\n",
            "[INFO] epoch: 411, train loss: 0.0000011030, val loss: 0.0004812522\n",
            "[INFO] epoch: 412, train loss: 0.0000010967, val loss: 0.0004795179\n",
            "[INFO] epoch: 413, train loss: 0.0000010987, val loss: 0.0004815725\n",
            "[INFO] epoch: 414, train loss: 0.0000010968, val loss: 0.0004815348\n",
            "[INFO] epoch: 415, train loss: 0.0000010971, val loss: 0.0004826282\n",
            "[INFO] epoch: 416, train loss: 0.0000010962, val loss: 0.0004829942\n",
            "[INFO] epoch: 417, train loss: 0.0000010960, val loss: 0.0004837456\n",
            "[INFO] epoch: 418, train loss: 0.0000010954, val loss: 0.0004841992\n",
            "[INFO] epoch: 419, train loss: 0.0000010951, val loss: 0.0004848025\n",
            "[INFO] epoch: 420, train loss: 0.0000010945, val loss: 0.0004852473\n",
            "[INFO] epoch: 421, train loss: 0.0000010942, val loss: 0.0004857632\n",
            "[INFO] epoch: 422, train loss: 0.0000010937, val loss: 0.0004861816\n",
            "[INFO] epoch: 423, train loss: 0.0000010933, val loss: 0.0004866292\n",
            "[INFO] epoch: 424, train loss: 0.0000010929, val loss: 0.0004870166\n",
            "[INFO] epoch: 425, train loss: 0.0000010925, val loss: 0.0004874126\n",
            "[INFO] epoch: 426, train loss: 0.0000010920, val loss: 0.0004877623\n",
            "[INFO] epoch: 427, train loss: 0.0000010917, val loss: 0.0004881214\n",
            "[INFO] epoch: 428, train loss: 0.0000010912, val loss: 0.0004884336\n",
            "[INFO] epoch: 429, train loss: 0.0000010908, val loss: 0.0004887562\n",
            "[INFO] epoch: 430, train loss: 0.0000010904, val loss: 0.0004890426\n",
            "[INFO] epoch: 431, train loss: 0.0000010900, val loss: 0.0004893269\n",
            "[INFO] epoch: 432, train loss: 0.0000010896, val loss: 0.0004895882\n",
            "[INFO] epoch: 433, train loss: 0.0000010892, val loss: 0.0004898464\n",
            "[INFO] epoch: 434, train loss: 0.0000010887, val loss: 0.0004900827\n",
            "[INFO] epoch: 435, train loss: 0.0000010883, val loss: 0.0004903170\n",
            "[INFO] epoch: 436, train loss: 0.0000010879, val loss: 0.0004905287\n",
            "[INFO] epoch: 437, train loss: 0.0000010875, val loss: 0.0004907430\n",
            "[INFO] epoch: 438, train loss: 0.0000010871, val loss: 0.0004909338\n",
            "[INFO] epoch: 439, train loss: 0.0000010867, val loss: 0.0004911297\n",
            "[INFO] epoch: 440, train loss: 0.0000010863, val loss: 0.0004912978\n",
            "[INFO] epoch: 441, train loss: 0.0000010859, val loss: 0.0004914787\n",
            "[INFO] epoch: 442, train loss: 0.0000010854, val loss: 0.0004916308\n",
            "[INFO] epoch: 443, train loss: 0.0000010850, val loss: 0.0004917946\n",
            "[INFO] epoch: 444, train loss: 0.0000010846, val loss: 0.0004919321\n",
            "[INFO] epoch: 445, train loss: 0.0000010842, val loss: 0.0004920829\n",
            "[INFO] epoch: 446, train loss: 0.0000010838, val loss: 0.0004922045\n",
            "[INFO] epoch: 447, train loss: 0.0000010834, val loss: 0.0004923443\n",
            "[INFO] epoch: 448, train loss: 0.0000010829, val loss: 0.0004924507\n",
            "[INFO] epoch: 449, train loss: 0.0000010825, val loss: 0.0004925808\n",
            "[INFO] epoch: 450, train loss: 0.0000010821, val loss: 0.0004926734\n",
            "[INFO] epoch: 451, train loss: 0.0000010817, val loss: 0.0004927962\n",
            "[INFO] epoch: 452, train loss: 0.0000010813, val loss: 0.0004928727\n",
            "[INFO] epoch: 453, train loss: 0.0000010809, val loss: 0.0004929902\n",
            "[INFO] epoch: 454, train loss: 0.0000010804, val loss: 0.0004930517\n",
            "[INFO] epoch: 455, train loss: 0.0000010800, val loss: 0.0004931675\n",
            "[INFO] epoch: 456, train loss: 0.0000010796, val loss: 0.0004932073\n",
            "[INFO] epoch: 457, train loss: 0.0000010792, val loss: 0.0004933286\n",
            "[INFO] epoch: 458, train loss: 0.0000010787, val loss: 0.0004933426\n",
            "[INFO] epoch: 459, train loss: 0.0000010784, val loss: 0.0004934793\n",
            "[INFO] epoch: 460, train loss: 0.0000010779, val loss: 0.0004934532\n",
            "[INFO] epoch: 461, train loss: 0.0000010776, val loss: 0.0004936286\n",
            "[INFO] epoch: 462, train loss: 0.0000010770, val loss: 0.0004935311\n",
            "[INFO] epoch: 463, train loss: 0.0000010768, val loss: 0.0004937843\n",
            "[INFO] epoch: 464, train loss: 0.0000010761, val loss: 0.0004935651\n",
            "[INFO] epoch: 465, train loss: 0.0000010760, val loss: 0.0004939678\n",
            "[INFO] epoch: 466, train loss: 0.0000010752, val loss: 0.0004935260\n",
            "[INFO] epoch: 467, train loss: 0.0000010754, val loss: 0.0004942191\n",
            "[INFO] epoch: 468, train loss: 0.0000010742, val loss: 0.0004933552\n",
            "[INFO] epoch: 469, train loss: 0.0000010751, val loss: 0.0004946327\n",
            "[INFO] epoch: 470, train loss: 0.0000010733, val loss: 0.0004929209\n",
            "[INFO] epoch: 471, train loss: 0.0000010758, val loss: 0.0004953998\n",
            "[INFO] epoch: 472, train loss: 0.0000010732, val loss: 0.0004919377\n",
            "[INFO] epoch: 473, train loss: 0.0000010800, val loss: 0.0004969492\n",
            "[INFO] epoch: 474, train loss: 0.0000010780, val loss: 0.0004897588\n",
            "[INFO] epoch: 475, train loss: 0.0000010990, val loss: 0.0005002650\n",
            "[INFO] epoch: 476, train loss: 0.0000011100, val loss: 0.0004848828\n",
            "[INFO] epoch: 477, train loss: 0.0000011882, val loss: 0.0005076646\n",
            "[INFO] epoch: 478, train loss: 0.0000012888, val loss: 0.0004737241\n",
            "[INFO] epoch: 479, train loss: 0.0000016310, val loss: 0.0005247527\n",
            "[INFO] epoch: 480, train loss: 0.0000022420, val loss: 0.0004476013\n",
            "[INFO] epoch: 481, train loss: 0.0000037534, val loss: 0.0005651611\n",
            "[INFO] epoch: 482, train loss: 0.0000063764, val loss: 0.0003894586\n",
            "[INFO] epoch: 483, train loss: 0.0000100532, val loss: 0.0006431323\n",
            "[INFO] epoch: 484, train loss: 0.0000124389, val loss: 0.0003231293\n",
            "[INFO] epoch: 485, train loss: 0.0000105496, val loss: 0.0006541835\n",
            "[INFO] epoch: 486, train loss: 0.0000064158, val loss: 0.0003561366\n",
            "[INFO] epoch: 487, train loss: 0.0000035714, val loss: 0.0005588472\n",
            "[INFO] epoch: 488, train loss: 0.0000018291, val loss: 0.0004225020\n",
            "[INFO] epoch: 489, train loss: 0.0000014457, val loss: 0.0005026628\n",
            "[INFO] epoch: 490, train loss: 0.0000011288, val loss: 0.0004554464\n",
            "[INFO] epoch: 491, train loss: 0.0000011045, val loss: 0.0004829714\n",
            "[INFO] epoch: 492, train loss: 0.0000010582, val loss: 0.0004698405\n",
            "[INFO] epoch: 493, train loss: 0.0000010640, val loss: 0.0004772519\n",
            "[INFO] epoch: 494, train loss: 0.0000010570, val loss: 0.0004752507\n",
            "[INFO] epoch: 495, train loss: 0.0000010589, val loss: 0.0004771590\n",
            "[INFO] epoch: 496, train loss: 0.0000010574, val loss: 0.0004774150\n",
            "[INFO] epoch: 497, train loss: 0.0000010574, val loss: 0.0004783058\n",
            "[INFO] epoch: 498, train loss: 0.0000010569, val loss: 0.0004788667\n",
            "[INFO] epoch: 499, train loss: 0.0000010565, val loss: 0.0004795051\n",
            "[INFO] epoch: 500, train loss: 0.0000010562, val loss: 0.0004800744\n",
            "[INFO] epoch: 501, train loss: 0.0000010558, val loss: 0.0004806143\n",
            "[INFO] epoch: 502, train loss: 0.0000010555, val loss: 0.0004811281\n",
            "[INFO] epoch: 503, train loss: 0.0000010551, val loss: 0.0004816067\n",
            "[INFO] epoch: 504, train loss: 0.0000010548, val loss: 0.0004820649\n",
            "[INFO] epoch: 505, train loss: 0.0000010544, val loss: 0.0004824918\n",
            "[INFO] epoch: 506, train loss: 0.0000010541, val loss: 0.0004828976\n",
            "[INFO] epoch: 507, train loss: 0.0000010538, val loss: 0.0004832780\n",
            "[INFO] epoch: 508, train loss: 0.0000010534, val loss: 0.0004836411\n",
            "[INFO] epoch: 509, train loss: 0.0000010531, val loss: 0.0004839789\n",
            "[INFO] epoch: 510, train loss: 0.0000010528, val loss: 0.0004843022\n",
            "[INFO] epoch: 511, train loss: 0.0000010524, val loss: 0.0004846053\n",
            "[INFO] epoch: 512, train loss: 0.0000010521, val loss: 0.0004848913\n",
            "[INFO] epoch: 513, train loss: 0.0000010518, val loss: 0.0004851624\n",
            "[INFO] epoch: 514, train loss: 0.0000010515, val loss: 0.0004854197\n",
            "[INFO] epoch: 515, train loss: 0.0000010511, val loss: 0.0004856569\n",
            "[INFO] epoch: 516, train loss: 0.0000010508, val loss: 0.0004858881\n",
            "[INFO] epoch: 517, train loss: 0.0000010505, val loss: 0.0004861028\n",
            "[INFO] epoch: 518, train loss: 0.0000010501, val loss: 0.0004863066\n",
            "[INFO] epoch: 519, train loss: 0.0000010498, val loss: 0.0004864961\n",
            "[INFO] epoch: 520, train loss: 0.0000010495, val loss: 0.0004866809\n",
            "[INFO] epoch: 521, train loss: 0.0000010491, val loss: 0.0004868482\n",
            "[INFO] epoch: 522, train loss: 0.0000010488, val loss: 0.0004870147\n",
            "[INFO] epoch: 523, train loss: 0.0000010485, val loss: 0.0004871598\n",
            "[INFO] epoch: 524, train loss: 0.0000010482, val loss: 0.0004873110\n",
            "[INFO] epoch: 525, train loss: 0.0000010478, val loss: 0.0004874391\n",
            "[INFO] epoch: 526, train loss: 0.0000010475, val loss: 0.0004875746\n",
            "[INFO] epoch: 527, train loss: 0.0000010472, val loss: 0.0004876867\n",
            "[INFO] epoch: 528, train loss: 0.0000010468, val loss: 0.0004878064\n",
            "[INFO] epoch: 529, train loss: 0.0000010465, val loss: 0.0004879044\n",
            "[INFO] epoch: 530, train loss: 0.0000010462, val loss: 0.0004880135\n",
            "[INFO] epoch: 531, train loss: 0.0000010458, val loss: 0.0004880962\n",
            "[INFO] epoch: 532, train loss: 0.0000010455, val loss: 0.0004881968\n",
            "[INFO] epoch: 533, train loss: 0.0000010451, val loss: 0.0004882600\n",
            "[INFO] epoch: 534, train loss: 0.0000010448, val loss: 0.0004883584\n",
            "[INFO] epoch: 535, train loss: 0.0000010445, val loss: 0.0004884037\n",
            "[INFO] epoch: 536, train loss: 0.0000010442, val loss: 0.0004885002\n",
            "[INFO] epoch: 537, train loss: 0.0000010438, val loss: 0.0004885235\n",
            "[INFO] epoch: 538, train loss: 0.0000010435, val loss: 0.0004886235\n",
            "[INFO] epoch: 539, train loss: 0.0000010431, val loss: 0.0004886222\n",
            "[INFO] epoch: 540, train loss: 0.0000010428, val loss: 0.0004887348\n",
            "[INFO] epoch: 541, train loss: 0.0000010424, val loss: 0.0004886963\n",
            "[INFO] epoch: 542, train loss: 0.0000010422, val loss: 0.0004888358\n",
            "[INFO] epoch: 543, train loss: 0.0000010417, val loss: 0.0004887454\n",
            "[INFO] epoch: 544, train loss: 0.0000010415, val loss: 0.0004889364\n",
            "[INFO] epoch: 545, train loss: 0.0000010410, val loss: 0.0004887594\n",
            "[INFO] epoch: 546, train loss: 0.0000010409, val loss: 0.0004890494\n",
            "[INFO] epoch: 547, train loss: 0.0000010402, val loss: 0.0004887208\n",
            "[INFO] epoch: 548, train loss: 0.0000010404, val loss: 0.0004892016\n",
            "[INFO] epoch: 549, train loss: 0.0000010395, val loss: 0.0004885978\n",
            "[INFO] epoch: 550, train loss: 0.0000010400, val loss: 0.0004894362\n",
            "[INFO] epoch: 551, train loss: 0.0000010386, val loss: 0.0004883233\n",
            "[INFO] epoch: 552, train loss: 0.0000010400, val loss: 0.0004898611\n",
            "[INFO] epoch: 553, train loss: 0.0000010380, val loss: 0.0004877480\n",
            "[INFO] epoch: 554, train loss: 0.0000010412, val loss: 0.0004906883\n",
            "[INFO] epoch: 555, train loss: 0.0000010385, val loss: 0.0004865616\n",
            "[INFO] epoch: 556, train loss: 0.0000010469, val loss: 0.0004923821\n",
            "[INFO] epoch: 557, train loss: 0.0000010456, val loss: 0.0004840681\n",
            "[INFO] epoch: 558, train loss: 0.0000010707, val loss: 0.0004959926\n",
            "[INFO] epoch: 559, train loss: 0.0000010860, val loss: 0.0004786830\n",
            "[INFO] epoch: 560, train loss: 0.0000011769, val loss: 0.0005039277\n",
            "[INFO] epoch: 561, train loss: 0.0000012959, val loss: 0.0004667302\n",
            "[INFO] epoch: 562, train loss: 0.0000016770, val loss: 0.0005218650\n",
            "[INFO] epoch: 563, train loss: 0.0000023444, val loss: 0.0004396118\n",
            "[INFO] epoch: 564, train loss: 0.0000039158, val loss: 0.0005629226\n",
            "[INFO] epoch: 565, train loss: 0.0000065129, val loss: 0.0003822275\n",
            "[INFO] epoch: 566, train loss: 0.0000098868, val loss: 0.0006363039\n",
            "[INFO] epoch: 567, train loss: 0.0000117970, val loss: 0.0003228181\n",
            "[INFO] epoch: 568, train loss: 0.0000098675, val loss: 0.0006405327\n",
            "[INFO] epoch: 569, train loss: 0.0000060373, val loss: 0.0003562495\n",
            "[INFO] epoch: 570, train loss: 0.0000034619, val loss: 0.0005517142\n",
            "[INFO] epoch: 571, train loss: 0.0000018198, val loss: 0.0004177298\n",
            "[INFO] epoch: 572, train loss: 0.0000014398, val loss: 0.0004991368\n",
            "[INFO] epoch: 573, train loss: 0.0000011116, val loss: 0.0004492139\n",
            "[INFO] epoch: 574, train loss: 0.0000010835, val loss: 0.0004796590\n",
            "[INFO] epoch: 575, train loss: 0.0000010264, val loss: 0.0004637032\n",
            "[INFO] epoch: 576, train loss: 0.0000010336, val loss: 0.0004733453\n",
            "[INFO] epoch: 577, train loss: 0.0000010227, val loss: 0.0004696034\n",
            "[INFO] epoch: 578, train loss: 0.0000010264, val loss: 0.0004726607\n",
            "[INFO] epoch: 579, train loss: 0.0000010234, val loss: 0.0004720771\n",
            "[INFO] epoch: 580, train loss: 0.0000010243, val loss: 0.0004734743\n",
            "[INFO] epoch: 581, train loss: 0.0000010232, val loss: 0.0004736280\n",
            "[INFO] epoch: 582, train loss: 0.0000010234, val loss: 0.0004744890\n",
            "[INFO] epoch: 583, train loss: 0.0000010228, val loss: 0.0004748458\n",
            "[INFO] epoch: 584, train loss: 0.0000010227, val loss: 0.0004754786\n",
            "[INFO] epoch: 585, train loss: 0.0000010222, val loss: 0.0004758690\n",
            "[INFO] epoch: 586, train loss: 0.0000010221, val loss: 0.0004763838\n",
            "[INFO] epoch: 587, train loss: 0.0000010217, val loss: 0.0004767577\n",
            "[INFO] epoch: 588, train loss: 0.0000010215, val loss: 0.0004771954\n",
            "[INFO] epoch: 589, train loss: 0.0000010211, val loss: 0.0004775389\n",
            "[INFO] epoch: 590, train loss: 0.0000010209, val loss: 0.0004779162\n",
            "[INFO] epoch: 591, train loss: 0.0000010206, val loss: 0.0004782275\n",
            "[INFO] epoch: 592, train loss: 0.0000010204, val loss: 0.0004785560\n",
            "[INFO] epoch: 593, train loss: 0.0000010201, val loss: 0.0004788344\n",
            "[INFO] epoch: 594, train loss: 0.0000010198, val loss: 0.0004791217\n",
            "[INFO] epoch: 595, train loss: 0.0000010195, val loss: 0.0004793701\n",
            "[INFO] epoch: 596, train loss: 0.0000010193, val loss: 0.0004796255\n",
            "[INFO] epoch: 597, train loss: 0.0000010190, val loss: 0.0004798413\n",
            "[INFO] epoch: 598, train loss: 0.0000010187, val loss: 0.0004800683\n",
            "[INFO] epoch: 599, train loss: 0.0000010184, val loss: 0.0004802615\n",
            "[INFO] epoch: 600, train loss: 0.0000010182, val loss: 0.0004804581\n",
            "[INFO] epoch: 601, train loss: 0.0000010179, val loss: 0.0004806282\n",
            "[INFO] epoch: 602, train loss: 0.0000010176, val loss: 0.0004808055\n",
            "[INFO] epoch: 603, train loss: 0.0000010173, val loss: 0.0004809504\n",
            "[INFO] epoch: 604, train loss: 0.0000010171, val loss: 0.0004811093\n",
            "[INFO] epoch: 605, train loss: 0.0000010168, val loss: 0.0004812357\n",
            "[INFO] epoch: 606, train loss: 0.0000010165, val loss: 0.0004813748\n",
            "[INFO] epoch: 607, train loss: 0.0000010162, val loss: 0.0004814840\n",
            "[INFO] epoch: 608, train loss: 0.0000010160, val loss: 0.0004816078\n",
            "[INFO] epoch: 609, train loss: 0.0000010157, val loss: 0.0004816986\n",
            "[INFO] epoch: 610, train loss: 0.0000010154, val loss: 0.0004818135\n",
            "[INFO] epoch: 611, train loss: 0.0000010151, val loss: 0.0004818848\n",
            "[INFO] epoch: 612, train loss: 0.0000010149, val loss: 0.0004819879\n",
            "[INFO] epoch: 613, train loss: 0.0000010146, val loss: 0.0004820455\n",
            "[INFO] epoch: 614, train loss: 0.0000010143, val loss: 0.0004821377\n",
            "[INFO] epoch: 615, train loss: 0.0000010140, val loss: 0.0004821802\n",
            "[INFO] epoch: 616, train loss: 0.0000010138, val loss: 0.0004822659\n",
            "[INFO] epoch: 617, train loss: 0.0000010135, val loss: 0.0004822915\n",
            "[INFO] epoch: 618, train loss: 0.0000010132, val loss: 0.0004823759\n",
            "[INFO] epoch: 619, train loss: 0.0000010129, val loss: 0.0004823783\n",
            "[INFO] epoch: 620, train loss: 0.0000010127, val loss: 0.0004824711\n",
            "[INFO] epoch: 621, train loss: 0.0000010123, val loss: 0.0004824413\n",
            "[INFO] epoch: 622, train loss: 0.0000010121, val loss: 0.0004825533\n",
            "[INFO] epoch: 623, train loss: 0.0000010117, val loss: 0.0004824820\n",
            "[INFO] epoch: 624, train loss: 0.0000010116, val loss: 0.0004826283\n",
            "[INFO] epoch: 625, train loss: 0.0000010111, val loss: 0.0004824928\n",
            "[INFO] epoch: 626, train loss: 0.0000010110, val loss: 0.0004827054\n",
            "[INFO] epoch: 627, train loss: 0.0000010105, val loss: 0.0004824650\n",
            "[INFO] epoch: 628, train loss: 0.0000010106, val loss: 0.0004827997\n",
            "[INFO] epoch: 629, train loss: 0.0000010099, val loss: 0.0004823776\n",
            "[INFO] epoch: 630, train loss: 0.0000010102, val loss: 0.0004829417\n",
            "[INFO] epoch: 631, train loss: 0.0000010092, val loss: 0.0004821921\n",
            "[INFO] epoch: 632, train loss: 0.0000010099, val loss: 0.0004831891\n",
            "[INFO] epoch: 633, train loss: 0.0000010085, val loss: 0.0004818255\n",
            "[INFO] epoch: 634, train loss: 0.0000010103, val loss: 0.0004836652\n",
            "[INFO] epoch: 635, train loss: 0.0000010081, val loss: 0.0004811057\n",
            "[INFO] epoch: 636, train loss: 0.0000010123, val loss: 0.0004846222\n",
            "[INFO] epoch: 637, train loss: 0.0000010097, val loss: 0.0004796596\n",
            "[INFO] epoch: 638, train loss: 0.0000010205, val loss: 0.0004866120\n",
            "[INFO] epoch: 639, train loss: 0.0000010209, val loss: 0.0004766673\n",
            "[INFO] epoch: 640, train loss: 0.0000010542, val loss: 0.0004908669\n",
            "[INFO] epoch: 641, train loss: 0.0000010804, val loss: 0.0004702739\n",
            "[INFO] epoch: 642, train loss: 0.0000012032, val loss: 0.0005002082\n",
            "[INFO] epoch: 643, train loss: 0.0000013788, val loss: 0.0004561901\n",
            "[INFO] epoch: 644, train loss: 0.0000018927, val loss: 0.0005212363\n",
            "[INFO] epoch: 645, train loss: 0.0000028046, val loss: 0.0004246596\n",
            "[INFO] epoch: 646, train loss: 0.0000047721, val loss: 0.0005682623\n",
            "[INFO] epoch: 647, train loss: 0.0000077398, val loss: 0.0003628049\n",
            "[INFO] epoch: 648, train loss: 0.0000106953, val loss: 0.0006382658\n",
            "[INFO] epoch: 649, train loss: 0.0000112639, val loss: 0.0003180963\n",
            "[INFO] epoch: 650, train loss: 0.0000084142, val loss: 0.0006162421\n",
            "[INFO] epoch: 651, train loss: 0.0000047328, val loss: 0.0003645414\n",
            "[INFO] epoch: 652, train loss: 0.0000027980, val loss: 0.0005314174\n",
            "[INFO] epoch: 653, train loss: 0.0000015712, val loss: 0.0004188156\n",
            "[INFO] epoch: 654, train loss: 0.0000013148, val loss: 0.0004886403\n",
            "[INFO] epoch: 655, train loss: 0.0000010608, val loss: 0.0004454815\n",
            "[INFO] epoch: 656, train loss: 0.0000010444, val loss: 0.0004724262\n",
            "[INFO] epoch: 657, train loss: 0.0000009967, val loss: 0.0004581129\n",
            "[INFO] epoch: 658, train loss: 0.0000010045, val loss: 0.0004671858\n",
            "[INFO] epoch: 659, train loss: 0.0000009938, val loss: 0.0004633849\n",
            "[INFO] epoch: 660, train loss: 0.0000009978, val loss: 0.0004666478\n",
            "[INFO] epoch: 661, train loss: 0.0000009945, val loss: 0.0004657649\n",
            "[INFO] epoch: 662, train loss: 0.0000009958, val loss: 0.0004673629\n",
            "[INFO] epoch: 663, train loss: 0.0000009944, val loss: 0.0004673020\n",
            "[INFO] epoch: 664, train loss: 0.0000009949, val loss: 0.0004682798\n",
            "[INFO] epoch: 665, train loss: 0.0000009941, val loss: 0.0004684935\n",
            "[INFO] epoch: 666, train loss: 0.0000009942, val loss: 0.0004691910\n",
            "[INFO] epoch: 667, train loss: 0.0000009937, val loss: 0.0004694827\n",
            "[INFO] epoch: 668, train loss: 0.0000009936, val loss: 0.0004700305\n",
            "[INFO] epoch: 669, train loss: 0.0000009932, val loss: 0.0004703323\n",
            "[INFO] epoch: 670, train loss: 0.0000009931, val loss: 0.0004707833\n",
            "[INFO] epoch: 671, train loss: 0.0000009928, val loss: 0.0004710748\n",
            "[INFO] epoch: 672, train loss: 0.0000009926, val loss: 0.0004714552\n",
            "[INFO] epoch: 673, train loss: 0.0000009923, val loss: 0.0004717223\n",
            "[INFO] epoch: 674, train loss: 0.0000009922, val loss: 0.0004720493\n",
            "[INFO] epoch: 675, train loss: 0.0000009919, val loss: 0.0004722915\n",
            "[INFO] epoch: 676, train loss: 0.0000009917, val loss: 0.0004725752\n",
            "[INFO] epoch: 677, train loss: 0.0000009914, val loss: 0.0004727883\n",
            "[INFO] epoch: 678, train loss: 0.0000009912, val loss: 0.0004730382\n",
            "[INFO] epoch: 679, train loss: 0.0000009910, val loss: 0.0004732258\n",
            "[INFO] epoch: 680, train loss: 0.0000009908, val loss: 0.0004734461\n",
            "[INFO] epoch: 681, train loss: 0.0000009905, val loss: 0.0004736071\n",
            "[INFO] epoch: 682, train loss: 0.0000009903, val loss: 0.0004738021\n",
            "[INFO] epoch: 683, train loss: 0.0000009900, val loss: 0.0004739439\n",
            "[INFO] epoch: 684, train loss: 0.0000009898, val loss: 0.0004741132\n",
            "[INFO] epoch: 685, train loss: 0.0000009896, val loss: 0.0004742352\n",
            "[INFO] epoch: 686, train loss: 0.0000009894, val loss: 0.0004743875\n",
            "[INFO] epoch: 687, train loss: 0.0000009891, val loss: 0.0004744875\n",
            "[INFO] epoch: 688, train loss: 0.0000009889, val loss: 0.0004746263\n",
            "[INFO] epoch: 689, train loss: 0.0000009886, val loss: 0.0004747033\n",
            "[INFO] epoch: 690, train loss: 0.0000009884, val loss: 0.0004748312\n",
            "[INFO] epoch: 691, train loss: 0.0000009882, val loss: 0.0004748927\n",
            "[INFO] epoch: 692, train loss: 0.0000009880, val loss: 0.0004750065\n",
            "[INFO] epoch: 693, train loss: 0.0000009877, val loss: 0.0004750495\n",
            "[INFO] epoch: 694, train loss: 0.0000009875, val loss: 0.0004751577\n",
            "[INFO] epoch: 695, train loss: 0.0000009872, val loss: 0.0004751789\n",
            "[INFO] epoch: 696, train loss: 0.0000009870, val loss: 0.0004752882\n",
            "[INFO] epoch: 697, train loss: 0.0000009867, val loss: 0.0004752833\n",
            "[INFO] epoch: 698, train loss: 0.0000009866, val loss: 0.0004754003\n",
            "[INFO] epoch: 699, train loss: 0.0000009862, val loss: 0.0004753606\n",
            "[INFO] epoch: 700, train loss: 0.0000009861, val loss: 0.0004754965\n",
            "[INFO] epoch: 701, train loss: 0.0000009858, val loss: 0.0004754117\n",
            "[INFO] epoch: 702, train loss: 0.0000009857, val loss: 0.0004755803\n",
            "[INFO] epoch: 703, train loss: 0.0000009852, val loss: 0.0004754337\n",
            "[INFO] epoch: 704, train loss: 0.0000009852, val loss: 0.0004756646\n",
            "[INFO] epoch: 705, train loss: 0.0000009847, val loss: 0.0004754183\n",
            "[INFO] epoch: 706, train loss: 0.0000009848, val loss: 0.0004757586\n",
            "[INFO] epoch: 707, train loss: 0.0000009842, val loss: 0.0004753532\n",
            "[INFO] epoch: 708, train loss: 0.0000009845, val loss: 0.0004758864\n",
            "[INFO] epoch: 709, train loss: 0.0000009836, val loss: 0.0004752042\n",
            "[INFO] epoch: 710, train loss: 0.0000009843, val loss: 0.0004760915\n",
            "[INFO] epoch: 711, train loss: 0.0000009830, val loss: 0.0004749200\n",
            "[INFO] epoch: 712, train loss: 0.0000009844, val loss: 0.0004764544\n",
            "[INFO] epoch: 713, train loss: 0.0000009825, val loss: 0.0004743821\n",
            "[INFO] epoch: 714, train loss: 0.0000009855, val loss: 0.0004771449\n",
            "[INFO] epoch: 715, train loss: 0.0000009829, val loss: 0.0004733489\n",
            "[INFO] epoch: 716, train loss: 0.0000009899, val loss: 0.0004785095\n",
            "[INFO] epoch: 717, train loss: 0.0000009876, val loss: 0.0004713225\n",
            "[INFO] epoch: 718, train loss: 0.0000010058, val loss: 0.0004812840\n",
            "[INFO] epoch: 719, train loss: 0.0000010119, val loss: 0.0004672165\n",
            "[INFO] epoch: 720, train loss: 0.0000010688, val loss: 0.0004870802\n",
            "[INFO] epoch: 721, train loss: 0.0000011263, val loss: 0.0004586341\n",
            "[INFO] epoch: 722, train loss: 0.0000013348, val loss: 0.0004994992\n",
            "[INFO] epoch: 723, train loss: 0.0000016538, val loss: 0.0004402264\n",
            "[INFO] epoch: 724, train loss: 0.0000024710, val loss: 0.0005265788\n",
            "[INFO] epoch: 725, train loss: 0.0000038595, val loss: 0.0004011796\n",
            "[INFO] epoch: 726, train loss: 0.0000063430, val loss: 0.0005813984\n",
            "[INFO] epoch: 727, train loss: 0.0000092315, val loss: 0.0003394953\n",
            "[INFO] epoch: 728, train loss: 0.0000105887, val loss: 0.0006308766\n",
            "[INFO] epoch: 729, train loss: 0.0000091132, val loss: 0.0003245018\n",
            "[INFO] epoch: 730, train loss: 0.0000060599, val loss: 0.0005788371\n",
            "[INFO] epoch: 731, train loss: 0.0000032463, val loss: 0.0003783292\n",
            "[INFO] epoch: 732, train loss: 0.0000021301, val loss: 0.0005094728\n",
            "[INFO] epoch: 733, train loss: 0.0000013424, val loss: 0.0004207298\n",
            "[INFO] epoch: 734, train loss: 0.0000011970, val loss: 0.0004782194\n",
            "[INFO] epoch: 735, train loss: 0.0000010165, val loss: 0.0004419571\n",
            "[INFO] epoch: 736, train loss: 0.0000010121, val loss: 0.0004655936\n",
            "[INFO] epoch: 737, train loss: 0.0000009716, val loss: 0.0004524022\n",
            "[INFO] epoch: 738, train loss: 0.0000009805, val loss: 0.0004614775\n",
            "[INFO] epoch: 739, train loss: 0.0000009688, val loss: 0.0004570353\n",
            "[INFO] epoch: 740, train loss: 0.0000009737, val loss: 0.0004609552\n",
            "[INFO] epoch: 741, train loss: 0.0000009693, val loss: 0.0004593777\n",
            "[INFO] epoch: 742, train loss: 0.0000009715, val loss: 0.0004614577\n",
            "[INFO] epoch: 743, train loss: 0.0000009694, val loss: 0.0004609254\n",
            "[INFO] epoch: 744, train loss: 0.0000009705, val loss: 0.0004622039\n",
            "[INFO] epoch: 745, train loss: 0.0000009693, val loss: 0.0004620994\n",
            "[INFO] epoch: 746, train loss: 0.0000009698, val loss: 0.0004629825\n",
            "[INFO] epoch: 747, train loss: 0.0000009690, val loss: 0.0004630532\n",
            "[INFO] epoch: 748, train loss: 0.0000009692, val loss: 0.0004637156\n",
            "[INFO] epoch: 749, train loss: 0.0000009687, val loss: 0.0004638583\n",
            "[INFO] epoch: 750, train loss: 0.0000009687, val loss: 0.0004643862\n",
            "[INFO] epoch: 751, train loss: 0.0000009683, val loss: 0.0004645473\n",
            "[INFO] epoch: 752, train loss: 0.0000009683, val loss: 0.0004649862\n",
            "[INFO] epoch: 753, train loss: 0.0000009679, val loss: 0.0004651475\n",
            "[INFO] epoch: 754, train loss: 0.0000009679, val loss: 0.0004655165\n",
            "[INFO] epoch: 755, train loss: 0.0000009675, val loss: 0.0004656682\n",
            "[INFO] epoch: 756, train loss: 0.0000009675, val loss: 0.0004659855\n",
            "[INFO] epoch: 757, train loss: 0.0000009672, val loss: 0.0004661229\n",
            "[INFO] epoch: 758, train loss: 0.0000009671, val loss: 0.0004663985\n",
            "[INFO] epoch: 759, train loss: 0.0000009668, val loss: 0.0004665176\n",
            "[INFO] epoch: 760, train loss: 0.0000009667, val loss: 0.0004667612\n",
            "[INFO] epoch: 761, train loss: 0.0000009664, val loss: 0.0004668597\n",
            "[INFO] epoch: 762, train loss: 0.0000009663, val loss: 0.0004670801\n",
            "[INFO] epoch: 763, train loss: 0.0000009660, val loss: 0.0004671561\n",
            "[INFO] epoch: 764, train loss: 0.0000009659, val loss: 0.0004673557\n",
            "[INFO] epoch: 765, train loss: 0.0000009656, val loss: 0.0004674105\n",
            "[INFO] epoch: 766, train loss: 0.0000009655, val loss: 0.0004675974\n",
            "[INFO] epoch: 767, train loss: 0.0000009652, val loss: 0.0004676279\n",
            "[INFO] epoch: 768, train loss: 0.0000009651, val loss: 0.0004678073\n",
            "[INFO] epoch: 769, train loss: 0.0000009648, val loss: 0.0004678104\n",
            "[INFO] epoch: 770, train loss: 0.0000009647, val loss: 0.0004679888\n",
            "[INFO] epoch: 771, train loss: 0.0000009643, val loss: 0.0004679615\n",
            "[INFO] epoch: 772, train loss: 0.0000009643, val loss: 0.0004681450\n",
            "[INFO] epoch: 773, train loss: 0.0000009639, val loss: 0.0004680818\n",
            "[INFO] epoch: 774, train loss: 0.0000009639, val loss: 0.0004682817\n",
            "[INFO] epoch: 775, train loss: 0.0000009635, val loss: 0.0004681710\n",
            "[INFO] epoch: 776, train loss: 0.0000009635, val loss: 0.0004684065\n",
            "[INFO] epoch: 777, train loss: 0.0000009630, val loss: 0.0004682264\n",
            "[INFO] epoch: 778, train loss: 0.0000009631, val loss: 0.0004685251\n",
            "[INFO] epoch: 779, train loss: 0.0000009626, val loss: 0.0004682448\n",
            "[INFO] epoch: 780, train loss: 0.0000009628, val loss: 0.0004686439\n",
            "[INFO] epoch: 781, train loss: 0.0000009621, val loss: 0.0004682162\n",
            "[INFO] epoch: 782, train loss: 0.0000009625, val loss: 0.0004687856\n",
            "[INFO] epoch: 783, train loss: 0.0000009616, val loss: 0.0004681168\n",
            "[INFO] epoch: 784, train loss: 0.0000009623, val loss: 0.0004689795\n",
            "[INFO] epoch: 785, train loss: 0.0000009611, val loss: 0.0004679140\n",
            "[INFO] epoch: 786, train loss: 0.0000009624, val loss: 0.0004692789\n",
            "[INFO] epoch: 787, train loss: 0.0000009606, val loss: 0.0004675284\n",
            "[INFO] epoch: 788, train loss: 0.0000009630, val loss: 0.0004697935\n",
            "[INFO] epoch: 789, train loss: 0.0000009605, val loss: 0.0004668193\n",
            "[INFO] epoch: 790, train loss: 0.0000009652, val loss: 0.0004707174\n",
            "[INFO] epoch: 791, train loss: 0.0000009622, val loss: 0.0004655027\n",
            "[INFO] epoch: 792, train loss: 0.0000009725, val loss: 0.0004724690\n",
            "[INFO] epoch: 793, train loss: 0.0000009710, val loss: 0.0004629909\n",
            "[INFO] epoch: 794, train loss: 0.0000009973, val loss: 0.0004758837\n",
            "[INFO] epoch: 795, train loss: 0.0000010095, val loss: 0.0004580774\n",
            "[INFO] epoch: 796, train loss: 0.0000010880, val loss: 0.0004827344\n",
            "[INFO] epoch: 797, train loss: 0.0000011716, val loss: 0.0004481745\n",
            "[INFO] epoch: 798, train loss: 0.0000014413, val loss: 0.0004968063\n",
            "[INFO] epoch: 799, train loss: 0.0000018442, val loss: 0.0004278589\n",
            "[INFO] epoch: 800, train loss: 0.0000027935, val loss: 0.0005258853\n",
            "[INFO] epoch: 801, train loss: 0.0000042833, val loss: 0.0003878091\n",
            "[INFO] epoch: 802, train loss: 0.0000066406, val loss: 0.0005784522\n",
            "[INFO] epoch: 803, train loss: 0.0000089231, val loss: 0.0003339511\n",
            "[INFO] epoch: 804, train loss: 0.0000095131, val loss: 0.0006123499\n",
            "[INFO] epoch: 805, train loss: 0.0000077700, val loss: 0.0003297463\n",
            "[INFO] epoch: 806, train loss: 0.0000052411, val loss: 0.0005603208\n",
            "[INFO] epoch: 807, train loss: 0.0000029331, val loss: 0.0003781063\n",
            "[INFO] epoch: 808, train loss: 0.0000020333, val loss: 0.0005013469\n",
            "[INFO] epoch: 809, train loss: 0.0000013289, val loss: 0.0004150921\n",
            "[INFO] epoch: 810, train loss: 0.0000011941, val loss: 0.0004734161\n",
            "[INFO] epoch: 811, train loss: 0.0000010091, val loss: 0.0004347960\n",
            "[INFO] epoch: 812, train loss: 0.0000010043, val loss: 0.0004612661\n",
            "[INFO] epoch: 813, train loss: 0.0000009542, val loss: 0.0004450675\n",
            "[INFO] epoch: 814, train loss: 0.0000009647, val loss: 0.0004567718\n",
            "[INFO] epoch: 815, train loss: 0.0000009478, val loss: 0.0004500300\n",
            "[INFO] epoch: 816, train loss: 0.0000009549, val loss: 0.0004557278\n",
            "[INFO] epoch: 817, train loss: 0.0000009477, val loss: 0.0004526660\n",
            "[INFO] epoch: 818, train loss: 0.0000009515, val loss: 0.0004558624\n",
            "[INFO] epoch: 819, train loss: 0.0000009479, val loss: 0.0004543736\n",
            "[INFO] epoch: 820, train loss: 0.0000009500, val loss: 0.0004563655\n",
            "[INFO] epoch: 821, train loss: 0.0000009479, val loss: 0.0004556211\n",
            "[INFO] epoch: 822, train loss: 0.0000009492, val loss: 0.0004569797\n",
            "[INFO] epoch: 823, train loss: 0.0000009477, val loss: 0.0004565977\n",
            "[INFO] epoch: 824, train loss: 0.0000009485, val loss: 0.0004575958\n",
            "[INFO] epoch: 825, train loss: 0.0000009475, val loss: 0.0004574034\n",
            "[INFO] epoch: 826, train loss: 0.0000009480, val loss: 0.0004581755\n",
            "[INFO] epoch: 827, train loss: 0.0000009472, val loss: 0.0004580793\n",
            "[INFO] epoch: 828, train loss: 0.0000009476, val loss: 0.0004587065\n",
            "[INFO] epoch: 829, train loss: 0.0000009470, val loss: 0.0004586539\n",
            "[INFO] epoch: 830, train loss: 0.0000009472, val loss: 0.0004591834\n",
            "[INFO] epoch: 831, train loss: 0.0000009466, val loss: 0.0004591449\n",
            "[INFO] epoch: 832, train loss: 0.0000009468, val loss: 0.0004596065\n",
            "[INFO] epoch: 833, train loss: 0.0000009463, val loss: 0.0004595686\n",
            "[INFO] epoch: 834, train loss: 0.0000009465, val loss: 0.0004599804\n",
            "[INFO] epoch: 835, train loss: 0.0000009460, val loss: 0.0004599336\n",
            "[INFO] epoch: 836, train loss: 0.0000009461, val loss: 0.0004603121\n",
            "[INFO] epoch: 837, train loss: 0.0000009456, val loss: 0.0004602448\n",
            "[INFO] epoch: 838, train loss: 0.0000009457, val loss: 0.0004606027\n",
            "[INFO] epoch: 839, train loss: 0.0000009453, val loss: 0.0004605094\n",
            "[INFO] epoch: 840, train loss: 0.0000009454, val loss: 0.0004608580\n",
            "[INFO] epoch: 841, train loss: 0.0000009449, val loss: 0.0004607288\n",
            "[INFO] epoch: 842, train loss: 0.0000009451, val loss: 0.0004610856\n",
            "[INFO] epoch: 843, train loss: 0.0000009446, val loss: 0.0004609088\n",
            "[INFO] epoch: 844, train loss: 0.0000009447, val loss: 0.0004612920\n",
            "[INFO] epoch: 845, train loss: 0.0000009442, val loss: 0.0004610469\n",
            "[INFO] epoch: 846, train loss: 0.0000009444, val loss: 0.0004614793\n",
            "[INFO] epoch: 847, train loss: 0.0000009438, val loss: 0.0004611440\n",
            "[INFO] epoch: 848, train loss: 0.0000009441, val loss: 0.0004616557\n",
            "[INFO] epoch: 849, train loss: 0.0000009434, val loss: 0.0004611958\n",
            "[INFO] epoch: 850, train loss: 0.0000009439, val loss: 0.0004618376\n",
            "[INFO] epoch: 851, train loss: 0.0000009429, val loss: 0.0004611913\n",
            "[INFO] epoch: 852, train loss: 0.0000009437, val loss: 0.0004620401\n",
            "[INFO] epoch: 853, train loss: 0.0000009425, val loss: 0.0004611116\n",
            "[INFO] epoch: 854, train loss: 0.0000009436, val loss: 0.0004622904\n",
            "[INFO] epoch: 855, train loss: 0.0000009420, val loss: 0.0004609242\n",
            "[INFO] epoch: 856, train loss: 0.0000009438, val loss: 0.0004626419\n",
            "[INFO] epoch: 857, train loss: 0.0000009417, val loss: 0.0004605625\n",
            "[INFO] epoch: 858, train loss: 0.0000009446, val loss: 0.0004631809\n",
            "[INFO] epoch: 859, train loss: 0.0000009417, val loss: 0.0004599134\n",
            "[INFO] epoch: 860, train loss: 0.0000009468, val loss: 0.0004640687\n",
            "[INFO] epoch: 861, train loss: 0.0000009432, val loss: 0.0004587583\n",
            "[INFO] epoch: 862, train loss: 0.0000009530, val loss: 0.0004656096\n",
            "[INFO] epoch: 863, train loss: 0.0000009499, val loss: 0.0004566761\n",
            "[INFO] epoch: 864, train loss: 0.0000009712, val loss: 0.0004683908\n",
            "[INFO] epoch: 865, train loss: 0.0000009752, val loss: 0.0004528418\n",
            "[INFO] epoch: 866, train loss: 0.0000010291, val loss: 0.0004735829\n",
            "[INFO] epoch: 867, train loss: 0.0000010686, val loss: 0.0004456038\n",
            "[INFO] epoch: 868, train loss: 0.0000012254, val loss: 0.0004835327\n",
            "[INFO] epoch: 869, train loss: 0.0000014137, val loss: 0.0004316640\n",
            "[INFO] epoch: 870, train loss: 0.0000019067, val loss: 0.0005028565\n",
            "[INFO] epoch: 871, train loss: 0.0000026236, val loss: 0.0004050421\n",
            "[INFO] epoch: 872, train loss: 0.0000040116, val loss: 0.0005387302\n",
            "[INFO] epoch: 873, train loss: 0.0000057851, val loss: 0.0003613232\n",
            "[INFO] epoch: 874, train loss: 0.0000076581, val loss: 0.0005845204\n",
            "[INFO] epoch: 875, train loss: 0.0000083491, val loss: 0.0003273964\n",
            "[INFO] epoch: 876, train loss: 0.0000073321, val loss: 0.0005813153\n",
            "[INFO] epoch: 877, train loss: 0.0000051131, val loss: 0.0003479246\n",
            "[INFO] epoch: 878, train loss: 0.0000034712, val loss: 0.0005262530\n",
            "[INFO] epoch: 879, train loss: 0.0000020779, val loss: 0.0003889947\n",
            "[INFO] epoch: 880, train loss: 0.0000016344, val loss: 0.0004853452\n",
            "[INFO] epoch: 881, train loss: 0.0000011887, val loss: 0.0004159810\n",
            "[INFO] epoch: 882, train loss: 0.0000011196, val loss: 0.0004654334\n",
            "[INFO] epoch: 883, train loss: 0.0000009813, val loss: 0.0004313370\n",
            "[INFO] epoch: 884, train loss: 0.0000009853, val loss: 0.0004561690\n",
            "[INFO] epoch: 885, train loss: 0.0000009381, val loss: 0.0004397152\n",
            "[INFO] epoch: 886, train loss: 0.0000009502, val loss: 0.0004524860\n",
            "[INFO] epoch: 887, train loss: 0.0000009305, val loss: 0.0004441583\n",
            "[INFO] epoch: 888, train loss: 0.0000009393, val loss: 0.0004513538\n",
            "[INFO] epoch: 889, train loss: 0.0000009294, val loss: 0.0004467603\n",
            "[INFO] epoch: 890, train loss: 0.0000009351, val loss: 0.0004512219\n",
            "[INFO] epoch: 891, train loss: 0.0000009293, val loss: 0.0004484917\n",
            "[INFO] epoch: 892, train loss: 0.0000009330, val loss: 0.0004514857\n",
            "[INFO] epoch: 893, train loss: 0.0000009294, val loss: 0.0004497495\n",
            "[INFO] epoch: 894, train loss: 0.0000009319, val loss: 0.0004519030\n",
            "[INFO] epoch: 895, train loss: 0.0000009293, val loss: 0.0004507187\n",
            "[INFO] epoch: 896, train loss: 0.0000009311, val loss: 0.0004523615\n",
            "[INFO] epoch: 897, train loss: 0.0000009292, val loss: 0.0004514953\n",
            "[INFO] epoch: 898, train loss: 0.0000009305, val loss: 0.0004528167\n",
            "[INFO] epoch: 899, train loss: 0.0000009290, val loss: 0.0004521344\n",
            "[INFO] epoch: 900, train loss: 0.0000009301, val loss: 0.0004532452\n",
            "[INFO] epoch: 901, train loss: 0.0000009288, val loss: 0.0004526658\n",
            "[INFO] epoch: 902, train loss: 0.0000009297, val loss: 0.0004536448\n",
            "[INFO] epoch: 903, train loss: 0.0000009285, val loss: 0.0004531083\n",
            "[INFO] epoch: 904, train loss: 0.0000009293, val loss: 0.0004540061\n",
            "[INFO] epoch: 905, train loss: 0.0000009282, val loss: 0.0004534781\n",
            "[INFO] epoch: 906, train loss: 0.0000009290, val loss: 0.0004543342\n",
            "[INFO] epoch: 907, train loss: 0.0000009279, val loss: 0.0004537819\n",
            "[INFO] epoch: 908, train loss: 0.0000009287, val loss: 0.0004546354\n",
            "[INFO] epoch: 909, train loss: 0.0000009276, val loss: 0.0004540239\n",
            "[INFO] epoch: 910, train loss: 0.0000009284, val loss: 0.0004549133\n",
            "[INFO] epoch: 911, train loss: 0.0000009273, val loss: 0.0004542128\n",
            "[INFO] epoch: 912, train loss: 0.0000009282, val loss: 0.0004551769\n",
            "[INFO] epoch: 913, train loss: 0.0000009269, val loss: 0.0004543402\n",
            "[INFO] epoch: 914, train loss: 0.0000009280, val loss: 0.0004554384\n",
            "[INFO] epoch: 915, train loss: 0.0000009265, val loss: 0.0004544005\n",
            "[INFO] epoch: 916, train loss: 0.0000009279, val loss: 0.0004557159\n",
            "[INFO] epoch: 917, train loss: 0.0000009262, val loss: 0.0004543832\n",
            "[INFO] epoch: 918, train loss: 0.0000009279, val loss: 0.0004560279\n",
            "[INFO] epoch: 919, train loss: 0.0000009258, val loss: 0.0004542601\n",
            "[INFO] epoch: 920, train loss: 0.0000009282, val loss: 0.0004564160\n",
            "[INFO] epoch: 921, train loss: 0.0000009255, val loss: 0.0004539899\n",
            "[INFO] epoch: 922, train loss: 0.0000009289, val loss: 0.0004569453\n",
            "[INFO] epoch: 923, train loss: 0.0000009255, val loss: 0.0004534925\n",
            "[INFO] epoch: 924, train loss: 0.0000009307, val loss: 0.0004577169\n",
            "[INFO] epoch: 925, train loss: 0.0000009265, val loss: 0.0004526346\n",
            "[INFO] epoch: 926, train loss: 0.0000009350, val loss: 0.0004589219\n",
            "[INFO] epoch: 927, train loss: 0.0000009303, val loss: 0.0004511623\n",
            "[INFO] epoch: 928, train loss: 0.0000009457, val loss: 0.0004608932\n",
            "[INFO] epoch: 929, train loss: 0.0000009428, val loss: 0.0004486308\n",
            "[INFO] epoch: 930, train loss: 0.0000009741, val loss: 0.0004642510\n",
            "[INFO] epoch: 931, train loss: 0.0000009820, val loss: 0.0004441882\n",
            "[INFO] epoch: 932, train loss: 0.0000010546, val loss: 0.0004701590\n",
            "[INFO] epoch: 933, train loss: 0.0000011071, val loss: 0.0004362426\n",
            "[INFO] epoch: 934, train loss: 0.0000012965, val loss: 0.0004807945\n",
            "[INFO] epoch: 935, train loss: 0.0000015077, val loss: 0.0004218701\n",
            "[INFO] epoch: 936, train loss: 0.0000020266, val loss: 0.0004999912\n",
            "[INFO] epoch: 937, train loss: 0.0000027050, val loss: 0.0003965630\n",
            "[INFO] epoch: 938, train loss: 0.0000039303, val loss: 0.0005321255\n",
            "[INFO] epoch: 939, train loss: 0.0000053031, val loss: 0.0003594489\n",
            "[INFO] epoch: 940, train loss: 0.0000066777, val loss: 0.0005675618\n",
            "[INFO] epoch: 941, train loss: 0.0000070552, val loss: 0.0003336393\n",
            "[INFO] epoch: 942, train loss: 0.0000063595, val loss: 0.0005635604\n",
            "[INFO] epoch: 943, train loss: 0.0000046797, val loss: 0.0003493890\n",
            "[INFO] epoch: 944, train loss: 0.0000034143, val loss: 0.0005202115\n",
            "[INFO] epoch: 945, train loss: 0.0000021792, val loss: 0.0003830049\n",
            "[INFO] epoch: 946, train loss: 0.0000017447, val loss: 0.0004843084\n",
            "[INFO] epoch: 947, train loss: 0.0000012683, val loss: 0.0004077214\n",
            "[INFO] epoch: 948, train loss: 0.0000011820, val loss: 0.0004647714\n",
            "[INFO] epoch: 949, train loss: 0.0000010081, val loss: 0.0004228966\n",
            "[INFO] epoch: 950, train loss: 0.0000010068, val loss: 0.0004547936\n",
            "[INFO] epoch: 951, train loss: 0.0000009381, val loss: 0.0004318347\n",
            "[INFO] epoch: 952, train loss: 0.0000009518, val loss: 0.0004501394\n",
            "[INFO] epoch: 953, train loss: 0.0000009200, val loss: 0.0004369638\n",
            "[INFO] epoch: 954, train loss: 0.0000009325, val loss: 0.0004482025\n",
            "[INFO] epoch: 955, train loss: 0.0000009154, val loss: 0.0004400685\n",
            "[INFO] epoch: 956, train loss: 0.0000009246, val loss: 0.0004475148\n",
            "[INFO] epoch: 957, train loss: 0.0000009141, val loss: 0.0004421201\n",
            "[INFO] epoch: 958, train loss: 0.0000009208, val loss: 0.0004473982\n",
            "[INFO] epoch: 959, train loss: 0.0000009137, val loss: 0.0004435738\n",
            "[INFO] epoch: 960, train loss: 0.0000009188, val loss: 0.0004475524\n",
            "[INFO] epoch: 961, train loss: 0.0000009136, val loss: 0.0004446562\n",
            "[INFO] epoch: 962, train loss: 0.0000009175, val loss: 0.0004478292\n",
            "[INFO] epoch: 963, train loss: 0.0000009134, val loss: 0.0004454933\n",
            "[INFO] epoch: 964, train loss: 0.0000009167, val loss: 0.0004481590\n",
            "[INFO] epoch: 965, train loss: 0.0000009133, val loss: 0.0004461518\n",
            "[INFO] epoch: 966, train loss: 0.0000009161, val loss: 0.0004485025\n",
            "[INFO] epoch: 967, train loss: 0.0000009131, val loss: 0.0004466760\n",
            "[INFO] epoch: 968, train loss: 0.0000009157, val loss: 0.0004488480\n",
            "[INFO] epoch: 969, train loss: 0.0000009129, val loss: 0.0004470857\n",
            "[INFO] epoch: 970, train loss: 0.0000009153, val loss: 0.0004491874\n",
            "[INFO] epoch: 971, train loss: 0.0000009126, val loss: 0.0004473996\n",
            "[INFO] epoch: 972, train loss: 0.0000009151, val loss: 0.0004495272\n",
            "[INFO] epoch: 973, train loss: 0.0000009124, val loss: 0.0004476200\n",
            "[INFO] epoch: 974, train loss: 0.0000009150, val loss: 0.0004498721\n",
            "[INFO] epoch: 975, train loss: 0.0000009121, val loss: 0.0004477473\n",
            "[INFO] epoch: 976, train loss: 0.0000009150, val loss: 0.0004502400\n",
            "[INFO] epoch: 977, train loss: 0.0000009119, val loss: 0.0004477679\n",
            "[INFO] epoch: 978, train loss: 0.0000009153, val loss: 0.0004506573\n",
            "[INFO] epoch: 979, train loss: 0.0000009117, val loss: 0.0004476592\n",
            "[INFO] epoch: 980, train loss: 0.0000009159, val loss: 0.0004511591\n",
            "[INFO] epoch: 981, train loss: 0.0000009117, val loss: 0.0004473824\n",
            "[INFO] epoch: 982, train loss: 0.0000009172, val loss: 0.0004518048\n",
            "[INFO] epoch: 983, train loss: 0.0000009123, val loss: 0.0004468652\n",
            "[INFO] epoch: 984, train loss: 0.0000009199, val loss: 0.0004526952\n",
            "[INFO] epoch: 985, train loss: 0.0000009141, val loss: 0.0004459852\n",
            "[INFO] epoch: 986, train loss: 0.0000009254, val loss: 0.0004539934\n",
            "[INFO] epoch: 987, train loss: 0.0000009193, val loss: 0.0004445294\n",
            "[INFO] epoch: 988, train loss: 0.0000009376, val loss: 0.0004559802\n",
            "[INFO] epoch: 989, train loss: 0.0000009330, val loss: 0.0004421368\n",
            "[INFO] epoch: 990, train loss: 0.0000009661, val loss: 0.0004591402\n",
            "[INFO] epoch: 991, train loss: 0.0000009698, val loss: 0.0004381615\n",
            "[INFO] epoch: 992, train loss: 0.0000010366, val loss: 0.0004643278\n",
            "[INFO] epoch: 993, train loss: 0.0000010711, val loss: 0.0004314633\n",
            "[INFO] epoch: 994, train loss: 0.0000012211, val loss: 0.0004730467\n",
            "[INFO] epoch: 995, train loss: 0.0000013539, val loss: 0.0004200870\n",
            "[INFO] epoch: 996, train loss: 0.0000017115, val loss: 0.0004877552\n",
            "[INFO] epoch: 997, train loss: 0.0000021127, val loss: 0.0004011335\n",
            "[INFO] epoch: 998, train loss: 0.0000029078, val loss: 0.0005113520\n",
            "[INFO] epoch: 999, train loss: 0.0000037828, val loss: 0.0003729736\n",
            "[INFO] Finished training! Smallest loss: 0.0000566547\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial test set loss: 5.6654744525985734e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12pztB9iafkH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "ffd25109-8dba-42e6-ebbe-46e04141c2f3"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "test_range_start = date(2020, 11, 30)\n",
        "test_range_end = date(2020, 12, 8)\n",
        "#test_range_end = date(2020, 12, 8)\n",
        "\n",
        "df = generate_time_series(canada_lockdown_measures)\n",
        "df = df[df['date'] >= test_range_start]\n",
        "df = df[df['date'] <= test_range_end]\n",
        "data = merge_dfs(df, twitter_sentiment['CA'], total_cases_worldwide_df, 'Canada')\n",
        "data_scaled = scaler.transform(data)\n",
        "\n",
        "xs, _, _, ys, _, _= create_dataset(data_scaled, look_back, test_size=0., val_size=0.)\n",
        "\n",
        "xs = torch.FloatTensor(xs).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    prediction = best_trained_model_2(xs)\n",
        "\n",
        "    actual = np.zeros(shape=(xs.shape[1], xs.shape[2]))\n",
        "    actual[:,-1] = ys[0]\n",
        "    actual = scaler.inverse_transform(actual)[:,-1]\n",
        "\n",
        "    preds = np.zeros(shape=(xs.shape[1], xs.shape[2]))\n",
        "    prediction = prediction.squeeze().cpu().numpy()\n",
        "    preds[:,-1] = prediction\n",
        "    preds = scaler.inverse_transform(preds)[:, -1]\n",
        "\n",
        "    plt.plot(actual, label='actual')\n",
        "    plt.plot(preds, label='prediction')\n",
        "    plt.legend()\n",
        "    axes = plt.gca()\n",
        "    axes.set_ylim([0, 500000])\n",
        "    plt.show()\n",
        "\n",
        "    rmse = sqrt(mean_squared_error(actual, preds))\n",
        "    print('Test RMSE: %.3f' % rmse)\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:582: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:775.)\n",
            "  self.dropout, self.training, self.bidirectional, self.batch_first)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Test RMSE: 210409.884\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}